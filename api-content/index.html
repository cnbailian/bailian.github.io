{"posts":[{"title":"Kubernetes 网络笔记","content":"集群网络系统是 Kubernetes 的核心部分，其中 Pod 之间的通信的部分 Kubernetes 没有自己实现，而是交给了外部组件进行处理。Kubernetes 对这部分网络模型的要求是：节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。这就需要一个跨主机的容器网络。 本篇笔记前半部分记录了 VXLAN 技术。VXLAN 全称是 Virtual eXtensible Local Area Network，虚拟可扩展的局域网。它是一种 Overlay 技术，通过三层网络来搭建的二层网络。在笔记的后半部分，通过学习 Flannel 的源码手动搭建跨主机容器网络示例。 笔记中 vxlan 内容学习自 cizixs 的两篇博客，一篇介绍协议原理，一篇结合实践。文章写的很详细，而且深入浅出适合学习，建议读者在 vxlan 部分直接看原文。 VXLAN 协议原理 上面提到 vxlan 是 overlay 技术，overlay 网络是建立在已有物理网络（underlay）上的虚拟网络，具有独立的控制和转发平面，对于连接到 overlay 的设备来说，物理网络是透明的。 那么 vxlan 这类的 Overlay 网络解决了那么些问题？ 传统的 VLAN 技术满足不了虚拟化场景下的数据中心规模，VLAN 最多只支持 4096 个网络上限。 数据中心需要提供多租户功能，不同用户之间需要独立的分配 IP 和 MAC 地址 云计算业务需要高灵活性，虚拟机可能会大规模迁移，并保证网络一直可用。 vxlan 实现原理就是使用 VTEP 设备对服务器发出和收到的数据包进行二次封装和解封。所以 vxlan 这类隧道网络对原有的网络架构影响小，原来的网络不需要做任何改动，在原有网络上架设一层新的网络。 VXLAN 模型 物理网络上可以创建多个 vxlan 网络，这些 vxlan 网络可以认为是一个隧道，不同节点的虚拟机能够通过隧道直连。在每个端点上都有一个 vtep 负责 vxlan 协议的封包和解包，也就是在虚拟报文上封装 vtep 通信的报文头部。每个 vxlan 网络由唯一的 VNI 标识，不同的 vxlan 可以不互相影响。 VTEP（VXLAN Tunnel Endpoints）：vxlan 网络的边缘设备，用来进行 vxlan 报文的处理（封包和解包）。vtep 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机）。 VNI（VXLAN Network Identifier）：VNI 是每个 vxlan 的标识，是个 24 位整数，一共有 2^24 = 16,777,216（一千多万），一般每个 VNI 对应一个租户，也就是说使用 vxlan 搭建的公有云可以理论上可以支撑千万级别的租户。 Tunnel：隧道是一个逻辑上的概念，在 vxlan 模型中并没有具体的物理实体想对应。隧道可以看做是一种虚拟通道，vxlan 通信双方（图中的虚拟机）认为自己是在直接通信，并不知道底层网络的存在。从整体来说，每个 vxlan 网络像是为通信的虚拟机搭建了一个单独的通信通道，也就是隧道。 VXLAN 报文解析 白色部分是虚拟机发送的原始报文（二层帧，包含了 MAC 头部、IP 头部和传输层头部的报文），前面加上了 vxlan 头部用于保存 vxlan 相关内容，再前面是标准的 UDP 协议头部（UDP 头部、IP 头部和 MAC 头部）用来在底层网络上传输报文。 最外层的 UDP 协议用来在底层网络上传输，也就是 vtep 之间互相通信的基础。中间是 VXLAN 头部，vetp 接到报文后，根据这部分内容处理 vxlan 逻辑，主要是根据 VNI 发送到最终的虚拟机。最里面是原始报文，也就是虚拟机看到的报文内容。 报文各部分意义如下： VXLAN header：8 字节 VXLAN flags：标志位 Reserved：保留位 VNID：24 位的 VNID 标识 Reserved：保留位 UDP 头部：8 字节 UDP：UDP 通信双方是 vtep 应用，IANA 分配的 vxlan 端口是 4789 IP 头部：20 字节 目的地址：是由虚拟机所在地址宿主机的 vtep 的 IP 地址 MAC 头部：14 字节 MAC 地址：主机之间通信的 MAC 地址 可以看出 vxlan 协议比原始报文多出 50 字节的内容，这会降低网络链路传输有效数据的比例。 实现 VXLAN Linux 在 3.7.0 版本才开始支持 vxlan，请尽量使用比较新版本的 kernel，以免因为内核版本太低导致功能或性能出现问题。 我的实验环境是 2 台 AWS Debian 系统实例： $ uname -r 4.19.0-14-cloud-amd64 $ echo ${HOST1_IP} 172.16.3.142 $ echo ${HOST2_IP} 172.16.2.21 同时为了实验容器网络，会保证每台主机上都有 network namespace（net0）与 bridge（br0） 的连接关系。创建过程在上一篇笔记。 $ ip netns net0 (id: 0) $ ip link veth1 br0 $ ip netns exec net0 ip addr # host1 veth0 link/ether 4e:3d:fd:29:55:38 inet 192.168.2.11/24 scope global veth0 $ ip netns exec net0 ip addr # host2 veth0 link/ether 46:63:12:3e:fa:da inet 192.168.2.12/24 scope global veth0 点对点 VXLAN 首先创建 host1 的点对点的 VXLAN 设备，点对点设备是指创建 vxlan 时指定了 remote 参数的设备： $ ip link add type vxlan id 1 dstport 4789 dev eth0 remote 172.16.2.21 id 1 表示 VNI，在点对点的设备中需要双方保持一致。 dstport 4789 是IANA 分配的 vxlan 端口是 4789，Linux 默认使用 8472，所以这里显式分配。 dev eth0 表示当前节点用于通信的网络设备，用于获取 IP，与 local 172.16.3.142 参数等效。 remote 172.16.2.21 显示指定了 vxlan 的对口 IP，所以只会发往这个地址，类似点对点协议。 host2 主机同样需要创建，注意 id &amp; dspport 参数要保持一致，remote 参数要指定 host1 IP： $ ip link add type vxlan id 1 dstport 4789 dev eth0 remote 172.16.3.142 在两台主机上将 vxlan 设备挂载至 bridge，并启动： $ ip link set vxlan0 master br0 $ ip link set up vxlan0 尝试 ping: $ ip netns exec net0 ping -c1 192.168.2.12 PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data. 64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=1.31 ms --- 192.168.2.12 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms VXLAN 网络 点对点设备只能两两通信，实际用处不大。我们需要组成 vxlan 网络，在 vxlan 网络中有着一个问题：vtep 如何感知彼此的存在并选择正确路径传输报文？从上面的封装的报文中来看，有两个地址在发送时是不确定的： 对方 vtep 的 IP 地址 在 IP 头部，需要的是双方 vtep 的 IP 地址，源地址可以很简单确定，目的地址是要发往的虚拟机所在地址的宿主机的 vtep 的 IP 地址，而我们在发送时只知道对方虚拟机 IP 的地址。 对方虚拟机 MAC 地址 在内部报文中，通信双方是知道对方 IP 地址的，但如果是同一网段的通信，还需要知道对方虚拟机的 MAC 地址。 那么在点对点的 vxlan 设备上为什么没有这个问题呢？ 在点对点的设备中，对方 vtep IP 地址在创建 vxlan 设备由 remote 参数指定。由于是同网段，vxlan 设备将 ARP 请求也发送到了对点的 vtep 上，所以能够直接获得对方的 ARP 响应。 《vxlan 协议原理简介》中提出了两个解决方案：多播和分布式控制中心；多播需要底层网络设备的配合，有一定局限性，而且多播方式会带来报文的浪费，在实际生产中很少用到。而分布式控制的 vxlan 是一种典型的 SDN 架构，也是目前使用最广泛的方式。 分布式控制中心 多播的解决方案是在发送报文前以广播的方式自动学习地址，可是这太浪费了。所以分布式控制中心的解决方案就是提前知道地址信息，直接告诉 vtep，这就不需要多播了。 一般情况下，在每个 vtep 所在的节点都会有一个 agent，它会和控制中心通信，获取 vtep 需要的信息以某种方式告知 vtep。不止告知的方式不同，告知的时间也有区别。一般有两种方式：常见的是一旦知道信息就立刻告知 vtep，即使它可能用不上，一般这时候第一次通信还没有发生；另一种方式是在第一次通信时 vtep 以某种方式通知 agent，然后 agent 才告诉 vtep 这些信息。 ARP 和 FDB 先解释一下 ARP 表和 FDB（二层转发表）表。 ARP 表是由三层设备（路由器，三层交换机，服务器，电脑）用来存储 ip 地址和 mac 地址对应关系的一张表。 FDB 是二层转发表，它是由2层设备（二层交换机）用来存储mac地址和交换机接口地址对应关系的一张表，用于帮助交换机指明 MAC 帧应从哪个端口发出去。Linux vxlan 设备的 FDB 表与上面说的交换机的 FDB 表略有不同，vxlan 设备的 FBD 表保存的是 mac 地址与其他 vxlan 设备的 vtep 地址。 手动维护 FDB 表 在多播中以广播的形式获取宿主机的 IP 地址。如果提前知道目的虚拟机的 MAC 地址和它所在的主机的 IP 地址，可以通过更新 FDB 表项来减少广播报文的数量。这就能解决第一个问题。 $ ip link add type vxlan id 1 dstport 4789 dev eth0 nolearning 添加 nolearning 参数告诉 vtep 不要通过收到的报文来学习 FDB 表项的内容，因为我们会手动维护这个列表。 $ bridge fdb append 4e:3d:fd:29:55:38 dev vxlan0 dst 172.16.3.142 # host1 netns 与宿主机 IP 映射 $ bridge fdb append 46:63:12:3e:fa:da dev vxlan0 dst 172.16.2.21 # host2 netns 与宿主机 IP 映射 通过这个映射表，在发送报文时，vtep 搜索 FDB 表项就知道应该发送到哪个对应的 vtep 上了。需要注意的是，还需要一个默认的表项，以便 vtep 在不知道对应关系时可以通过默认方式发送 ARP 报文去查询对方的 MAC 地址。 $ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 172.16.3.142 $ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 172.16.2.21 手动维护 ARP 表 单独维护 FDB 表并没有作用，因为在不知道对方虚拟机 MAC 地址的情况下还是会广播大量的 ARP 报文。所以 ARP 表也需要手动维护。这能解决第二个问题。 但 ARP 表的维护不同于 FDB 表，因为最终通信的双方是容器。到每个容器里面去更新对应的 ARP 表，是件工作量很大的事情，而且容器的创建和删除还是动态的。Linux 提供了一个解决方案，vtep 可以作为 ARP 代理，回复 ARP 请求，也就是说只要 vtep interface 知道对应的 IP - MAC 关系，在接收到容器发来的 ARP 请求时可以直接做出应答。我们只需要更新 vtep interface 上的 ARP 表项就行了。 $ ip link add type vxlan id 1 dstport 4789 dev eth0 nolearning proxy 添加 proxy 参数告知 vtep 承担 ARP 代理的功能。如果收到 ARP 请求，并且自己知道结果就直接作出应答。 $ ip neigh add 192.168.2.11 lladdr 4e:3d:fd:29:55:38 dev vxlan0 $ ip neigh add 192.168.2.12 lladdr 46:63:12:3e:fa:da dev vxlan0 在要通信的所有节点配置完之后，容器就能相互 ping 通。当容器要访问彼此，并且第一次发送 ARP 请求时，这个请求并不会发送给所有的 vtep，而是由当前的 vtep 作出应答，大大减少了网络上的报文。 $ ip netns exec net0 ping -c1 192.168.2.12 PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data. 64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=1.15 ms --- 192.168.2.12 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms 这里要注意的是前面的示例中 netns 都是在同一网段 192.168.2.0/24，实际的项目会需要更大的网段，而跨网段就需要走网关。 动态维护 ARP 和 FDB 表 尽管通过手动维护 FDB 表和 ARP 表可以避免多余的网络报文，但是还有一个问题：为了能让所有的容器正常工作，所有可能会通信的容器都必须提前添加到 ARP 和 FDB 表项中。但并不是网络上所有的容器都会相互通信，所以添加的有些表项是用不到的。 Linux 提供了一种方法，内核能够动态地通知节点要和哪个容器通信，应用程序可以订阅这些事件，如果内核发现需要的 ARP 或者 FDB 表项不存在，会发送事件给订阅的应用程序，这样应用程序可以从控制中心拿到这些信息来更新表项，做到更精确的控制。 $ ip link add vxlan0 type vxlan id 1 dstport 4789 dev eth0 nolearning proxy l2miss l3miss 这次多了两个参数 l2miss 和 l3miss： l2miss：如果设备找不到 MAC 地址需要的 vtep 地址，就会发送通知事件 l3miss：如果设备找不到 IP 地址需要的 MAC 地址，就会发送通知事件 ip monitor 命令可以监听某个 interface 的事件： $ ip monitor all dev vxlan0 如果从本节点容器 ping 另外一个节点的容器，就先发生 l3 miss： $ ipmonitor all dev vxlan0 [nsid current]miss 10.20.1.3 STALE l3miss 是说这个 IP 地址，vtep 不知道它对应的 MAC 地址，因此要手动添加 ARP 记录： $ ip neigh add 192.168.2.12 lladdr 46:63:12:3e:fa:da dev vxlan0 nud reachable nud reachable 参数代表系统发现其无效一段时间后会自动删除。 添加 ARP 表项后还是不能正常通信，接着会出现 l2miss 的通知事件： $ ip monitor all dev vxlan0 [nsid current]miss lladdr 46:63:12:3e:fa:da STALE 这个事件是说不知道这个容器的 MAC 地址在哪个节点上，所以要手动添加 FDB 记录： $ bridge fdb append 46:63:12:3e:fa:da dev vxlan0 dst 172.16.2.21 Flannel Flannel 是 CoreOS 为 Kubernetes 设计的网络插件，实现简单且容易配置，但社区不怎么活跃，不过用来学习还是很好的。 Some design notes and history Flannel 对于网络的实现有不同的 backend，vxlan 的实现在 backend/vxlan 中， 源码文件 vxlan.go 的注释中记载了一些修改历史： Flannel 的第一个版本，l3miss 学习，通过查找 ARP 表 MAC 完成的。 l2miss 学习，通过获取 VTEP 上的 public ip 完成的。 Flannel 的第二个版本，移除了 l3miss 学习的需求，当远端主机上线，只是直接添加对应的 ARP 表项即可，不用查找学习了。 Flannel的最新版本，移除了 l2miss 学习的需求，不再监听 netlink 消息。 它的工作模式： 创建 vxlan 设备，不再监听任何 l2miss 和 l3miss 事件消息 为远端的子网创建路由 为远端主机创建静态 ARP 表项 创建 FDB 转发表项，包含 VTEP MAC 和远端 Flannel 的 public IP 同一个 VNI 下每一台 Host 主机仅包含 1 route，1 arp entry and 1 FDB entry。 还有一个选项是跳过对位于同一子网的主机使用vxlan，这被称为“directRouting” l2miss 和 l3miss 方案缺陷 每一台 Host 需要配置所有需要互通 Guest 路由，路由记录会膨胀，不适合大型组网 通过 netlink 通知学习路由的效率不高 Flannel Daemon 异常后无法持续维护 ARP 和 FDB 表，从而导致网络不通 在最新的方案中，有选项可以跳过对同一子网上的主机使用vxlan，称为“directRouting（直达路由）”。 源码分析 func main() { // 创建 SubnetManager 用于管理子网。sm 有两种模式，通过 kube-subnet-mgr 划分。kubeSubnetMgr 使用 Kubernetes 管理子网；etcdSubnetMgr 使用 etcd 管理子网。 sm, err := newSubnetManager() if err != nil { log.Error(&quot;Failed to create SubnetManager: &quot;, err) os.Exit(1) } log.Infof(&quot;Created subnet manager: %s&quot;, sm.Name()) // 创建 BackendManager，随后根据类型获取 BackendNetwork，用于在 Node 上创建网络。backends 通过 init 函数在 backend.Register 中注册，BackendManager 通过 GetBackend 获得对应类型的 backend，类型通过 Flannel config 文件 BackendType 字段获取。 // Create a backend manager then use it to create the backend and register the network with it. bm := backend.NewManager(ctx, sm, extIface) be, err := bm.GetBackend(config.BackendType) if err != nil { log.Errorf(&quot;Error fetching backend: %s&quot;, err) cancel() wg.Wait() os.Exit(1) } // 获得 backend 后，使用 RegisterNetwork 方法创建主机网络。 bn, err := be.RegisterNetwork(ctx, &amp;wg, config) if err != nil { log.Errorf(&quot;Error registering network: %s&quot;, err) cancel() wg.Wait() os.Exit(1) } // Start &quot;Running&quot; the backend network. This will block until the context is done so run in another goroutine. log.Info(&quot;Running backend.&quot;) wg.Add(1) go func() { // 监听子网事件，通过 handleSubnetEvents 为主机创建静态路由、ARP表项、FDB表项。 // kubeSubnetManager 在 newKubeSubnetManager 时通过 informer 监听 Node 事件，发送给 events 不同的 object，然后进行处理 bn.Run(ctx) wg.Done() }() daemon.SdNotify(false, &quot;READY=1&quot;) // Kube subnet mgr doesn't lease the subnet for this node - it just uses the podCidr that's already assigned. if !opts.kubeSubnetMgr { err = MonitorLease(ctx, sm, bn, &amp;wg) if err == errInterrupted { // The lease was &quot;revoked&quot; - shut everything down cancel() } } } vxlan.go RegisterNetwork() func (be *VXLANBackend) RegisterNetwork(ctx context.Context, wg *sync.WaitGroup, config *subnet.Config) (backend.Network, error) { // 通过 config 文件中 Backend 字段获取配置。设置 vxlanDeviceAttrs，使用 VNI 作为 name。 devAttrs := vxlanDeviceAttrs{ vni: uint32(cfg.VNI), name: fmt.Sprintf(&quot;flannel.%v&quot;, cfg.VNI), vtepIndex: be.extIface.Iface.Index, vtepAddr: be.extIface.IfaceAddr, vtepPort: cfg.Port, gbp: cfg.GBP, learning: cfg.Learning, } // 使用 vxlanDeviceAttrs 设置 vxlanDevice // newVXLANDevice 函数通过 github.com/vishvananda/netlink 包创建 vxlan 设备，然后设置 net/ipv6/conf/${device_name}/accept_ra 的配置。 dev, err := newVXLANDevice(&amp;devAttrs) if err != nil { return nil, err } dev.directRouting = cfg.DirectRouting // 通过 newSubnetAttrs 函数获取配置，使用 subnetMgr 设置子网并得到 Lease。 subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr()) if err != nil { return nil, err } lease, err := be.subnetMgr.AcquireLease(ctx, subnetAttrs) switch err { case nil: case context.Canceled, context.DeadlineExceeded: return nil, err default: return nil, fmt.Errorf(&quot;failed to acquire lease: %v&quot;, err) } // Ensure that the device has a /32 address so that no broadcast routes are created. // This IP is just used as a source address for host to workload traffic (so // the return path for the traffic has an address on the flannel network to use as the destination) // 配置 vxlan 设备 addr，然后启动设备。设置 vxlan 设备为子网中的 /32 地址 if err := dev.Configure(ip.IP4Net{IP: lease.Subnet.IP, PrefixLen: 32}); err != nil { return nil, fmt.Errorf(&quot;failed to configure interface %s: %s&quot;, dev.link.Attrs().Name, err) } return newNetwork(be.subnetMgr, be.extIface, dev, ip.IP4Net{}, lease) } Linux 实现 flannel 网络 Flannel 网络配置不需要维护过多的表项，在同一个 VNI 下的每台主机仅需要配置一个路由、一个 ARP 表项、一个 FDB 表项。配置的表项变少，解决了手动维护 FDB 表和 ARP 表所带来的过多的无用表项问题，但相应的也会增加报文的发送，这也是 flannel 在实现上的取舍问题。 环境 Flannel 配置： { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } } Flannel 使用 /16 CIDR，为每个节点分配一个 /24 的子网，所以此时的 network namespace 变为： $ ip netns exec net0 ip addr # host1 veth0: inet 10.244.0.2/24 scope global veth0 $ ip netns exec net0 ip addr # host2 veth0: inet 10.244.1.2/24 scope global veth0 因为跨网段，所以为 br0 设置 IP 地址，并修改路由表做为网关： $ ip netns exec net0 ip route add 10.244.0.0/16 via 10.244.0.1 dev veth0 onlink # host1 $ ip addr br0: inet 10.244.0.1/24 scope global br0 $ ip netns exec net0 ip route add 10.244.0.0/16 via 10.244.1.1 dev veth0 onlink # host2 $ ip addr br0: inet 10.244.1.1/24 scope global br0 示例 配置 vxlan 设备： $ ip link add vxlan0 type vxlan id 1 dstport 4789 dev eth0 nolearning $ ip link set up vxlan0 $ ip link # host1 vxlan0: link/ether c2:cb:69:f5:a6:e4 $ ip link # host2 vxlan0: link/ether 66:8e:33:ac:7a:22 设置路由表： $ ip addr add 10.244.0.0 dev vxlan0 # 本机 vxlan IP $ ip route add 10.244.1.0/24 via 10.244.1.0 dev vxlan0 onlink # 在 host1 设置 host2 路由表 # $ ip addr add 10.244.1.0 dev vxlan0 # $ ip route add 10.244.0.0/24 via 10.244.0.0 dev vxlan0 onlink 设置 FDB 表： $ bridge fdb append 66:8e:33:ac:7a:22 dev vxlan0 dst 172.16.2.21 # host2 主机的 vxlan MAC 地址与主机 IP # bridge fdb append c2:cb:69:f5:a6:e4 dev vxlan0 dst 172.16.3.142 设置 ARP 表： $ ip neigh add 10.244.1.0 dev vxlan0 lladdr 66:8e:33:ac:7a:22 # host2 vxlan MAC 与 vxlan IP # $ ip neigh add 10.244.0.0 dev vxlan0 lladdr c2:cb:69:f5:a6:e4 测试 ping： $ ip netns exec net0 ping -c1 10.244.1.2 PING 10.244.1.2 (10.244.1.2) 56(84) bytes of data. 64 bytes from 10.244.1.2: icmp_seq=1 ttl=62 time=1.11 ms --- 10.244.1.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms 如果需要，别忘记设置内核 ip forward 参数： $ sysctl -w net.ipv4.ip_forward=1 总结 Flannel 基于每台节点一个 /24 的网段，大大减少了维护 ARP 和 FDB 表项的工作，所增加的只是数据包达到目的地主机后的少量 ARP 请求，每次容器的增减也不需要触发维护。对比完全手动维护的方案来说，要好得多。 参考文章 vxlan 协议原理简介 linux 上实现 vxlan 网络 为什么 flannel.1 丢失后不会自动重建 ","link":"https://cnbailian.github.io/post/kubernetes-network-notes/"},{"title":"容器网络学习笔记","content":"容器技术涉及的知识点很多，包括进程隔离、容器网络、分层存储等等，我对其中容器网络部分很感兴趣，并有较为深入的学习。此篇文章用于记录我的学习笔记。 概述 提到容器技术，大家可能知道容器通过 Linux Namespace 技术实现资源隔离。Namespace 是 kernel 对全局系统资源的一种封装隔离，比如 PID、User、Network 等等，改变 namespace 中被隔离的系统资源，只会影响当前 namespace 中的进程，对其他 namespace 中的进程没有影响。 Network namespace 就是本文主要涉及的一个 namespace，它被用来隔离网络设备、IP 地址端口等，每个 namespace 都有自己独立的网络协议栈、IP 路由表、防火墙规则、sockets等。 有了不同的 network namespace 之后，也就有了网络隔离，但一个完全被隔离的网络环境没有实际用处，这就需要通过 Linux 的虚拟网络设备为其插上“网卡”，以连通更多的网络。Linux 虚拟网络设备很多，这里主要介绍的是构建容器网络要用到的 Veth 与 Bridge，前者可以连接两个被隔离的 network namespace，后者则可以让更多的 network namespace 加入进来。 Linux Veth Linux 网络设备 Linux 的网络设备就像一个双向的管道，数据从一端进，就会从另一端出，关键要看这两端是什么。用常见的 eth0 举例，eth0 设备的一端连接网络协议栈，另一端连接网卡。用户通过 socket api 调用，经过 Linux 网络协议栈，进入 eth0 网络设备，最后发送到网卡。 +-------------------------------------------+ | | | +-------------------+ | | | User Application | | | +-------------------+ | | | | |.................|.........................| | ↓ | | +----------+ | | | socket | | | +----------+ | | | | |.................|.........................| | ↓ | | +------------------------+ | | | Newwork Protocol Stack | | | +------------------------+ | | | | |.................|.........................| | ↓ | | +----------------+ | | | eth0 | | | +----------------+ | | | | | | | | | | +-----------------|-------------------------+ ↓ Physical Network Veth Pair Veth 作为 Linux 的虚拟网络设备，它总是成对（pair）出现，它的一端连接着网络协议栈，另一端两个设备彼此相连。这个特性使得一个设备收到协议栈的数据请求后，会将数据发送到另一个设备上去。 +----------------------------------------------------------------+ | | | +------------------------------------------------+ | | | Newwork Protocol Stack | | | +------------------------------------------------+ | | ↑ ↑ ↑ | |..............|...............|...............|.................| | ↓ ↓ ↓ | | +----------+ +-----------+ +-----------+ | | | eth0 | | veth0 | | veth1 | | | +----------+ +-----------+ +-----------+ | | ↑ ↑ ↑ | | | +---------------+ | | | 192.168.2.11 192.168.2.1 | +--------------|-------------------------------------------------+ ↓ Physical Network 可以通过这个特性，实现两个 network namespace 网络的互通。 示例 通过示例创建 network namespace 与 veth pair，并实现网络互通。 # 创建 network namespace root@ubuntu:~$ ip netns add net0 root@ubuntu:~$ ip netns add net1 # 创建 veth pair # 因为未指定名称，会默认生成 veth0 和 veth1，如果有其他 veth 设备序号会顺延 # 如果想指定名字：ip link add vethfoo type veth peer name vethbar root@ubuntu:~$ ip link add type veth # 将 veth0 设备转给 net0 namespace root@ubuntu:~$ ip link set dev veth0 netns net0 # 将 veth1 设备转给 net1 namespace root@ubuntu:~$ ip link set dev veth1 netns net1 # 分别设置设备 IP # ip netns exec 命令是进入 network namespace 内执行指令 root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0 root@ubuntu:~$ ip netns exec net1 ip addr add 192.168.2.1/24 dev veth1 # 启动 veth pair root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up root@ubuntu:~$ ip netns exec net1 ip link set dev veth1 up # 尝试 ping root@ubuntu:~$ ip netns exec net0 ping -c1 192.168.2.1 PING 192.168.2.1 (192.168.2.1) from 192.168.2.11 veth0: 56(84) bytes of data. 64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=0.032 ms --- 192.168.2.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms Network namespace 多记一些 network namespace 相关的知识点。 每个新的 network namespace 创建之后默认会有一个 lo 设备，除此之外的其他网络设备就需要创建或移动过来。注意 lo 设备默认是关闭的，需要自己手动启动。 root@ubuntu:~$ ip netns add net0 root@ubuntu:~$ ip netns exec net0 ip link lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 上面的示例中将 veth pair 设备分别给了两个 namespace，但被标记为“local device”的设备不能被移动，比如 loopback、bridge、ppp 等。可以通过 ethtool -k 命令查看设备的 netns-local 属性： root@ubuntu:~$ ethtool -k lo|grep netns-local netns-local: on [fixed] root@ubuntu:~$ ethtool -k veth0|grep netns-local netns-local: off [fixed] Linux Bridge 虽然 veth pair 可以实现两个 network namespace 之间的通信，但是当多个 namespace 需要通信的时候，就需要 bridge 了。bridge 同样是 Linux 虚拟网络设备，具有网络设备的特征，可以配置 IP、MAC 地址等，但 bridge 同时也是一个虚拟交换机，和物理交换机有类似的功能。 对于普通的网络设备来说，只有两个端口，从一端进来的数据会从另一端出去。而 bridge 不同，bridge 有多个端口，数据可以从任何端口进来，进来之后从哪个端口出去和物理交换机的原理差不多，要看 MAC 地址。 所以，要想实现多 network namespace 的网络通信，就需要 bridge 这个虚拟交换机。 使用 bridge 连接不同的 namespace 首先创建并启动 bridge，将其取名为 br0： root@ubuntu:~$ ip link add name br0 type bridge root@ubuntu:~$ ip link set br0 up root@ubuntu:~$ ip link br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 9a:a8:84:37:d4:56 brd ff:ff:ff:ff:ff:ff 同样，network namespace 也要准备好： root@ubuntu:~$ ip netns add net0 root@ubuntu:~$ ip netns add net1 示例 现在两个网络环境与虚拟交换机都已准备好，接下来将使用 veth pair 进行连接互通： # 创建 net0 使用的 veth pair root@ubuntu:~$ ip link add type veth # 将 veth0 移至 net0 root@ubuntu:~$ ip link set dev veth0 netns net0 # 设置 IP 并启动 root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0 root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up # 将其对应的另一个设备 attach 到 bridge 上并启动 root@ubuntu:~$ ip link set dev veth1 master br0 root@ubuntu:~$ ip link set dev veth1 up # net1 同理 root@ubuntu:~$ ip link add type veth root@ubuntu:~$ ip link set dev veth0 netns net1 root@ubuntu:~$ ip netns exec net1 ip addr add 192.168.2.1/24 dev veth0 root@ubuntu:~$ ip netns exec net1 ip link set dev veth0 up root@ubuntu:~$ ip link set dev veth2 master br0 root@ubuntu:~$ ip link set dev veth2 up 测试 ping： root@ubuntu:~$ ip netns exec net0 ping -c1 192.168.2.1 PING 192.168.2.1 (192.168.2.1) 56(84) bytes of data. 64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=0.045 ms --- 192.168.2.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms Veth pair 在此时的作用就相当于网线，一头（veth0）连着容器（network namespace），另一头（veth1）连着交换机（bridge）。bridge 作为交换机，当有设备 attach 到 bridge，就相当于交换机上插了一个新网线。当有请求到达 bridge 设备时，就可以通过报文中的 MAC 地址进行广播、转发、丢弃处理。 给 bridge 配上 IP Bridge 与现实世界的二层交换机有一个区别：数据可以直接被发到 bridge 上，而不是从一个端口接受。这种情况可以看做 bridge 自己有一个 MAC 可以主动发送报文，或者说 bridge 自带了一个隐藏端口和寄主 Linux 系统自动连接，Linux 上的程序可以直接从这个端口向 bridge 上的其他端口发数据。 由此带来一个有意思的事情是，bridge 可以设置 IP 地址。通常来讲 IP 地址是三层协议的内容，不应该出现在二层设备 bridge 上，但 bridge 是虚拟交换机，属于通用网络设备的抽象的一种，只要是网络设备就能够设定 IP 地址。 当一个 bridge 拥有 IP 后，Linux 便可以通过路由表或者 IP 表规则在三层定位 bridge，此时相当于 Linux 拥有了另外一个隐藏的虚拟网卡和 bridge 的隐藏端口相连，这个网卡就是名为 br0 的通用网络设备，IP 可以看成是这个网卡的。当有符合此 IP 的数据到达 br0 时，内核协议栈认为收到了一包目标为本机的数据，此时应用程序可以通过 socket 接收到它。 示例 接上文环境，为 bridge 配置 IP： root@ubuntu:~$ ip addr add 192.168.2.12/24 dev br0 在主机上尝试 ping net0： root@ubuntu:~$ ping -I br0 -c1 192.168.2.11 PING 192.168.2.11 (192.168.2.11) from 192.168.2.12 br0: 56(84) bytes of data. 64 bytes from 192.168.2.11: icmp_seq=1 ttl=64 time=0.057 ms --- 192.168.2.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms 在 net1 中尝试 ping br0: root@ubuntu:~$ ip netns exec net1 ping -c1 192.168.2.12 PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data. 64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=0.061 ms --- 192.168.2.12 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms 与外部网络通信 上文给 bridge 配置 IP 后，network namespace 已经可以通过 br0 与宿主机的网络协议栈通信，但我们还需要与外部的网络通信。 其中的一种方法是将物理网卡设备 eth0 也 attach 到 br0 上。br0 根本不区分 attach 的是物理设备还是虚拟设备，对它来说都一样，都是网络设备，这就相当于 br0 拥有了一条连接外部物理设备的网线。此时连接到 br0 的 network namespace 都可以通过 br0 访问外部网络。但由于我是使用的云主机，通过 ssh 连接，无法很方便的调试，所以没有试过这种方法。 上一种方法不需要经过宿主机网络协议栈，直接就可以通过 eth0 设备发送数据。而第二种方法，可以不接入 eth0 设备，而是通过 IP forward 将数据转发。同时由于 network namespace 是分配的内网 IP，所以一般在发出去之前还需要经过 NAT 转换。 IP forward “IP forwarding” 和 “routing” 是同义词，因为属于 Linux 内核的特性，所以也被叫做 “kernel IP forwarding”。所谓转发的概念就是 Linux 内核实现了路由器的功能，根据数据包的 IP 地址将数据从一个网络发送到另一个网络，该网络根据路由表配置继续发送数据包。 出于安全考虑，Linux 默认是禁止数据包转发的。如果想要启用，需要修改内核参数 net.ipv4.ip_forward。这个参数的值指定了是否启用转发功能；为 0 时禁用，为 1 时表示启用。 root@ubuntu:~$ sysctl net.ipv4.ip_forward net.ipv4.ip_forward = 0 # 也可以通过 /proc 查看 root@ubuntu:~$ cat /proc/sys/net/ipv4/ip_forward 0 修改内核参数 临时生效 root@ubuntu:~$ sysctl -w net.ipv4.ip_forward=1 net.ipv4.ip_forward = 1 # 或直接修改 /proc 文件 root@ubuntu:~$ echo 1 &gt; /proc/sys/net/ipv4/ip_forward 永久生效 修改 sysctl.conf 文件，找到 net.ipv4.ip_forward 配置项，修改为 1： root@ubuntu:~$ vi /etc/sysctl.conf # 需要在当前环境中刷新更改 root@ubuntu:~$ sysctl -p /etc/sysctl.conf NAT 网络地址转换 NAT（Network Address Translation）的作用是将数据包中的 network namespace 内网 IP 转为主机所拥有的公网 IP。 NAT 根据数据流向可以分为两种：SNAT 是源 IP 转换，将发送的数据包中的源 IP 转为公网 IP；DNAT 是目标 IP 转换，将接收到的数据包中的公网 IP 转为 network namespace 的内网 IP。 netfilter/iptables 无论是 IP forward 还是 NAT，在 Linux 系统上都可以通过 netfilter/iptables 配置规则。netfilter 和 iptables 可以拆开来说，netfilter 指的是整个项目，在这个项目中 netfilter 特指内核中的 netfilter 框架，而我们更熟悉的 iptables 则是用户空间的配置工具，用于与 netfilter 框架打交道。 netfilter 框架 netfilter 在内核协议栈的 IP 层添加了几个钩子（hooks），允许内核模块在这些钩子的地方注册回调函数，这样经过钩子的所有数据包都会被注册在相应钩子上的函数所处理，包括修改数据包内容或者丢弃数据包等等。 netfilter 框架负责维护钩子上注册的处理函数或者模块，以及它们的优先级。 iptables iptables 是用户空间的一个程序，与内核的 neifilter 框架打交道，根据规则在钩子上配置回调函数。 iptables 用表（table）来分类管理它的规则（rule），根据 rule 的作用可以分类为几个表，比如用于过滤数据的 filter 表，用于处理 NAT 规则的 nat 表等等。 conntrack onntrack 是 netfilter 实现 NAT 的基础，当加载内核模块 nf_conntrack 后，connection tracking 机制就开始工作，它工作在 NF_IP_PRE_ROUTING 和 NF_IP_LOCAL_OUT 这两个钩子处。它会追踪每个数据包（被 raw 表中的 rule 标记过的除外），并生成 conntrack 条目用于追踪此连接，对于后续通过的数据包，内核会判断若此数据包属于某个连接，则会更新对应的 conntrack 条目。 所有的 conntrack 条目都存放在一张表里，称为连接跟踪表。可以用 cat /proc/net/nf_conntrack 来查看当前的所有连接。下面是所有的连接状态： NEW：当检测到一个不和任何现有连接关联的新包时，如果该包是一个合法的建立连接的数据包，一个新的连接将会被保存，并且标记为状态 NEW。 ESTABLISHED：对于状态是 NEW 的连接，当检测到一个相反方向的包时，连接的状态将会由 NEW 变成 ESTABLISHED，表示连接成功建立。对于TCP连接，意味着收到了一个 SYN/ACK 包， 对于 UDP 和 ICMP，任何反方向的包都可以。 RELATED：数据包不属于任何现有的连接，但它跟现有的状态为 ESTABLISHED 的连接有关系，对于这种数据包，将会创建一个新的连接，且状态被标记为 RELATED。这种连接一般是辅助连接，比如 FTP 的数据传输连接（FTP 有两个连接，另一个是控制连接），或者和某些连接有关的ICMP报文。 INVALID：数据包不和任何现有连接关联，并且不是一个合法的建立连接的数据包，对于这种连接，将会被标记为 INVALID，一般这种都是垃圾数据包，比如收到一个 TCP 的 RST 包，但实际上没有任何相关的 TCP 连接，或者别的地方误发过来的 ICMP 包。 UNTRACKED：被 raw 表里面的 rule 标记为不需要 tracking 的数据包，这种连接将会标记成 UNTRACKED。 示例 创建 bridge，并配置 IP： root@ubuntu:~$ ip link add br0 type bridge root@ubuntu:~$ ip link set dev br0 up root@ubuntu:~$ ip addr add 192.168.2.1/24 dev br0 创建 network namespace 并与 bridge 相连： root@ubuntu:~$ ip netns add net0 root@ubuntu:~$ ip link add type veth root@ubuntu:~$ ip link set veth0 netns net0 root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up root@ubuntu:~$ ip link set veth1 up root@ubuntu:~$ ip link set veth1 master br0 root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0 修改 net0 路由表，默认网关设置为 br0： root@ubuntu:~$ ip netns exec net0 ip route add 0.0.0.0/0 via 192.168.2.1 dev veth0 onlink 注意 IP forward 配置： root@ubuntu:~$ sysctl -w net.ipv4.ip_forward=1 net.ipv4.ip_forward = 1 屏蔽环境干扰，先默认不允许转发： root@ubuntu:~$ iptables -P FORWARD DROP 开始配置 iptables rules，首先设置 bridge 转发规则，此条规则的意思是允许 br0 转发给 eth0： root@ubuntu:~$ iptables -A FORWARD -i br0 -o eth0 -j ACCEPT 接下来配置 SNAT 规则： root@ubuntu:~$ iptables -t nat -A POSTROUTING -s 192.168.2.0/24 -j SNAT --to # to eth0 ip # 也可以直接配置在 eth0 上 root@ubuntu:~$ # iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE netfilter 通过 conntrack 来实现 NAT 转换，所以我们要对 RELATED,ESTABLISHED 状态的包予以通行： root@ubuntu:~$ iptables -A FORWARD -o br0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 通过上面的配置，conntrack 状态监测到是回包的数据包，都给予通行，而后回包经过 conntrack 表会变为原始 IP 关系，相当于 DNAT 转换。 在 network namespace 中使用 ping 来测试访问外部网络： root@ubuntu:~$ ip netns exec net0 ping -c1 110.242.68.4 # 百度的一个 IP PING 110.242.68.4 (110.242.68.4) 56(84) bytes of data. 64 bytes from 110.242.68.4: icmp_seq=1 ttl=34 time=56.7 ms --- 110.242.68.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms 端口转发 上面的示例是从 network namespace 内部访问外部网络，可以利用 conntrack 来替代 DNAT，如果想让外部请求访问内部服务，就需要配置 DNAT 的映射规则。可映射是一对一的，一个宿主机 IP 对应一个 network namespace 的内网 IP，当我们有多个内部服务想要暴露给公网，就需要配置 NAPT 规则。 NAPT 网络地址与端口号转换 NAPT (Network Address andPort Translation) 就是使用端口号的 NAT，有端口号的配置，就能实现内网 IP 的多对一映射，只是映射到不同的端口上。 内网 IP 公网 IP 192.168.2.11:80 x.x.x.x:8080 192.168.2.1:80 x.x.x.x:8081 示例 除 iptables rules 外规则不变，首先是在 network namespace 中启动一个 http server： # 注意：这会暴露当前目录下的文件 root@ubuntu:~$ ip netns exec net0 python -m SimpleHTTPServer 80 添加 DNAT 规则，设置主机端口为 8080，映射 net0 的 80： root@ubuntu:~$ iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to 192.168.2.11:80 添加 ip forward： root@ubuntu:~$ iptables -A FORWARD -i eth0 -d 192.168.2.0/24 -o br0 -p tcp --dport 80 -j ACCEPT 现在就可以通过宿主机的 IP 访问了。 写在最后 上述例子用于学习需要，与真实的容器配置不同，但所用的基础技术都是一样的。笔记内容主要学习和参考自 Segmentfault 用户 public0821 的 Linux 专栏文章，还有网络上的一些相关文章。 接下来会继续学习跨主机的容器网络搭建，这次会结合实际项目 Flannel。 参考文章 Linux虚拟网络设备之veth Linux虚拟网络设备之bridge(桥) netfilter/iptables简介 通过iptables实现端口转发和内网共享上网 ","link":"https://cnbailian.github.io/post/container-netwrok-notes/"},{"title":"记一次 Traefik 无法代理 MySQL 问题","content":"Traefik 从 2.0 版本开始支持 TCP route，我也使用 Traefik 作为 kubernetes 集群的 Ingress，但是在使用过程中，发现 Traefik 为 MySQL 创建的 TCP route 无法正常工作，经过排查搜索后发现了官方人员关于这个疑惑的解答，以下截取片段： But be careful: not all protocols based on TCP and using TLS supports the SNI routing or the passthrough. It requires the protocol supporting SNI (for instance MySQL doesn't) and doing a TLS handshake (if it is a STARTTLS, then it does not work). 虽然找到了问题是由于 MySQL 不支持，但也勾起了我的好奇心，什么是 SNI？Traefik 为什么要使用 HostSNI 创建 TCP route 呢？为什么 MySQL 不支持 SNI 呢？于是带着这些问题，我开始寻找答案。 TLS Extensions —— SNI 首先从了解 SNI 开始，SNI 是 TLS 的一个扩展协议。 什么是 TLS Extensions？ TLS 扩展于 2003 年以一个独立的规范（RFC 3546）被提出，经过不断的发展：RFC 4366、RFC 6066 等，先后被加入到 TLS1.1、TLS1.2、TLS1.3 中。它能让 Client 和 Server 在不更新 TLS 的基础上，获得新的功能。 Client 在 ClientHello 中声明多个自己可以支持的 Extensions，Server 收到 ClientHello 以后，依次解析 Extensions，有些如果需要立即回应，就在 ServerHello 中作出回应，有些不需要回应，或者 Server 不支持的 Extensions 就不用响应，忽略不处理。 在 ClientHello 中，Extension 字段位于 Compression Methods 字段之后，通过 Wireshark 工具进行查看： 什么是 SNI 扩展？ 我们知道，在 Nginx 中可以通过指定不同的 server_name 来配置多个站点。HTTP/1.1 协议请求头中的 Host 字段可以标识出当前请求属于哪个站点。但是在 TLS 协议中，没有提供一种机制来告诉 Server 它正在建立连接的 Server 的名称，那么对于在同一个地址，并且还使用不同证书的情况下，Server 怎么知道该发送哪个证书？ 于是为了解决这个问题，SNI 应运而生。SNI 全称是 Server Name Indication，最初是 2003 年标准化的，在 RFC 6066 中有更新。它允许 Server 在同一个网络地址上托管多个启用了 TLS 的服务，要求 Client 在初始 TLS 握手期间指定要连接到哪个服务。 struct { NameType name_type; select (name_type) { case host_name: HostName; } name; } ServerName; enum { host_name(0), (255) } NameType; opaque HostName&lt;1..2^16-1&gt;; struct { ServerName server_name_list&lt;1..2^16-1&gt; } ServerNameList; Extension type 是 server_name，点开上图 Wireshark 中 server_name 一行，查看更详细信息： ServerNameList 不能包含多个具有相同 ServerNameType 的名称，当前 ServernameType 只有 host_name 一种，在以后可能会添加更多类型，host_name 包含标准的 DNS hostname 且不含结尾点。如果 Server 支持 SNI 扩展，但不能识别 server_name，则应该发送 fatal-level unrecognized_name(112) 来终止握手或继续握手。 更多详细的规范内容可以到 RFC 6066 中查看。这里 有一个扩展协议列表。 Traefik 的 TCP 路由与 SNI Traefik 从 2.0 开始支持 TCP 路由，也支持在相同的 entryPoints（traefik 中的入口端口） 中定义不同的 TCP 路由，但是我们都知道，TCP 是传输层协议，没有任何 SNI 类的机制来保证同一地址入口可以处理不同的服务。那么，Traefik 是怎么做的呢？ 部署基于 TLS 的 TCP 路由 答案很简单，Traefik 支持通过 SNI 在每台主机上进行路由，因为这是通过 TCP 进行路由的惟一标准方法，但是 TCP 本身没有 SNI，因此必须使用 TLS。部署配置： apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: example spec: entryPoints: - web routes: - match: HostSNI(`web.example.com`) services: - name: example-service-name port: 80 tls: secretName: traefik-tls-certs HostSNI 中的值对应 SNI 扩展中 server_name 的值，Traefik 以此来进行路由，并找到对应证书。还需要注意的是 entryPoints 部分由部署的 Traefik 配置中的 entryPoints 参数决定，此处的 web 是我们指定的一个 entryPoints 名称，端口地址对应为 80 端口： ...... - image: traefik:2.1.1 name: traefik ports: - name: web containerPort: 80 hostPort: 80 args: - --entryPoints.web.address=:80 ...... 此处使用 hostPort 的方式暴露入口点，是为了能够通过 Traefik 部署的节点的入口点端口直接访问到 backend service。 部署非 TLS 的 TCP 路由 如果有不支持 SNI/TLS 协议的应用客户端，Traefik 也可以部署 “plain TCP”，也就是标准的通过端口进行路由。此时虽然 metch 还是使用 HostSNI，但需要指定为通配符 *： apiVersion: traefik.containo.us/v1alpha1 kind: IngressRouteTCP metadata: name: example spec: entryPoints: - web routes: - match: HostSNI(`*`) services: - name: example-service-name port: 80 其他 使用 Traefik 代理 TLS 服务时，backend service 可不设置 TLS 相关，由 Traefik 负责全部相关机制。如果 backend service 有需要加密后的数据时，可通过 passthrough 参数配置，Traefik 将发送加密后的数据给 backend service： ...... tls: secretName: traefik-tls-certs passthrough: true 为什么不能为 MySQL 代理 当我明白 SNI 协议以及 Traefik 如何使用 SNI/TLS 为 TCP 创建路由时，我开始研究为什么 MySQL 不能使用 SNI 扩展，甚至在 2016 年就有人提出过这个问题，但可惜一直没有人跟进：https://bugs.mysql.com/bug.php?id=82872。这让我有些疑惑，毕竟 MySQL 已经实现了 TLS 功能，为什么在有用户有需求的情况下不加上 SNI 扩展呢？毕竟这又不是过于复杂的功能。 在寻找到答案之前，让我们先简单复习下 TLS 协议的标准流程：首先是 TCP 的三次握手，随后开始 TLS 的握手，如果是 TLS1.2 或之前需要四次握手，如果是 TLS1.3 则需要三次握手，最后开始传输加密数据。 下面来看看 MySQL 的流程，输入命令：mysql -hmysql.example.com -P3306 -uroot -pmysql --ssl-mode=REQUIRED，使用 wireshark 查看： MySQL 对于 TCP 连接已经默认使用 tls，如果不想使用需要修改参数为 --ssl-mode=DISABLED，同时对于 localhost 默认使用 soket 连接，强制使用 TCP 连接需要增加参数: --protocol tcp。 上图中可以看出，在 TCP 握手后，Server 会发送 MySQL 协议 HandShake Paket：Server Greeting proto=10 version=5.7.29，开始 MySQL 协议的握手流程，随后 Client 发送 Auth Paket，图中为开启 TLS 认证的流程，所以并未显示 user 的内容，如果设置 MySQL Client 参数为 --ssl-mode=DISABLED，将显示认证的用户名，并且 Server 会在随后发送 Auth Switch Request 包继续认证流程，此处不再赘述，有兴趣的可以自己抓包看一下。 看到这里其实就已经很清晰了，MySQL 在连接时会将自定义协议握手流程置于 TLS 协议握手之前，以至于 Traefik 无法通过 TLS SNI 找到对应 backend service，也就无法发送 MySQL 的 HandShake Paket。对于 MySQL Client 来说，如果是有超时机制，会响应 waiting for initial communication packet 或类似的错误，如果没有超时机制，就会一直等待。 这点对于 Traefik 来说也很无奈，MySQL 自定义协议中也没有 SNI 的机制，而 TLS 又在 MySQL 协议握手之后发生，导致它完全没办法进行路由，只好期望 MySQL 能尽快修改这部分的流程。这里有官方对于这件事的一些回复：https://github.com/containous/traefik/issues/5155 其他常见数据库 了解到了 MySQL 的问题，不禁让我好奇，其他的常见数据库是否也拥有相同问题，于是我又去看了 MongoDB 和 Redis。 MongoDB 使用命令进行连接：mongo --host mongo.example.com --port 27017 --ssl 非常标准的流程，也支持 SNI 扩展，Traefik 可以顺利的进行路由。 Redis Redis 从 6.0 开始支持 SSL/TLS，但 6.0 正在处于 RC（Release Candidate） 阶段，如果想要测试，可以下载代码后自行编译。TLS 特性是个可选特性，需要在编译时使用参数确认使用：make BUILD_TLS=yes。 相关官方文档：https://redis.io/topics/encryption 编译后尝试连接 Traefik 代理的地址：./redis-cli --tls -h testtcp.ohuna.cloud -p 6379，却发现 Traefik 响应了 fatal level error： Unknown CA： 很明显是因为 redis 没有使用 SNI 扩展，但文档中又没有提及，所以我去 redis 源码中寻找答案。在 tls.h 中了解到 redis 使用了 openssl： ...... #ifdef USE_OPENSSL #include &lt;openssl/ssl.h&gt; #include &lt;openssl/err.h&gt; #include &lt;openssl/rand.h&gt; 于是通过 openssl 设置 SNI 的函数 SSL_set_tlsext_host_name 进行查找： #redis-cli.c if (config.sni &amp;&amp; !SSL_set_tlsext_host_name(ssl, config.sni)) { *err = &quot;Failed to configure SNI&quot;; SSL_free(ssl); return REDIS_ERR; } ...... #ifdef USE_OPENSSL } else if (!strcmp(argv[i],&quot;--tls&quot;)) { config.tls = 1; } else if (!strcmp(argv[i],&quot;--sni&quot;) &amp;&amp; !lastarg) { config.sni = argv[++i]; ...... 发现可以通过 --sni 参数进行指定，通过 redis-cli --help 能查看到相关说明： redis-cli 5.9.103 Usage: redis-cli [OPTIONS] [cmd [arg [arg ...]]] ...... --tls Establish a secure TLS connection. --sni &lt;host&gt; Server name indication for TLS. 由于粗心大意，导致耽误了时间去寻找 SNI 的设置方法，不过 redis 需要必须手动设置 SNI 的方式也是很奇怪。重新使用带有 --sni 参数的命令进行连接：./redis-cli --tls -h redis.example.com -p 6379 --sni redis.example.com，这次成功连接，查看 TLS ClientHello 中也带有 server_name： 扩展阅读——ESNI 虽然关于 Traefik 与 MySQL 的问题告一段落，但 SNI 本身还有其他可学习的内容。 SNI 的安全问题 由于 SNI 扩展是在 TLS 握手期间通过 ClientHello 进行发送，在此时 Client 和 Server 还未共享加密密钥，因此 ClientHello 消息未被加密发送。这就意味着如果有中间人，是可以拦截明文的 ClientHello 消息，并知道 Client 将要访问的网址。 ESNI 当前有一项草案正在试图解决这个问题，也就是 ESNI（Encrypted Server Name Indication）。 对于加密 SNI 内容这种先有鸡还是先有蛋的问题，ESNI 通过引入 DNS 来解决。服务器在已知的 DNS 记录上发布一个公钥，客户端可以在连接 Server 之前获得该公钥。然后，客户端将 ClientHello 中的 SNI 扩展替换为 ESNI，也就是使用获得的公钥对 SNI 信息对称加密。 ESNI 必须要基于 TLS1.3 版本，因为 TLS1.3 使用了 Deffie-Hellman 算法进行密钥交换，DH 算法可以使通信的双方能在非安全的信道中安全的交换密钥。否则，就算加密了 SNI，也可以通过明文证书进行验证。 如果仅仅使用 DNS 也不行，因为 DNS 默认是为加密的，所以需要使用的 DNS 支持 DNS over TLS（DoT）或 DNS over HTTPS（DoH）特性。 简单的学习下 ESNI，更多详细内容可以通过 Cloudflare 的文章或草案进行了解。 参考和致谢 学习过程中碰到了诸多问题，幸好互联网上有着众多的学习资料，感谢以下文档与博客： 一文搞懂 Traefik2.1 的使用 HTTPS 交互过程分析 关于启用 HTTPS 的一些经验分享（二） HTTPS 温故知新（六） —— TLS 中的 Extensions RFC 6066 实现自己的数据库驱动——WireShark分析MySQL网络协议中的数据包（二） 不加密，无隐私：加密SNI工作原理 ","link":"https://cnbailian.github.io/post/traefik-cannot-proxy-mysql/"},{"title":"Kubernetes Cluster Autoscaler","content":"当我们使用 Kubernetes 部署应用后，会发现如果用户增长速度超过预期，以至于计算资源不够时，你会怎么做呢？Kubernetes 给出的解决方案就是：自动伸缩（auto-scaling），通过自动伸缩组件之间的配合，可以 7*24 小时的监控着你的集群，动态变化负载，以适应你的用户需求。 自动伸缩组件 水平自动伸缩（Horizontal Pod Autoscaler，HPA） HPA 可以基于实时的 CPU 利用率自动伸缩 Replication Controller、Deployment 和 Replica Set 中的 Pod 数量。也可以通过搭配 Metrics Server 基于其他的度量指标。 垂直自动伸缩（Vertical Pod Autoscaler，VPA） VPA 可以基于 Pod 的使用资源来自动设置 Pod 所需资源并且能够在运行时自动调整资源。 集群自动伸缩（Cluster Autoscaler，CA） CA 是一个可以自动伸缩集群 Node 的组件。如果集群中有未被调度的 Pod，它将会自动扩展 Node 来使 Pod 可用，或是在发现集群中的 Node 资源使用率过低时，删除 Node 来节约资源。 插件伸缩（Addon Resizer） 这是一个小插件，它以 Sidecar 的形式来垂直伸缩与自己同一个部署中的另一个容器，目前唯一的策略就是根据集群中节点的数量来进行线性扩展。通常与 Metrics Server 配合使用，以保证其可以负担不断扩大的整个集群的 metrics API 服务。 通过 HPA 伸缩无状态应用，VPA 伸缩有状态应用，CA 保证计算资源，它们的配合使用，构成了一个完整的自动伸缩解决方案。 Cluster Autoscaler 详细介绍 上面介绍的四个组件中，HPA 是在 kubernetes 代码仓库中的，随着 kubernetes 的版本进行更新发布，不需要部署，可以直接使用。其他的三个组件都在官方社区维护的仓库中，Cluster Autoscaler 的 v1.0(GA) 版本已经随着 kubernetes 1.8 一起发布，剩下两个则还是 beta 版本。 部署 Cluster Autoscaler 通常需要搭配云厂商使用，它提供了 Cloud Provider 接口供各个云厂商接入，云厂商通过伸缩组（Scaling Group）或节点池（Node Pool）的功能对 ECS 类产品节点进行增加删除等操作。 目前（v1.18.1）已接入的云厂商： **Alicloud：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md **Aws：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md **Azure：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md **Baiducloud：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/baiducloud/README.md **Digitalocean：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/digitalocean/README.md **GoogleCloud GCE：**https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#upgrading-google-compute-engine-clusters **GoogleCloud GKE：**https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler **OpenStack Magnum：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/magnum/README.md **Packet：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/packet/README.md 启动参数列表：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca 工作原理 Cluster Autoscaler 抽象出了一个 NodeGroup 的概念，与之对应的是云厂商的伸缩组服务。Cluster Autoscaler 通过 CloudProvider 提供的 NodeGroup 计算集群内节点资源，以此来进行伸缩。 在启动后，Cluster Autoscaler 会定期（默认 10s）检查未调度的 Pod 和 Node 的资源使用情况，并进行相应的 Scale UP 和 Scale Down 操作。 Scale UP 当 Cluster Autoscaler 发现有 Pod 由于资源不足而无法调度时，就会通过调用 Scale UP 执行扩容操作。 在 Scale UP 中会只会计算在 NodeGroup 中存在的 Node，我们可以将 Worker Node 统一交由伸缩组进行管理。并且由于伸缩组非同步加入的特性，也会考虑到 Upcoming Node。 为了业务需要，集群中可能会有不同规格的 Node，我们可以创建多个 NodeGroup，在扩容时会根据 --expander 选项配置指定的策略，选择一个扩容的节点组，支持如下五种策略： **random：**随机选择一个 NodeGroup。如果未指定，则默认为此策略。 **most-pods：**选择能够调度最多 Pod 的 NodeGroup，比如有的 Pod 未调度是因为 nodeSelector，此策略会优先选择能满足的 NodeGroup 来保证大多数的 Pod 可以被调度。 **least-waste：**为避免浪费，此策略会优先选择能满足 Pod 需求资源的最小资源类型的 NodeGroup。 **price：**根据 CloudProvider 提供的价格模型，选择最省钱的 NodeGroup。 **priority：**通过配置优先级来进行选择，用起来比较麻烦，需要额外的配置，可以看文档。 如果有需要，也可以平衡相似 NodeGroup 中的 Node 数量，避免 NodeGroup 达到 MaxSize 而导致无法加入新 Node。通过 --balance-similar-node-groups 选项配置，默认为 false。 再经过一系列的操作后，最终计算出要扩容的 Node 数量及 NodeGroup，使用 CloudProvider 执行 IncreaseSize 操作，增加云厂商的伸缩组大小，从而完成扩容操作。 文字表达能力不足，如果有不清晰的地方，可以参考下面的 ScaleUP 源码解析。 Scale Down 缩容是一个可选的功能，通过 --scale-down-enabled 选项配置，默认为 true。 在 Cluster Autoscaler 监控 Node 资源时，如果发现有 Node 满足以下三个条件时，就会标记这个 Node 为 unneeded： Node 上运行的所有的 Pod 的 Cpu 和内存之和小于该 Node 可分配容量的 50%。可通过 --scale-down-utilization-threshold 选项改变这个配置。 Node 上所有的 Pod 都可以被调度到其他节点。 Node 没有表示不可缩容的 annotaition。 如果一个 Node 被标记为 unneeded 超过 10 分钟（可通过 --scale-down-unneeded-time 选项配置），则使用 CloudProvider 执行 DeleteNodes 操作将其删除。一次最多删除一个 unneeded Node，但空 Node 可以批量删除，每次最多删除 10 个（通过 ----max-empty-bulk-delete 选项配置）。 实际上并不是只有这一个判定条件，还会有其他的条件来阻止删除这个 Node，比如 NodeGroup 已达到 MinSize，或在过去的 10 分钟内有过一次 Scale UP 操作（通过 --scale-down-delay-after-add 选项配置）等等，更详细可查看文档。 Cluster Autoscaler 的工作机制很复杂，但其中大部分都能通过 flags 进行配置，如果有需要，请详细阅读文档：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md 如何实现 CloudProvider 如果使用上述中已实现接入的云厂商，只需要通过 --cloud-provider 选项指定来自哪个云厂商就可以，如果想要对接自己的 IaaS 或有特定的业务逻辑，就需要自己实现 CloudProvider Interface 与 NodeGroupInterface。并将其注册到 builder 中，用于通过 --cloud-provider 参数指定。 builder 在 cloudprovider/builder 中的 builder_all.go 中注册，也可以在其中新建一个自己的 build，通过 go 文件的 +build 编译参数来指定使用的 CloudProvider。 CloudProvider 接口与 NodeGroup 接口在 cloud_provider.go 中定义，其中需要注意的是 Refresh 方法，它会在每一次循环（默认 10 秒）的开始时调用，可在此时请求接口并刷新 NodeGroup 状态，通常的做法是增加一个 manager 用于管理状态。有不理解的部分可参考其他 CloudProvider 的实现。 type CloudProvider interface { // Name returns name of the cloud provider. Name() string // NodeGroups returns all node groups configured for this cloud provider. // 会在一此循环中多次调用此方法，所以不适合每次都请求云厂商服务，可以在 Refresh 时存储状态 NodeGroups() []NodeGroup // NodeGroupForNode returns the node group for the given node, nil if the node // should not be processed by cluster autoscaler, or non-nil error if such // occurred. Must be implemented. // 同上 NodeGroupForNode(*apiv1.Node) (NodeGroup, error) // Pricing returns pricing model for this cloud provider or error if not available. // Implementation optional. // 如果不使用 price expander 就可以不实现此方法 Pricing() (PricingModel, errors.AutoscalerError) // GetAvailableMachineTypes get all machine types that can be requested from the cloud provider. // Implementation optional. // 没用，不需要实现 GetAvailableMachineTypes() ([]string, error) // NewNodeGroup builds a theoretical node group based on the node definition provided. The node group is not automatically // created on the cloud provider side. The node group is not returned by NodeGroups() until it is created. // Implementation optional. // 通常情况下，不需要实现此方法，但如果你需要 ClusterAutoscaler 创建一个默认的 NodeGroup 的话，也可以实现。 // 但其实更好的做法是将默认 NodeGroup 写入云端的伸缩组 NewNodeGroup(machineType string, labels map[string]string, systemLabels map[string]string, taints []apiv1.Taint, extraResources map[string]resource.Quantity) (NodeGroup, error) // GetResourceLimiter returns struct containing limits (max, min) for resources (cores, memory etc.). // 资源限制对象，会在 build 时传入，通常情况下不需要更改，除非在云端有显示的提示用户更改的地方，否则使用时会迷惑用户 GetResourceLimiter() (*ResourceLimiter, error) // GPULabel returns the label added to nodes with GPU resource. // GPU 相关，如果集群中有使用 GPU 资源，需要返回对应内容。 hack: we assume anything which is not cpu/memory to be a gpu. GPULabel() string // GetAvailableGPUTypes return all available GPU types cloud provider supports. // 同上 GetAvailableGPUTypes() map[string]struct{} // Cleanup cleans up open resources before the cloud provider is destroyed, i.e. go routines etc. // CloudProvider 只会在启动时被初始化一次，如果每次循环后有需要清除的内容，在这里处理 Cleanup() error // Refresh is called before every main loop and can be used to dynamically update cloud provider state. // In particular the list of node groups returned by NodeGroups can change as a result of CloudProvider.Refresh(). // 会在 StaticAutoscaler RunOnce 中被调用 Refresh() error } // NodeGroup contains configuration info and functions to control a set // of nodes that have the same capacity and set of labels. type NodeGroup interface { // MaxSize returns maximum size of the node group. MaxSize() int // MinSize returns minimum size of the node group. MinSize() int // TargetSize returns the current target size of the node group. It is possible that the // number of nodes in Kubernetes is different at the moment but should be equal // to Size() once everything stabilizes (new nodes finish startup and registration or // removed nodes are deleted completely). Implementation required. // 响应的是伸缩组的节点数，并不一定与 kubernetes 中的节点数保持一致 TargetSize() (int, error) // IncreaseSize increases the size of the node group. To delete a node you need // to explicitly name it and use DeleteNode. This function should wait until // node group size is updated. Implementation required. // 扩容的方法，增加伸缩组的节点数 IncreaseSize(delta int) error // DeleteNodes deletes nodes from this node group. Error is returned either on // failure or if the given node doesn't belong to this node group. This function // should wait until node group size is updated. Implementation required. // 删除的节点一定要在该节点组中 DeleteNodes([]*apiv1.Node) error // DecreaseTargetSize decreases the target size of the node group. This function // doesn't permit to delete any existing node and can be used only to reduce the // request for new nodes that have not been yet fulfilled. Delta should be negative. // It is assumed that cloud provider will not delete the existing nodes when there // is an option to just decrease the target. Implementation required. // 当 ClusterAutoscaler 发现 kubernetes 节点数与伸缩组的节点数长时间不一致，会调用此方法来调整 DecreaseTargetSize(delta int) error // Id returns an unique identifier of the node group. Id() string // Debug returns a string containing all information regarding this node group. Debug() string // Nodes returns a list of all nodes that belong to this node group. // It is required that Instance objects returned by this method have Id field set. // Other fields are optional. // This list should include also instances that might have not become a kubernetes node yet. // 返回伸缩组中的所有节点，哪怕它还没有成为 kubernetes 的节点 Nodes() ([]Instance, error) // TemplateNodeInfo returns a schedulernodeinfo.NodeInfo structure of an empty // (as if just started) node. This will be used in scale-up simulations to // predict what would a new node look like if a node group was expanded. The returned // NodeInfo is expected to have a fully populated Node object, with all of the labels, // capacity and allocatable information as well as all pods that are started on // the node by default, using manifest (most likely only kube-proxy). Implementation optional. // ClusterAutoscaler 会将节点信息与节点组对应，来判断资源条件，如果是一个空的节点组，那么就会通过此方法来虚拟一个节点信息。 TemplateNodeInfo() (*schedulernodeinfo.NodeInfo, error) // Exist checks if the node group really exists on the cloud provider side. Allows to tell the // theoretical node group from the real one. Implementation required. Exist() bool // Create creates the node group on the cloud provider side. Implementation optional. // 与 CloudProvider.NewNodeGroup 配合使用 Create() (NodeGroup, error) // Delete deletes the node group on the cloud provider side. // This will be executed only for autoprovisioned node groups, once their size drops to 0. // Implementation optional. Delete() error // Autoprovisioned returns true if the node group is autoprovisioned. An autoprovisioned group // was created by CA and can be deleted when scaled to 0. Autoprovisioned() bool } ScaleUP 源码解析 func ScaleUp(context *context.AutoscalingContext, processors *ca_processors.AutoscalingProcessors, clusterStateRegistry *clusterstate.ClusterStateRegistry, unschedulablePods []*apiv1.Pod, nodes []*apiv1.Node, daemonSets []*appsv1.DaemonSet, nodeInfos map[string]*schedulernodeinfo.NodeInfo, ignoredTaints taints.TaintKeySet) (*status.ScaleUpStatus, errors.AutoscalerError) { ...... // 验证当前集群中所有 ready node 是否来自于 nodeGroups，取得所有非组内的 node nodesFromNotAutoscaledGroups, err := utils.FilterOutNodesFromNotAutoscaledGroups(nodes, context.CloudProvider) if err != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, err.AddPrefix(&quot;failed to filter out nodes which are from not autoscaled groups: &quot;) } nodeGroups := context.CloudProvider.NodeGroups() gpuLabel := context.CloudProvider.GPULabel() availableGPUTypes := context.CloudProvider.GetAvailableGPUTypes() // 资源限制对象，会在 build cloud provider 时传入 // 如果有需要可在 CloudProvider 中自行更改，但不建议改动，会对用户造成迷惑 resourceLimiter, errCP := context.CloudProvider.GetResourceLimiter() if errCP != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errors.ToAutoscalerError( errors.CloudProviderError, errCP) } // 计算资源限制 // nodeInfos 是所有拥有节点组的节点与示例节点的映射 // 示例节点会优先考虑真实节点的数据，如果 NodeGroup 中还没有真实节点的部署，则使用 Template 的节点数据 scaleUpResourcesLeft, errLimits := computeScaleUpResourcesLeftLimits(context.CloudProvider, nodeGroups, nodeInfos, nodesFromNotAutoscaledGroups, resourceLimiter) if errLimits != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errLimits.AddPrefix(&quot;Could not compute total resources: &quot;) } // 根据当前节点与 NodeGroups 中的节点来计算会有多少节点即将加入集群中 // 由于云服务商的伸缩组 increase size 操作并不是同步加入 node，所以将其统计，以便于后面计算节点资源 upcomingNodes := make([]*schedulernodeinfo.NodeInfo, 0) for nodeGroup, numberOfNodes := range clusterStateRegistry.GetUpcomingNodes() { ...... } klog.V(4).Infof(&quot;Upcoming %d nodes&quot;, len(upcomingNodes)) // 最终会进入选择的节点组 expansionOptions := make(map[string]expander.Option, 0) ...... // 出于某些限制或错误导致不能加入新节点的节点组，例如节点组已达到 MaxSize skippedNodeGroups := map[string]status.Reasons{} // 综合各种情况，筛选出节点组 for _, nodeGroup := range nodeGroups { ...... } if len(expansionOptions) == 0 { klog.V(1).Info(&quot;No expansion options&quot;) return &amp;status.ScaleUpStatus{ Result: status.ScaleUpNoOptionsAvailable, PodsRemainUnschedulable: getRemainingPods(podEquivalenceGroups, skippedNodeGroups), ConsideredNodeGroups: nodeGroups, }, nil } ...... // 选择一个最佳的节点组进行扩容，expander 用于选择一个合适的节点组进行扩容，默认为 RandomExpander，flag: expander // random 随机选一个，适合只有一个节点组 // most-pods 选择能够调度最多 pod 的节点组，比如有 noSchedulerPods 是有 nodeSelector 的，它会优先选择此类节点组以满足大多数 pod 的需求 // least-waste 优先选择能满足 pod 需求资源的最小资源类型的节点组 // price 根据价格模型，选择最省钱的 // priority 根据优先级选择 bestOption := context.ExpanderStrategy.BestOption(options, nodeInfos) if bestOption != nil &amp;&amp; bestOption.NodeCount &gt; 0 { ...... newNodes := bestOption.NodeCount // 考虑到 upcomingNodes, 重新计算本次新加入节点 if context.MaxNodesTotal &gt; 0 &amp;&amp; len(nodes)+newNodes+len(upcomingNodes) &gt; context.MaxNodesTotal { klog.V(1).Infof(&quot;Capping size to max cluster total size (%d)&quot;, context.MaxNodesTotal) newNodes = context.MaxNodesTotal - len(nodes) - len(upcomingNodes) if newNodes &lt; 1 { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errors.NewAutoscalerError( errors.TransientError, &quot;max node total count already reached&quot;) } } createNodeGroupResults := make([]nodegroups.CreateNodeGroupResult, 0) // 如果节点组在云服务商端处不存在，会尝试创建根据现有信息重新创建一个云端节点组 // 但是目前所有的 CloudProvider 实现都没有允许这种操作，这好像是个多余的方法 // 云服务商不想，也不应该将云端节点组的创建权限交给 ClusterAutoscaler if !bestOption.NodeGroup.Exist() { oldId := bestOption.NodeGroup.Id() createNodeGroupResult, err := processors.NodeGroupManager.CreateNodeGroup(context, bestOption.NodeGroup) ...... } // 得到最佳节点组的示例节点 nodeInfo, found := nodeInfos[bestOption.NodeGroup.Id()] if !found { // This should never happen, as we already should have retrieved // nodeInfo for any considered nodegroup. klog.Errorf(&quot;No node info for: %s&quot;, bestOption.NodeGroup.Id()) return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, errors.NewAutoscalerError( errors.CloudProviderError, &quot;No node info for best expansion option!&quot;) } // 根据 CPU、Memory及可能存在的 GPU 资源（hack: we assume anything which is not cpu/memory to be a gpu.），计算出需要多少个 Nodes newNodes, err = applyScaleUpResourcesLimits(context.CloudProvider, newNodes, scaleUpResourcesLeft, nodeInfo, bestOption.NodeGroup, resourceLimiter) if err != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, err } // 需要平衡的节点组 targetNodeGroups := []cloudprovider.NodeGroup{bestOption.NodeGroup} // 如果需要平衡节点组，根据 balance-similar-node-groups flag 设置。 // 检测相似的节点组，并平衡它们之间的节点数量 if context.BalanceSimilarNodeGroups { ...... } // 具体平衡策略可以看 (b *BalancingNodeGroupSetProcessor) BalanceScaleUpBetweenGroups 方法 scaleUpInfos, typedErr := processors.NodeGroupSetProcessor.BalanceScaleUpBetweenGroups(context, targetNodeGroups, newNodes) if typedErr != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, typedErr } klog.V(1).Infof(&quot;Final scale-up plan: %v&quot;, scaleUpInfos) // 开始扩容，通过 IncreaseSize 扩容 for _, info := range scaleUpInfos { typedErr := executeScaleUp(context, clusterStateRegistry, info, gpu.GetGpuTypeForMetrics(gpuLabel, availableGPUTypes, nodeInfo.Node(), nil), now) if typedErr != nil { return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, typedErr } } ...... } ...... } ","link":"https://cnbailian.github.io/post/kubernetes-cluster-autoscaler/"},{"title":"Kubernetes 的 Dynamic Provisioning 实现","content":"存储一直是容器运行的关键部分，Kubernetes 为此做了很多努力，从一开始的 Pod Volumes、PV(Persistent Volumes) 与 PVC(Persistent Volume Claim)，到 StorageClass 与 Dynamic Provisioning，再到现在 “out-of-tree” 的 CSI(Container Storage Interface)，Kubernetes 社区一直在演进存储的实现。 前面基础的就不讲了，我们从 StorageClass 与 Dynamic Provisioning 开始了解。 关于 StorageClass 与 Dynamic Provisioning StorageClass 为存储提供了“类”的概念，使得 PVC 可以申请不同类别的 PV，以满足用户不同质量、不同策略要求的存储需求。但仅仅是这样还不够，我们还需要手动去创建存储，创建 PV 并与之绑定。所以 StorageClass 还有一个功能就是动态卷供应（Dynamic Provisioning），通过它，Kubernetes 可以根据用户的需求，自动创建其需要的存储。 如何使用 我们需要创建 StorageClass 对象，通过 provisioner 属性指定所用的动态供应的种类： apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 创建好以后，所有指定这个 StorageClass 的 PVC 都会动态分配 PV： apiVersion: v1 kind: PersistentVolumeClaim metadata: name: example namespace: default spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: standard 当然，也需要些其他的配置，比如 aws-ebs 需要在启动参数中加入 --cloud-provider=aws。Glusterfs 需要在集群节点中预先安装好分布式存储等。具体请参考官方手册或 Google，这里不赘述了。 External provisioner 官方提供了许多 Provisioner 的实现：AWSElasticBlockStore、AzureFile、Glusterfs 等等，这些都是 “in-tree” 的，所以官方也在实验一些 external provisioner 的实现方式。在 kubernetes-incubator/external-storage 这个仓库中，就有一些孵化中的项目，不过随着 CSI 的出现，应该已经孵死了。官方也正在将 “in-tree” 的存储实现迁移到 CSI 上。 如何实现 我们根据 external-storage 仓库中的项目，简单的分析一下如何自定义一个 Dynamic Provisioner。 其实这个仓库中的项目都很简单，文件没有几个，代码也没有几行。这是因为它们都是基于官方社区的 library 实现的，它实现了 Provisioner Controller 的整个流程，包括监听、创建 PV 资源等，我们只需要实现 Provisioner 接口的两个方法就可以： // Provisioner is an interface that creates templates for PersistentVolumes // and can create the volume as a new resource in the infrastructure provider. // It can also remove the volume it created from the underlying storage // provider. type Provisioner interface { // Provision creates a volume i.e. the storage asset and returns a PV object // for the volume Provision(ProvisionOptions) (*v1.PersistentVolume, error) // Delete removes the storage asset that was created by Provision backing the // given PV. Does not delete the PV object itself. // // May return IgnoredError to indicate that the call has been ignored and no // action taken. Delete(*v1.PersistentVolume) error } Provision 方法需要根据给定的数据，分配存储，响应 PV 对象。Delete 方法需要在 PV 删除时，也删除对应存储中的数据。 我们选择仓库中的 nfs 项目来进行详细的分析，它不同于其他 client 类项目，它还维护了一份 nfs server，使得它可以不基于其他外部存储服务。可以在 main 函数中看到，通过 runServer flag 判断是否需要启动服务，默认为 true： if *runServer { ...... go func() { for { // This blocks until server exits (presumably due to an error) err = server.Run(ganeshaLog, ganeshaPid, ganeshaConfig) if err != nil { glog.Errorf(&quot;NFS server Exited Unexpectedly with err: %v&quot;, err) } // take a moment before trying to restart time.Sleep(time.Second) } }() // Wait for NFS server to come up before continuing provisioner process time.Sleep(5 * time.Second) } 随后通过 Provisioner Controller 的 Run 方法启动 Provisioner 服务： // Create the provisioner: it implements the Provisioner interface expected by // the controller nfsProvisioner := vol.NewNFSProvisioner(exportDir, clientset, outOfCluster, *useGanesha, ganeshaConfig, *enableXfsQuota, *serverHostname, *maxExports, *exportSubnet) // Start the provision controller which will dynamically provision NFS PVs pc := controller.NewProvisionController( clientset, *provisioner, nfsProvisioner, serverVersion.GitVersion, ) pc.Run(wait.NeverStop) NewNFSProvisioner 返回的是实现了 Provisioner 接口的结构体： type nfsProvisioner struct { ...... } var _ controller.Provisioner = &amp;nfsProvisioner{} 接下来就看下如何实现的 Provision 方法： // options 里包含创建 pv 的数据，pvName、pvc、sc、selectedNode 等 func (p *nfsProvisioner) Provision(options controller.ProvisionOptions) (*v1.PersistentVolume, error) { // 在这里进行验证，创建目录等操作 volume, err := p.createVolume(options) if err != nil { return nil, err } annotations := make(map[string]string) ...... pv := &amp;v1.PersistentVolume{ ObjectMeta: metav1.ObjectMeta{ Name: options.PVName, Labels: map[string]string{}, Annotations: annotations, }, Spec: v1.PersistentVolumeSpec{ PersistentVolumeReclaimPolicy: *options.StorageClass.ReclaimPolicy, AccessModes: options.PVC.Spec.AccessModes, Capacity: v1.ResourceList{ v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)], }, PersistentVolumeSource: v1.PersistentVolumeSource{ NFS: &amp;v1.NFSVolumeSource{ Server: volume.server, Path: volume.path, ReadOnly: false, }, }, MountOptions: options.StorageClass.MountOptions, }, } return pv, nil } func (p *nfsProvisioner) createVolume(options controller.ProvisionOptions) (volume, error) { // 在这里验证剩余磁盘空间是否超出请求大小，只计算当前剩余 gid, rootSquash, mountOptions, err := p.validateOptions(options) if err != nil { return volume{}, fmt.Errorf(&quot;error validating options for volume: %v&quot;, err) } ...... // 根据 pvc 创建目录 path := path.Join(p.exportDir, options.PVName) err = p.createDirectory(options.PVName, gid) if err != nil { return volume{}, fmt.Errorf(&quot;error creating directory for volume: %v&quot;, err) } ...... } func (p *nfsProvisioner) validateOptions(options controller.ProvisionOptions) (string, bool, string, error) { ...... var stat syscall.Statfs_t if err := syscall.Statfs(p.exportDir, &amp;stat); err != nil { return &quot;&quot;, false, &quot;&quot;, fmt.Errorf(&quot;error calling statfs on %v: %v&quot;, p.exportDir, err) } capacity := options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)] requestBytes := capacity.Value() available := int64(stat.Bavail) * int64(stat.Bsize) if requestBytes &gt; available { return &quot;&quot;, false, &quot;&quot;, fmt.Errorf(&quot;insufficient available space %v bytes to satisfy claim for %v bytes&quot;, available, requestBytes) } return gid, rootSquash, mountOptions, nil } 然后是 Delete 方法的实现： func (p *nfsProvisioner) Delete(volume *v1.PersistentVolume) error { ...... // pv 删除后，删除对应的目录 err = p.deleteDirectory(volume) if err != nil { return fmt.Errorf(&quot;error deleting volume's backing path: %v&quot;, err) } ...... return nil } 这里只是简单的讲解下 Provisioner 的实现，省略了其他一些比如 xfs quota 等操作，有兴趣的可以去项目中看一下。顺便提一下，这个项目虽然部署了 nfs server，但没有部署成分布式存储，局限性很大，毕竟只是实验中的项目，生产环境慎用。 后记 碰巧在项目中接触到了 nfs 这个 Provisioner，并且经过测试及源码分析验证了这个项目不可用。经过查阅学习之后写下了这篇文章，算是为以后学习 CSI 作准备吧。 ","link":"https://cnbailian.github.io/post/dynamic-provisioning-of-kubernetes/"},{"title":"Vue 学习路线","content":"本文旨在规划 Vue 框架的学习路线，通过掌握基本概念了解框架，熟悉生态系统，最后深入至框架本身。并未涉及到框架使用方式等详细内容，对每个知识点也只是浅尝即止。 为什么选择vue 可能有很多人只是知道 Vue 这个框架，并没有详细的了解，所以在这里简单的列举下 Vue 的优势。 Vue 有着前端框架中最多的 stars，人数众多的开发者，保证了社区的繁荣。 相对来说较平滑的学习曲线，这主要取决于vue是一个渐进式框架，同时使用基础的HTML模版语法，这让有HTML经验的人很少上手。 渐进式框架也可以更好的逐步的改变原有项目。 团队中有来自世界各地的专家开发者，中文社区和文档质量相对不错。 学习路线 JavaScript与web基础 学习 Vue 框架之前必须先了解 JavaScript 与 web 开发的基本知识，就像看一本英语书前，你需要先掌握英文。 Vue 基本概念 使用 Vue 来构建项目，需要先了解一些基本概念： 渐进式框架 渐进式就是：一步一步，不需要在一开始就把所有的东西都用上。 在 Vue 上的体现就是：它的核心库只包含视图，其他的客户端路由、全局状态管理等通过核心插件提供。 Vue 在设计角度上，包含了解决构建大型单页面应用的大部分问题，但你不需要一开始就把所有的东西都用上。这就带来了较平滑的学习曲线与对老项目渐进式重构的好处。 声明式渲染 &lt;div id=&quot;app&quot;&gt; {{ message }} &lt;/div&gt; var app = new Vue({ el: '#app', data: { message: 'Hello Vue!' } }) 这里的示例代码就是声明式渲染，你写出想要的结果，由框架执行渲染的命令。 响应式数据 在上面的示例代码中，数据就与 DOM 建立了关联，成为响应式数据。此时改变 app.message 的值，就可以看到页面也会发生对应改变。 组件化 组件化的核心思想就是：将页面结构映射为组件树。 组件是资源独立的，组件可以复用，组件与组件之间可以嵌套。 单页面应用与客户端路由 单页面应用（SPA）可以通过单个页面实现传统网站多个页面的功能，通过客户端路由实现加载新内容，而不需要通过浏览器跳转，重新加载页面。 Vue Router 就是 Vue 的实现，由官方维护，通过插件的形式加载。 状态管理 在 Vue 中，每个组件管理着自己的状态，如果有状态需要在多个组件间复用，就需要把共享的状态抽离出来，作为全局的状态来管理，这样，在任何组件中都能获取到。 这就是 Vuex 所做的事情。 使用 Vue 构建单页面应用 以上的基本概念用于理解 Vue，如果要将它实际应用到项目中，还需要了解更多的东西。 构建工具 Vue 提供了一个官方的 CLI：Vue CLI，为单页面应用搭建繁杂的脚手架。 最新的版本 Vue CLI3中加入了 GUI 的支持，对用户更为友好。 使用 axios 访问 Web API Vue 的一个核心思想就是数据驱动。所谓数据驱动，是指视图是由数据驱动生成的，Vue 将数据与 DOM 关联，构建响应式数据，我们对视图的修改，不会直接修改 DOM，而是修改数据，响应至视图。 作为一个单页面应用，数据需要通过 Web API 获取，这些数据可能通过 RESTful API 或 GraphQL 提供，也可能通过 WebSocket 提供。 如果是使用的 HTTP 协议，在 Vue Cookbook 中，推荐使用基于 promise 的 axios。 测试 如果想要开发出稳定可维护的项目，测试是必不可少的。 Vue 官方团队提供了 Vue Test Utils，Vue Test Utils 通过将组件隔离挂载，然后模拟必要的输入和对输出的断言来测试。 Chrome 开发者工具 Vue.js devtools 是一个用于 Chrome 的开发者工具，使用它可以清楚的看到组件树的结构，组件的状态等信息。如果使用了 Vuex，还可以看到全局状态，并将其快照发送给其他人，这个人可以在控制台导入状态，方便定位问题。 多端支持 可以在 Weex 中使用 Vue，Vue 的官方也与 Weex 的团队加深联系，在未来的 Vue3 中，会有更好的支持。 前端技术栈 上述所讲的大多是 Vue 或 Vue 生态系统中的工具。但 Vue 并不是独立存在的，它知识前端技术栈中的一部分。 现代 JavaScript 与 Babel Vue 应用程序可以使用 ES5 开发，这是现代浏览器都支持的 JavaScript 标准。 如果想要获得更好的开发体验，可以更新 JavaScript 标准 ES2015 或更高版本，但这会导致不支持旧版浏览器，为了解决这个问题，就需要使用 Babel，它可以将你的新语法编译为 ES5 代码。 Webpack Webpack 是一个模块打包器，它可以将你的应用程序中各个模块的代码打包至一个或多个文件中，形成浏览器可读的 js 文件。还可以在打包过程中，对代码进行转换、使用 Babel、Sass、TypeScript 等。 虽然 Vue CLI 可以为我们构建基础的 webpack 配置，并且在新版本中，可以使用 GUI 来调整，但这并不意味着你可以不学习它，你还是不可避免的需要自行调试它的配置。 TypeScript 与 Flow Vue2 版本中使用的是 Flow，在 Vue3 中将重构为使用 TypeScript。 这两门语言的主要目的是让 js 拥有类型系统，使用它们可以写出高健壮性的代码，并且可以编译为普通的 ES 语法。 Vue3 将完全使用 TypeScript 编写，这并不意味着你必须使用它。但是如果想要了解 Vue 源码，也是不可避免的。 Vue 生态系统 官方核心插件 上述提到的 Vue Router、Vuex，还有 Vue SSR 都是由官方维护的，这区别于 React，官方主要是考虑到了社区维护会导致更新频繁、解决方案太杂乱的问题。 官方工具 上述也提到过的 Vue devtools、Vue CLI，还有 Vue Loader，也都是基于同样的原因。但这不意味着没有社区参与，作为开源项目，依然可以提出建议，修复问题，只是官方有一个发展方向作为参考。 UI 组件库 也可以称为 UI 框架，主要是一系列常用的组件，例如 Form、Table 等常见的元素，方便快速开发。 市面上有非常多的 UI 框架可供选择，Element UI、iView、Vux 等，各有各的风格特色。 深入理解 Vue 为什么是渐进式框架 框架的存在是为了帮助我们应对复杂度 - 《Vue 2.0——渐进式前端解决方案》 当我们在做一个前端应用时，会遇许多的问题，这些问题可以称为应用复杂度，前端框架的出现，就是为了降低应用复杂度，解决一些重复的并且已经有良好解决方案的问题。 但是，框架本身由于其学习曲线，也会带来不同的复杂度，称为框架复杂度。如何权衡应用复杂度与框架复杂度就称为了一个问题。 React 与 Vue 的选择的模式就是：以可弹性伸缩的框架复杂度来应对不同的应用复杂度。框架核心库只包含视图层，其他的问题都由可选的附加库/工具来解决。 Facebook 团队只专注做 React 本身，其他的问题都是由社区贡献解决方案，社区非常活跃，也有很多优秀的想法和思路，但社区的活跃性也会带来一些副作用，版本更新太快，一个问题有太多的解决方案导致的选择困难，库与库之间可能存在的磨合问题。 Vue 的团队选择的方向就是渐进式，核心插件\\工具由团队开发，负责一些大方向上的统一，同时也是模块化的，可供选择。 声明式渲染 Vue 或者说现代 js 框架，都有一个统一的看法，数据状态是唯一的真相，DOM 状态只是数据状态的映射。所有的逻辑操作都应在状态的层面进行，当状态发生改变时，DOM 在框架的帮助下自动更新至合理的状态。 那么，Vue 时如何实现的呢？主要是使用的虚拟（Virtual） DOM。 虚拟 DOM 简单来说就是使用 js 对象去描述一个 DOM 节点，它产生的前提就是一个 DOM 元素在浏览器中是非常庞大的，因为有着各种属性，各种事件，浏览器的标准就是这么设计的。相比于 DOM 对象，原生的 js 对象处理起来更快，而且更简单。 Vue 将它所有要监听的 DOM 映射为一个虚拟 DOM 树，这个树非常的轻量，它的职责就是描述当前页面的 DOM 状态。 当数据状态发生改变时，Vue 的响应系统会侦测到变化，并生成一个新的虚拟 DOM 树，通过与上一个虚拟 DOM 树进行比较，将改动应用至真实 DOM 状态。 不同于 React 的是，Vue 可以使用 HTML 模版，也可以是用 JSX，这是 Vue 在编译时将模版编译为渲染函数。 状态管理 状态管理本质上就是把整个应用抽象为下图中的循环，State 驱动 View 的渲染，而用户对 View 进行操作产生 Action，会使 State 产生变化，从而导致 View 重新渲染，这就是单向数据流。 在 Vue 中，一个组件就已经是这样的结构了，在多个组件共享状态时，或是来自不同视图的行为变更一个状态时，应该如何管理呢？此问题的答案就是 Vuex。 它将组件的共享状态抽离出来，放入 Store，组件通过调度（dispatch）使用 Action，Action 通过提交（commit）Mutation 修改 State，然后响应到组件。 实现原理 生命周期 Virtual DOM Virtual DOM 在 Vue 中的实现。 响应式数据原理 在 Vue2，使用的是 ES5 的 Object.defineProperty 来构成数据监听系统，这也是 Vue2 不能兼容 IE8 及以下的原因。 在即将到来的 Vue3 中，会使用 Proxy 进行重构数据监听系统，这会导致 Vue3 不能兼容 IE11 及一下，Vue 团队会提出其他的办法来解决这个问题。 编译与渲染函数 在 Vue 中，会将模版编译为渲染函数，在 Vue3 中，也做出了相当的优化。 组件化 每一个组件就是一个 Vue 实例，组件内部是如何工作的，组件间的嵌套等实现。 v-model Vue 提供了 v-model 的指令，用于实现表单与数据状态之间的双向绑定，这也没有破坏单向数据流，只是语法糖。 &lt;input v-model=&quot;sth&quot; /&gt; &lt;input v-bind:value=&quot;sth&quot; v-on:input=&quot;sth = $event.target.value&quot; /&gt; 核心插件 Vue Router：客户端路由中存在的种种问题，嵌套路由、重定向/别名、懒加载等。 Vuex：初始化过程，如何管理全局状态等。 思维导图 相关学习资料 《Vue 2.0——渐进式前端解决方案》 尤雨溪 《Vue Guide》 Vue 官方团队 《Vue.js 技术揭秘》 ustbhuangyi 《Vuex》 Vuex VueConf VueConf Vue.js developers vuejsdevelopers.com 参考文章 《2019 年 Vue 学习路线图》 ","link":"https://cnbailian.github.io/post/vue-learning-route/"},{"title":"Go 的并发性与调度器","content":"本篇文章是我对 Go 语言并发性的理解总结，适合初步了解并发，对 Go 语言的并发编程与调度器原理有兴趣的读者。 你真的了解并发吗？ 相信读者都对并发有着一定的理解，也都对 Go 语言感兴趣，Go 最吸引人的地方可能就是它的内建并发支持，使用 go 关键字，就可以轻松的实现并发。但是，你真正的了解并发吗？ 并发这个词，你去问编程领域中不同的人，会给出不同的答案。对于 WEB 领域的开发人员来说，并发通常是指同一时刻的请求量，WEB 领域的面试官经常会问到的问题：“做过多少并发的项目？”或“接触过高并发的项目吗？”就是应用的这个概念。高并发在这里还有个可能的概念是：同时应对许多请求所使用的技术，这通常与分布式、并行等概念挂钩，需要结合上下文语境来判断。 并发是一个有趣的词，因为它对编程领域中的不同人员意味着不同的事情，在广义概念下，有着许多狭义概念。除了“并发”之外，你可能还听过“并行”、“多线程”、“异步”等词汇，有些人认为这些词意思相同，而其他人则在每个词之间划清界限。 下面，让我们看看 Go 语言编程中，“并发”这个词的概念。 Go 语言中的并发性 Go 语言的并发性并不是 WEB 领域的并发概念，很多人对此有所混淆。在 Go 语言发布之初，大家对 Go 的并发特性都有所疑问： 为什么要有并发？ 什么是并发？ 这个想法源自哪里？ 并发有什么好处？ 我该如何使用它？ 面对这些问题，Rob Pike（Go 语言作者之一）在2012年的 Google I/O 上做了一次精彩的演讲：《Go Concurrency Patterns》，在这场演讲中，他回答了上述问题，并通过详细的示例讲解了 goroutine、channel 与 select 的使用，建议大家都去看一看这场演讲。 简单的总结一下并发在 Go 语言编程中的概念： “并发是一种将程序分解成小片段独立执行的程度设计方法”，它是一种结构化程序的方式，独立执行计算的组合。 在上述的演讲中可以看出，Go 语言推荐使用并发，我们也应该遵循这种编程方式。对于程序员来说，代码更有说服力，我们可以通过这个素数筛选程序来理解 Go 的并发编程： // A concurrent prime sieve package main // Send the sequence 2, 3, 4, ... to channel 'ch'. func Generate(ch chan&lt;- int) { for i := 2; ; i++ { ch &lt;- i // Send 'i' to channel 'ch'. } } // Copy the values from channel 'in' to channel 'out', // removing those divisible by 'prime'. func Filter(in &lt;-chan int, out chan&lt;- int, prime int) { for { i := &lt;-in // Receive value from 'in'. if i%prime != 0 { out &lt;- i // Send 'i' to 'out'. } } } // The prime sieve: Daisy-chain Filter processes. func main() { ch := make(chan int) // Create a new channel. go Generate(ch) // Launch Generate goroutine. for i := 0; i &lt; 10; i++ { prime := &lt;-ch print(prime, &quot;\\n&quot;) ch1 := make(chan int) go Filter(ch, ch1, prime) ch = ch1 } } 它并不是复杂度最低的算法，特别是寻找大素数方面，但却是最能体现 Go 并发编程、通过通信共享内存的理念，而且非常优雅。 在这段代码中，通过 goroutine 的组合，实现一层层的筛选器，筛选器之间通过 channel 通信，每一个筛选器就是一个素数，每个给 main goroutine 通信的内容也是素数，简直精妙。 通过下面的 gif 动画能清晰的看到程序运行过程： 并发不是并行 Go 的并行 只需要 GOMAXPROCS 的值大于1，就可以让 Go 程序在多核机器上实现以并行的形式运行。但并发的程序一定可以并行吗？ 我们需要明确一个观点：**并发不是为了效率，并发的程序不一定可以并行。**还是上面素数的例子，这段代码是并发的，但不可以并行，因为它的每一个执行片段都需要上一个片段的筛选与通信。 正交概念 正交概念：从数学上引进正交这个词，用于表示指相互独立，相互间不可替代，并且可以组合起来。 在广义概念上来讲，并发与并行是正交概念，对于 Go 语言的并发性来讲也是如此。 《Concurrency is not Parallelism》 同样，在 Go 语言发布之初，有很多人混淆了并发与并行的概念，对此，Rob Pike 发表了另一篇演讲《Concurrency is not Parallelism》，通过地鼠烧书的比喻与简单负载均衡器的示例，详细的阐述了并发与并行的区别。 这里不再复述地鼠例子，只是简单的总结，感兴趣的建议去看演讲： 并行是指同时能执行多个事情。 并发关乎结构，是一种结构化程序的方式。 并行关乎执行，表述的是程序的运行状态。 Go 语言是如何支持并发的？ 上面一直在讲 Go 语言的并发性，接下来看下 Go 语言是如何做到的并发支持。 我们在使用 Go 编写并发程序的过程中，无需关心线程的维护、调度等一系列问题，只需要关心程序结构的分解与组合、goroutine 之间的通信就可以写出良好的并发程序，这全部都要依赖于 Go 语言内建的 G-P-M 模型。 模型演化过程 在 Go 语言1.0版本时，只有 G-M 模型，Google 工程师 Dmitry Vyukov 在《Scalable Go Scheduler Design Doc》中指出了该模型在并发伸缩性方面的问题： 所有对 G 的操作：创建、重新调用等由单个全局锁(Sched.Lock)保护，浪费时间。 当 M 阻塞时，G 需要传递给别的 M 执行，这导致调度延迟增大以及额外的性能损耗； M 用到的 mCache 属于内核线程，当 M 阻塞后相应的内存资源仍被占用，导致内存占用过高； 由于 syscall 导致 M 的阻塞和恢复，导致了额外性能损耗。 并且亲自下场，重新设计、改进了 Go scheduler，在 Go1.1 版本中实现了 G-P-M 模型： G-P-M 模型 那么这套模型与调度是怎样的呢？先来简单的说一下 G、P、M 的定义： G：表示 Goroutine，G 存储了 goroutine 执行的栈信息，状态，任务函数，可重用。 P：Processor，表示逻辑处理器，拥有一个本地队列。对于 G 来说，P 相当于 CPU 内核，只有进入到 P 的队列中，才可以被调度。对于 M 来说，P 提供了相关的执行环境（Context），如内存分配状态，任务队列等。P 的数量就是程序可最大可并行的 G 的数量（前提：物理CPU核数 &gt;= P的数量），由用户设置的 GOMAXPROCS 决定。 M：Machine，是对系统线程的抽象，是真正执行计算的部分。M 在绑定 P 后会在队列中获取 G，切换到 G 的执行栈并执行 G 的函数。M 数量不定，但同时只有 P 个 M 在执行，为了防止创建过多系统线程导致系统调度出现问题，目前默认最大限制10000个。 接下来了解这套模型的基本调度，在调度过程中还有一个 work-stealing 的算法： 每个 P 维护一个本地队列； 当一个 G 被创建后，放入当前 P 的本地队列中，如果队列已满，放入全局队列； 当 M 执行完一个 G 后，会在 当前 P 的队列中取出新的 G，队列为空则在全局队列中加锁获取； 如果全局队列也为空，则去其他的 P 的队列中偷出一半的 G，放入自己的本地队列。 Go 语言就是凭借着这套优秀的并发模型与调度，实现了内建的并发支持。 Goroutine 调度器的深入 让我们深入的了解一下 goroutine 调度器。 调度器解决了什么问题？ 阻塞问题 如果任务G陷入到阻塞的系统调用中，内核线程M将一起阻塞，于是实际的运行线程少了一个。更严重的，如果所有M都阻塞了，那些本可以运行的任务G将没有系统资源运行。 Go 在执行阻塞的系统调用时会调用 entersyscallblock ，然后通过 handoffp 解绑 M 对应的 P。如果此时 P 的本地队列中还有 G，P 会去寻找别的 M 或创建新的 M 继续执行，若本地队列为空，则进入 pidle 链表，等待有需要时被取出。 如果是调用的 entersyscall，会将 P 的状态置为 _Psyscall。监控线程 sysmon 会通过 retake 循环所有的 P，发现是 _Psyscall 状态，就会调用 handoffp 来释放。 抢占调度 在 Go1.1 版本中，是没有抢占调度的，当前 G 只有涉及到锁操作，读写 channel 才会触发切换。若没有抢占机制，同一个 M 上的其他任务 G 有可能会长时间执行不到，甚至会被死循环锁住。 于是 Dmitry Vyukov 提出了《Go Preemptive Scheduler Design Doc》, 并在1.2版本中引入了初级的抢占。 监控线程 sysmon 会通过 retake 循环所有的 P，发现运行时间超出 forcePreemptNS 限制（10ms）的 P，就会通过 preemptone 发起抢占。 Goroutine 的负载均衡 内核线程M不是从全局任务队列中得到G，而是从M本地维护的G缓存中获取任务。如果某个M的G执行完了，而别的M还有很多G，这时如果G不能切换将造成CPU的浪费。 这部分的实现是在 M 的启动函数 mstart 中 schedule 的调用来实现，它会先查找本地队列，然后查找全局队列，最后是随机偷取其他 P 的一半 G，直到取到 G 或停掉 M。为了防止全局队列被“饿死”，每61次调度，会先在全局队列中查找。 调度器相关源码 调度器部分的代码主要集中在 src/runtime/runtime2.go 与 src/runtime/proc.go 这两个文件中。 调度器的4个基本结构：g、m、p、schedt，都在 runtime2.go 中，schedt 可能有些陌生，它是调度器的核心结构，也是全局资源池，用来存储 G 的全局队列，空闲的 P 链表 pidle，空闲的 M 链表 midle 等等。 调度器的具体实现函数都在 proc.go 中，用户的所有代码都是运行在 goroutine 中，Go 在运行时会将 main 中的代码放入 main goroutine 中运行，这时还会启动监控系统 sysmon。 更多关于调度器的细节，例如加锁，与 GC 的交互等，需要通过进一步的阅读源码来了解。 结束语 看到这里，相信大家对“并发”会有全新的认识，本文旨在讲清 Go 语言的并发性，在以后的 Go 语言编程过程中，希望更倾向于并发编程。并发编程不仅结构清晰，通常来说也会更容易并行运行，使得程序运行效率提高。 参考文章 《Go Concurrency Patterns》 《Concurrency is not Parallelism》 《go-under-the-hood》 《也谈goroutine调度器》 《Goroutine浅析》 ","link":"https://cnbailian.github.io/post/concurrency-and-scheduler-of-go/"},{"title":"JSON Schema","content":"JSON Schema 详解：未完待续 第一篇：了解 JSON 介绍 JSON（JavaScriptObjectNotation） 是一种轻量级、基于文本、不限语言的数据交换格式，源自 ECMAScript，《Standard ECMA-262 3rd Edition - December 1999》的一个子集，而后成为写入 RFC 文档：RFC 4627，最新的 RFC 标准为：RFC 8259。 JSON 的官方 MIME 类型是 application/json，文件扩展名是 .json。 类型 两种结构化类型： Object：名称/值的集合（A collection of name/value pairs），在不同的语言中，被理解为 object，record，struct，dictionary，hash table 等等。 使用大括号{，}包裹名称/值的集合，名称是字符串类型，值可以是以上任意类型。名称与值使用:分隔。名称/值之间使用,分隔。 格式示例：{&quot;example&quot;:&quot;string&quot;}。 Array：值的有序列表（An ordered list of values），在大部分语言中，被理解为 array。 使用中括号[，]包裹值的列表，值可以是以上任意类型。值与值之间使用`,``分隔。 格式示例：[0,&quot;1&quot;,null,false,true,{},[]]。 四种基本类型： 字符串（string） 字符串是 Unicode 字符组成的集合，使用双引号包裹（&quot;），反斜杠转义（\\）。 与 C 或 Java 的字符串相似。 数值（number） 使用十进制，可以为负数或者小数。还可以用e或者E表示为指数形式。 数值也与 C 或 Java 的数值相似。 布尔（boolean） true 或 false。 空（null） null。 Tips： 值的是可以嵌套的。 名称是字符串类型，也就是说可以是任意 Unicode 字符，但是不推荐使用英文以外的语言，虽然 JSON 允许，但是有些使用 JSON 的语言可能不支持。 符号中间允许空白字符。 JSON 没有限制必须以 object 或结构类型作为顶层类型。 但是某些语言解析某种类型会更加方便，比如给 ios 最好是使用 object 作为顶层类型。 上述的集合或列表（string，object，array）可以是空集或空列。 示例 # Object { &quot;Image&quot;: { &quot;Width&quot;: 800, &quot;Height&quot;: 600, &quot;Title&quot;: &quot;View from 15th Floor&quot;, &quot;Thumbnail&quot;: { &quot;Url&quot;: &quot;http://www.example.com/image/481989943&quot;, &quot;Height&quot;: 125, &quot;Width&quot;: 100 }, &quot;Animated&quot; : false, &quot;IDs&quot;: [116, 943, 234, 38793] } } # Array [ { &quot;precision&quot;: &quot;zip&quot;, &quot;Latitude&quot;: 37.7668, &quot;Longitude&quot;: -122.3959, &quot;Address&quot;: &quot;&quot;, &quot;City&quot;: &quot;SAN FRANCISCO&quot;, &quot;State&quot;: &quot;CA&quot;, &quot;Zip&quot;: &quot;94107&quot;, &quot;Country&quot;: &quot;US&quot; }, { &quot;precision&quot;: &quot;zip&quot;, &quot;Latitude&quot;: 37.371991, &quot;Longitude&quot;: -122.026020, &quot;Address&quot;: &quot;&quot;, &quot;City&quot;: &quot;SUNNYVALE&quot;, &quot;State&quot;: &quot;CA&quot;, &quot;Zip&quot;: &quot;94085&quot;, &quot;Country&quot;: &quot;US&quot; } ] # Only values &quot;Hello world!&quot; 42 true null 第二篇：介绍 版本 当前版本：draft-07。 简介 JSON Schema 是一个用于注释和验证 JSON documents 的词汇表。 JSON Schema 本身也是基于 JSON 格式，并且提供了一系列的规范，用于描述 JSON 数据的结构，旨在定义 JSON 数据的验证，文档，超链接导航和交互控制。 用途 对数据结构进行描述 构建人机可读的文档 校验数据 项目状态 共有三个规范： JSON Schema (core) JSON Schema Validation JSON Hyper-Schema 项目组正在努力使它们成为正式的 RFC 标准。 draft-08 如何应用 可以用于生成模拟数据，确保接近真实数据。 用于校验数据，实现自动化测试。 多端共用同一份验证。 第三篇：Core 规范 简介 JSON Schema (core) 此规范主要是定义了核心术语、机制，包括引用其他 JSON Schema，以及正在使用的词汇表。 状态 草案目前于2018年03月19日最后更新，到期时间2018年09月20日，ietf地址：https://datatracker.ietf.org/doc/draft-handrews-json-schema/。 概述 媒体类型 JSON 的媒体类型：application/json 。 JSON Schema 的媒体类型为 application/schema+json，它还有另外一种可选媒体类型用于提供额外的扩展功能：application/schema-instance+json。 验证 JSON Schema 描述了 JSON 文档(JSON document) 的结构，例如属性、长度限制等，程序可以以此判断 实例(Instance) 是否符合规范。 规范与关键字在 JSON Schema Validation 中定义。 注释 JSON Schema 可以对 实例(Instance) 添加注释。 规范与关键字在 JSON Schema Validation 中定义。 超媒体与链接 JSON Hyper-Schema 描述了 JSON 文档(JSON document) 的超文本结构，包括 实例(Instance) 中的资源链接关系等。 规范与关键字在 JSON Hyper-Schema 中定义。 定义 JSON Document JSON 文档(JSON document) 是使用 application/json 媒体类型描述的资源。 简言之就是 JSON 值。 JSON Schema 中， JSON 值(JSON value)、JSON 文本(JSON text)、JSON 文档(JSON document)是等义词。 JSON Schema 也是 JSON 文档(JSON document)。 Instance 实例(Instance) 是使用 JSON Schema 描述的 JSON 文档(JSON document)。 JSON Schema document 使用 application/schema+json 媒体类型描述的 JSON 文档(JSON document)，简称 schema。 关键字 $schema $schema 关键字用于声明当前 JSON 文档(JSON document) 是 JSON Schema document。 值可以是 JSON Schema 版本的标识符，也是资源的位置，但必须是 URI。 $id $id 关键字用于定义 schema 基本的 URI，可用于 $ref 的引用标识，值必须是 URI。 $ref $ref 关键字用于定义引用 schema，值必须是 URI。 URI 只是标识符，不是网络定位器。如果 URI 是可以访问的 URL，不应该去下载。 如果有两个 schema 互相使用 $ref 进行引用，不应该陷入无限循环。 $comment $comment 用于 schema 开发人员对文档进行注释，不需要展示给最终用户看。 ","link":"https://cnbailian.github.io/post/json-schema/"},{"title":"OAuth2.0 与 oauth2-server 库的使用","content":"OAuth2.0 是关于授权的开放网络标准，它允许用户已第三方应用获取该用户在某一网站的私密资源，而无需提供用户名与密码，目前已在全世界得到广泛应用。 league/oauth2-server 是一个轻量级并且功能强大的符合 OAuth2.0 协议的 PHP 库，使用它可以构建出标准的 OAuth2.0 授权服务器。 本文通过对 PHP 库：league/oauth2-server 进行实践的同时，理解 OAuth2.0 的工作流程与设计思路。 术语 了解 OAuth2.0 与 oauth2-server 的专用术语，对于理解后面内容很有帮助。 OAuth2.0 定义了四个角色 Client：客户端，第三方应用程序。 Resource Owner：资源所有者，授权 Client 访问其帐户的用户。 Authorization server：授权服务器，服务商专用于处理用户授权认证的服务器。 Resource server：资源服务器，服务商用于存放用户受保护资源的服务器，它可以与授权服务器是同一台服务器，也可以是不同的服务器。 oauth2-server Access token：用于访问受保护资源的令牌。 Authorization code：发放给应用程序的中间令牌，客户端应用使用此令牌交换 access token。 Scope：授予应用程序的权限范围。 JWT：Json Web Token 是一种用于安全传输的数据传输格式。 运行流程 安装 推荐使用 Composer 进行安装： composer require league/oauth2-server 根据授权模式的不同，oauth2-server 提供了不同的 Interface 与 Triat 帮助实现。 本文发布时，版本号为7.3.1。 生成公钥与私钥 公钥与私钥用于签名和验证传输的 JWT，授权服务器使用私钥签名 JWT，资源服务器拥有公钥验证 JWT。 oauth2-server 使用 JWT 传输访问令牌(access token)，方便资源服务器获取其中内容，所以需要使用非对称加密。 生成私钥，在终端中运行： openssl genrsa -out private.key 2048 使用私钥提取私钥： openssl rsa -in private.key -pubout -out public.key 私钥必须保密于授权服务器中，并将公钥分发给资源服务器。 生成加密密钥 加密密钥用于加密授权码(auth code)与刷新令牌(refesh token)，AuthorizationServer(授权服务器启动类)接受两种加密密钥，string 或 defuse/php-encryption 库的对象。 加密授权码(auth code)与刷新令牌(refesh token)只有授权权服务器使用，所以使用对称加密。 生成字符串密钥，在终端中输入： php -r 'echo base64_encode(random_bytes(32)), PHP_EOL;' 生成对象，在项目根目录的终端中输入： vendor/bin/generate-defuse-key 将获得的内容，传入 AuthorizationServer： use \\Defuse\\Crypto\\Key; $server = new AuthorizationServer( $clientRepository, $accessTokenRepository, $scopeRepository, $privateKeyPath, Key::loadFromAsciiSafeString($encryptionKey) //传入加密密钥 ); PHP版本支持 PHP 7.0 PHP 7.1 PHP 7.2 授权模式 OAuth2.0 定义了四种授权模式，以应对不同情况时的授权。 授权码模式 隐式授权模式 密码模式 客户端模式 客户端类型 保密的： 客户端可以安全的存储自己与用户的凭据（例如：有所属的服务器端） 公开的： 客户端无法安全的存储自己与用户的凭据（例如：运行在浏览器的单页应用） 选用哪种授权模式？ 如果客户端是保密的，应使用授权码模式。 如果客户端是公开的，应使用隐式授权模式。 如果用户对于此客户端高度信任（例如：第一方应用程序或操作系统程序），应使用密码模式。 如果客户端是以自己的名义，不与用户产生关系，应使用客户端模式。 预先注册 客户端需要预先在授权服务器进行注册，用以获取 client_id 与 client_secret，也可以在注册是预先设定好 redirect_uri，以便于之后可以使用默认的 redirect_uri。 授权码模式 授权码模式是 OAuth2.0 种功能最完整，流程最严密的一种模式，如果你使用过 Google 或 QQ 登录过第三方应用程序，应该会对这个流程的第一部分很熟悉。 流程 第一部分（用户可见） 用户访问客户端，客户端将用户导向授权服务器时，将以下参数通过 GET query 传入： response_type：授权类型，必选项，值固定为：code client_id：客户端ID，必选项 redirect_uri：重定向URI，可选项，不填写时默认预先注册的重定向URI scope：权限范围，可选项，以空格分隔 state：CSRF令牌，可选项，但强烈建议使用，应将该值存储与用户会话中，以便在返回时验证 用户选择是否给予客户端授权 假设用户给予授权，授权服务器将用户导向客户端事先指定的 redirect_uri，并将以下参数通过 GET query 传入： code：授权码(Authorization code) state：请求中发送的 state，原样返回。客户端将此值与用户会话中的值进行对比，以确保授权码响应的是此客户端而非其他客户端程序 第二部分（用户不可见） 客户端已得到授权，通过 POST 请求向授权服务器获取访问令牌(access token)： grant_type：授权模式，值固定为：authorization_code client_id：客户端ID client_secret：客户端 secret redirect_uri：使用与第一部分请求相同的 URI code：第一部分所获的的授权码，要注意URL解码 授权服务器核对授权码与重定向 URI，确认无误后，向客户端响应下列内容： token_type：令牌类型，值固定为：Bearer expires_in：访问令牌的存活时间 access_token：访问令牌 refresh_token：刷新令牌，访问令牌过期后，使用刷新令牌重新获取 使用 oauth2-server 实现 初始化 OAuth2.0 只是协议，在实现上需要联系到用户与数据库存储，oauth2-server 的新版本并没有指定某种数据库，但它提供了 Interfaces 与 Traits 帮助我们实现，这让我们可以方便的使用任何形式的数据存储方式，这种方便的代价就是需要我们自行创建 Repositories 与 Entities。 初始化 server // 初始化存储库 $clientRepository = new ClientRepository(); // Interface: ClientRepositoryInterface $scopeRepository = new ScopeRepository(); // Interface: ScopeRepositoryInterface $accessTokenRepository = new AccessTokenRepository(); // Interface: AccessTokenRepositoryInterface $authCodeRepository = new AuthCodeRepository(); // Interface: AuthCodeRepositoryInterface $refreshTokenRepository = new RefreshTokenRepository(); // Interface: RefreshTokenRepositoryInterface $userRepository = new UserRepository(); //Interface: UserRepositoryInterface // 私钥与加密密钥 $privateKey = 'file://path/to/private.key'; //$privateKey = new CryptKey('file://path/to/private.key', 'passphrase'); // 如果私钥文件有密码 $encryptionKey = 'lxZFUEsBCJ2Yb14IF2ygAHI5N4+ZAUXXaSeeJm6+twsUmIen'; // 加密密钥字符串 // $encryptionKey = Key::loadFromAsciiSafeString($encryptionKey); //如果通过 generate-defuse-key 脚本生成的字符串，可使用此方法传入 // 初始化 server $server = new \\League\\OAuth2\\Server\\AuthorizationServer( $clientRepository, $accessTokenRepository, $scopeRepository, $privateKey, $encryptionKey ); 初始化授权码类型 // 授权码授权类型初始化 $grant = new \\League\\OAuth2\\Server\\Grant\\AuthCodeGrant( $authCodeRepository, $refreshTokenRepository, new \\DateInterval('PT10M') // 设置授权码过期时间为10分钟 ); $grant-&gt;setRefreshTokenTTL(new \\DateInterval('P1M')); // 设置刷新令牌过期时间1个月 // 将授权码授权类型添加进 server $server-&gt;enableGrantType( $grant, new \\DateInterval('PT1H') // 设置访问令牌过期时间1小时 ); DateInterval 使用 注意：这里的示例演示的是 Slim Framework 的用法，Slim 不是这个库的必要条件，只需要请求与响应符合PSR-7规范即可。 用户向客户端提出 OAuth 登录请求，客户端将用户重定向授权服务器的地址（例如：https://example.com/authorize?response_type=code&amp;client_id={client_id}&amp;redirect_uri={redirect_uri}&amp;scope{scope}&amp;state={state})： $app-&gt;get('/authorize', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 验证 HTTP 请求，并返回 authRequest 对象 $authRequest = $server-&gt;validateAuthorizationRequest($request); // 此时应将 authRequest 对象序列化后存在当前会话(session)中 $_SESSION['authRequest'] = serialize($authRequest); // 然后将用户重定向至登录入口或在当前地址直接响应登录页面 return $response-&gt;getBody()-&gt;write(file_get_contents(&quot;login.html&quot;)); } catch (OAuthServerException $exception) { // 可以捕获 OAuthServerException，将其转为 HTTP 响应 return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { // 其他异常 $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 此时展示给用户的是这样的页面： 用户提交登录后，设置好用户实体(userEntity)： $app-&gt;post('/login', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 在会话(session)中取出 authRequest 对象 $authRequest = unserialize($_SESSION['authRequest']); // 设置用户实体(userEntity) $authRequest-&gt;setUser(new UserEntity(1)); // 设置权限范围 $authRequest-&gt;setScopes(['basic']) // true = 批准，false = 拒绝 $authRequest-&gt;setAuthorizationApproved(true); // 完成后重定向至客户端请求重定向地址 return $server-&gt;completeAuthorizationRequest($authRequest, $response); } catch (OAuthServerException $exception) { // 可以捕获 OAuthServerException，将其转为 HTTP 响应 return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { // 其他异常 $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 客户端通过授权码请求访问令牌： $app-&gt;post('/access_token', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 这里只需要这一行就可以，具体的判断在 Repositories 中 return $server-&gt;respondToAccessTokenRequest($request, $response); } catch (\\League\\OAuth2\\Server\\Exception\\OAuthServerException $exception) { return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 隐式授权模式 隐式授权相当于是授权码模式的简化版本： 流程(用户可见) 用户访问客户端，客户端将用户导向授权服务器时，将以下参数通过 GET query 传入： response_type：授权类型，必选项，值固定为：token client_id：客户端ID，必选项 redirect_uri：重定向URI，可选项，不填写时默认预先注册的重定向URI scope：权限范围，可选项，以空格分隔 state：CSRF令牌，可选项，但强烈建议使用，应将该值存储与用户会话中，以便在返回时验证 用户选择是否给予客户端授权 假设用户给予授权，授权服务器将用户导向客户端事先指定的 redirect_uri，并将以下参数通过 GET query 传入： token_type：令牌类型，值固定为：Bearer expires_in：访问令牌的存活时间 access_token：访问令牌 state：请求中发送的 state，原样返回。客户端将此值与用户会话中的值进行对比，以确保授权码响应的是此应用程序而非其他应用程序 整个流程与授权码模式的第一部分类似，只是授权服务器直接响应了访问令牌，跳过了授权码的步骤。它适用于没有服务器，完全运行在前端的应用程序。 此模式下没有刷新令牌(refresh token)的返回。 使用 oauth2-server 实现 初始化 server 初始化授权码类型 // 将隐式授权类型添加进 server $server-&gt;enableGrantType( new ImplicitGrant(new \\DateInterval('PT1H')), new \\DateInterval('PT1H') // 设置访问令牌过期时间1小时 ); DateInterval 使用 注意：这里的示例演示的是 Slim Framework 的用法，Slim 不是这个库的必要条件，只需要请求与响应符合PSR-7规范即可。 $app-&gt;post('/login', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 在会话(session)中取出 authRequest 对象 $authRequest = unserialize($_SESSION['authRequest']); // 设置用户实体(userEntity) $authRequest-&gt;setUser(new UserEntity(1)); // 设置权限范围 $authRequest-&gt;setScopes(['basic']) // true = 批准，false = 拒绝 $authRequest-&gt;setAuthorizationApproved(true); // 完成后重定向至客户端请求重定向地址 return $server-&gt;completeAuthorizationRequest($authRequest, $response); } catch (OAuthServerException $exception) { // 可以捕获 OAuthServerException，将其转为 HTTP 响应 return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { // 其他异常 $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 此时展示给用户的是这样的页面： 用户提交登录后，设置好用户实体(userEntity)： $app-&gt;post('/login', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 在会话(session)中取出 authRequest 对象 $authRequest = unserialize($_SESSION['authRequest']); // 设置用户实体(userEntity) $authRequest-&gt;setUser(new UserEntity(1)); // 设置权限范围 $authRequest-&gt;setScopes(['basic']) // true = 批准，false = 拒绝 $authRequest-&gt;setAuthorizationApproved(true); // 完成后重定向至客户端请求重定向地址 return $server-&gt;completeAuthorizationRequest($authRequest, $response); } catch (OAuthServerException $exception) { // 可以捕获 OAuthServerException，将其转为 HTTP 响应 return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { // 其他异常 $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 密码模式 密码模式是由用户提供给客户端账号密码来获取访问令牌，这属于危险行为，所以此模式只适用于高度信任的客户端（例如第一方应用程序）。客户端不应存储用户的账号密码。 OAuth2 协议规定此模式不需要传 client_id &amp; client_secret，但 oauth-server 库需要 流程 客户端要求用户提供授权凭据，通常是账号密码 然后，客户端发送 POST 请求至授权服务器，携带以下参数： grant_type：授权类型，必选项，值固定为：password client_id：客户端ID，必选项 client_secret：客户端 secret scope：权限范围，可选项，以空格分隔 username：用户账号 password：用户密码 授权服务器响应以下内容： token_type：令牌类型，值固定为：Bearer expires_in：访问令牌的存活时间 access_token：访问令牌 refresh_token：刷新令牌，访问令牌过期后，使用刷新令牌重新获取 使用 oauth2-server 实现 初始化 server 初始化授权码类型 $grant = new \\League\\OAuth2\\Server\\Grant\\PasswordGrant( $userRepository, $refreshTokenRepository ); $grant-&gt;setRefreshTokenTTL(new \\DateInterval('P1M')); // 设置刷新令牌过期时间1个月 // 将密码授权类型添加进 server $server-&gt;enableGrantType( $grant, new \\DateInterval('PT1H') // 设置访问令牌过期时间1小时 ); DateInterval 使用 注意：这里的示例演示的是 Slim Framework 的用法，Slim 不是这个库的必要条件，只需要请求与响应符合PSR-7规范即可。 $app-&gt;post('/access_token', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 这里只需要这一行就可以，具体的判断在 Repositories 中 return $server-&gt;respondToAccessTokenRequest($request, $response); } catch (\\League\\OAuth2\\Server\\Exception\\OAuthServerException $exception) { return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 客户端模式 客户端模式是指以客户端的名义，而不是用户的名义，向授权服务器获取认证。在这个模式下，用户与授权服务器不产生关系，用户只能感知到的客户端，所产生的资源也都由客户端处理。 流程 客户端发送 POST 请求至授权服务器，携带以下参数： grant_type：授权类型，必选项，值固定为：client_credentials client_id：客户端ID，必选项 client_secret：客户端 secret scope：权限范围，可选项，以空格分隔 授权服务器响应以下内容： token_type：令牌类型，值固定为：Bearer expires_in：访问令牌的存活时间 access_token：访问令牌 此模式下无需刷新令牌(refresh token)的返回。 使用 oauth2-server 实现 初始化 server 初始化授权码类型 // 将客户端授权类型添加进 server $server-&gt;enableGrantType( new \\League\\OAuth2\\Server\\Grant\\ClientCredentialsGrant(), new \\DateInterval('PT1H') // 设置访问令牌过期时间1小时 ); DateInterval 使用 注意：这里的示例演示的是 Slim Framework 的用法，Slim 不是这个库的必要条件，只需要请求与响应符合PSR-7规范即可。 $app-&gt;post('/access_token', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 这里只需要这一行就可以，具体的判断在 Repositories 中 return $server-&gt;respondToAccessTokenRequest($request, $response); } catch (\\League\\OAuth2\\Server\\Exception\\OAuthServerException $exception) { return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 刷新访问令牌(access token) 访问令牌有一个较短的存活时间，在过期后，客户端通过刷新令牌来获得新的访问令牌与刷新令牌。当用户长时间不活跃，刷新令牌也过期后，就需要重新获取授权。 流程 客户端发送 POST 请求至授权服务器，携带以下参数： grant_type：授权类型，必选项，值固定为：refresh_token client_id：客户端ID，必选项 client_secret：客户端 secret scope：权限范围，可选项，以空格分隔 refresh_token：刷新令牌 授权服务器响应以下内容： token_type：令牌类型，值固定为：Bearer expires_in：访问令牌的存活时间 access_token：访问令牌 refresh_token：刷新令牌，访问令牌过期后，使用刷新令牌重新获取 使用 oauth2-server 实现 初始化 server 初始化授权码类型 $grant = new \\League\\OAuth2\\Server\\Grant\\RefreshTokenGrant($refreshTokenRepository); $grant-&gt;setRefreshTokenTTL(new \\DateInterval('P1M')); // 新的刷新令牌过期时间1个月 // 将刷新访问令牌添加进 server $server-&gt;enableGrantType( $grant, new \\DateInterval('PT1H') // 新的访问令牌过期时间1小时 ); DateInterval 使用 $app-&gt;post('/access_token', function (ServerRequestInterface $request, ResponseInterface $response) use ($server) { try { // 这里只需要这一行就可以，具体的判断在 Repositories 中 return $server-&gt;respondToAccessTokenRequest($request, $response); } catch (\\League\\OAuth2\\Server\\Exception\\OAuthServerException $exception) { return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { $body = new Stream(fopen('php://temp', 'r+')); $body-&gt;write($exception-&gt;getMessage()); return $response-&gt;withStatus(500)-&gt;withBody($body); } }); 资源服务器验证访问令牌 oauth2-server 为资源服务器提供了一个中间件用于验证访问令牌。 客户端需要在 HTTP Header 中使用 Authorization 传入访问令牌，如果通过，中间件将会在 request 中加入对应数据： oauth_access_token_id：访问令牌 id oauth_client_id: 客户端id oauth_user_id：用户id oauth_scopes：权限范围 授权不通过，则抛出 OAuthServerException::accessDenied 异常。 // 初始化 $accessTokenRepository = new AccessTokenRepository(); // Interface: AccessTokenRepositoryInterface // 授权服务器分发的公钥 $publicKeyPath = 'file://path/to/public.key'; // 创建 ResourceServer $server = new \\League\\OAuth2\\Server\\ResourceServer( $accessTokenRepository, $publicKeyPath ); // 中间件 new \\League\\OAuth2\\Server\\Middleware\\ResourceServerMiddleware($server); 如果所用路由不支持中间件，可自行实现，符合PSR-7规范即可 ： try { $request = $server-&gt;validateAuthenticatedRequest($request); } catch (OAuthServerException $exception) { return $exception-&gt;generateHttpResponse($response); } catch (\\Exception $exception) { return (new OAuthServerException($exception-&gt;getMessage(), 0, 'unknown_error', 500))-&gt;generateHttpResponse($response); } oauth2-server 实现 oauth2-server 的实现需要我们手动创建 Repositories 与 Entities，下面展示一个项目目录示例： - Entities - AccessTokenEntity.php - AuthCodeEntity.php - ClientEntity.php - RefreshTokenEntity.php - ScopeEntity.php - UserEntity.php - Repositories - AccessTokenRepository.php - AuthCodeRepository.php - ClientRepository.php - RefreshTokenRepository.php - ScopeRepository.php - UserRepository.php Repositories Repositories 里主要是处理关于授权码、访问令牌等数据的存储逻辑，oauth2-server 提供了 Interfaces 来定义所需要实现的方法。 class AccessTokenRepository implements AccessTokenRepositoryInterface { /** * @return AccessTokenEntityInterface */ public function getNewToken(ClientEntityInterface $clientEntity, array $scopes, $userIdentifier = null) { // 创建新访问令牌时调用方法 // 需要返回 AccessTokenEntityInterface 对象 // 需要在返回前，向 AccessTokenEntity 传入参数中对应属性 // 示例代码： $accessToken = new AccessTokenEntity(); $accessToken-&gt;setClient($clientEntity); foreach ($scopes as $scope) { $accessToken-&gt;addScope($scope); } $accessToken-&gt;setUserIdentifier($userIdentifier); return $accessToken; } public function persistNewAccessToken(AccessTokenEntityInterface $accessTokenEntity) { // 创建新访问令牌时调用此方法 // 可以用于持久化存储访问令牌，持久化数据库自行选择 // 可以使用参数中的 AccessTokenEntityInterface 对象，获得有价值的信息： // $accessTokenEntity-&gt;getIdentifier(); // 获得令牌唯一标识符 // $accessTokenEntity-&gt;getExpiryDateTime(); // 获得令牌过期时间 // $accessTokenEntity-&gt;getUserIdentifier(); // 获得用户标识符 // $accessTokenEntity-&gt;getScopes(); // 获得权限范围 // $accessTokenEntity-&gt;getClient()-&gt;getIdentifier(); // 获得客户端标识符 } public function revokeAccessToken($tokenId) { // 使用刷新令牌创建新的访问令牌时调用此方法 // 参数为原访问令牌的唯一标识符 // 可将其在持久化存储中过期 } public function isAccessTokenRevoked($tokenId) { // 资源服务器验证访问令牌时将调用此方法 // 用于验证访问令牌是否已被删除 // return true 已删除，false 未删除 return false; } } class AuthCodeRepository implements AuthCodeRepositoryInterface { /** * @return AuthCodeEntityInterface */ public function getNewAuthCode() { // 创建新授权码时调用方法 // 需要返回 AuthCodeEntityInterface 对象 return new AuthCodeEntity(); } public function persistNewAuthCode(AuthCodeEntityInterface $authCodeEntity) { // 创建新授权码时调用此方法 // 可以用于持久化存储授权码，持久化数据库自行选择 // 可以使用参数中的 AuthCodeEntityInterface 对象，获得有价值的信息： // $authCodeEntity-&gt;getIdentifier(); // 获得授权码唯一标识符 // $authCodeEntity-&gt;getExpiryDateTime(); // 获得授权码过期时间 // $authCodeEntity-&gt;getUserIdentifier(); // 获得用户标识符 // $authCodeEntity-&gt;getScopes(); // 获得权限范围 // $authCodeEntity-&gt;getClient()-&gt;getIdentifier(); // 获得客户端标识符 } public function revokeAuthCode($codeId) { // 当使用授权码获取访问令牌时调用此方法 // 可以在此时将授权码从持久化数据库中删除 // 参数为授权码唯一标识符 } public function isAuthCodeRevoked($codeId) { // 当使用授权码获取访问令牌时调用此方法 // 用于验证授权码是否已被删除 // return true 已删除，false 未删除 return false; } } class ClientRepository implements ClientRepositoryInterface { /** * @return ClientEntityInterface */ public function getClientEntity($clientIdentifier, $grantType = null, $clientSecret = null, $mustValidateSecret = true) { // 获取客户端对象时调用方法，用于验证客户端 // 需要返回 ClientEntityInterface 对象 // $clientIdentifier 客户端唯一标识符 // $grantType 代表授权类型，根据类型不同，验证方式也不同 // $clientSecret 代表客户端密钥，是客户端事先在授权服务器中注册时得到的 // $mustValidateSecret 代表是否需要验证客户端密钥 $client = new ClientEntity(); $client-&gt;setIdentifier($clientIdentifier); return $client; } } class RefreshTokenRepository implements RefreshTokenRepositoryInterface { /** * @return RefreshTokenEntityInterface */ public function getNewRefreshToken() { // 创建新授权码时调用方法 // 需要返回 RefreshTokenEntityInterface 对象 return new RefreshTokenEntity(); } public function persistNewRefreshToken(RefreshTokenEntityInterface $refreshTokenEntity) { // 创建新刷新令牌时调用此方法 // 用于持久化存储授刷新令牌 // 可以使用参数中的 RefreshTokenEntityInterface 对象，获得有价值的信息： // $refreshTokenEntity-&gt;getIdentifier(); // 获得刷新令牌唯一标识符 // $refreshTokenEntity-&gt;getExpiryDateTime(); // 获得刷新令牌过期时间 // $refreshTokenEntity-&gt;getAccessToken()-&gt;getIdentifier(); // 获得访问令牌标识符 } public function revokeRefreshToken($tokenId) { // 当使用刷新令牌获取访问令牌时调用此方法 // 原刷新令牌将删除，创建新的刷新令牌 // 参数为原刷新令牌唯一标识 // 可在此删除原刷新令牌 } public function isRefreshTokenRevoked($tokenId) { // 当使用刷新令牌获取访问令牌时调用此方法 // 用于验证刷新令牌是否已被删除 // return true 已删除，false 未删除 return false; } } class ScopeRepository implements ScopeRepositoryInterface { /** * @return ScopeEntityInterface */ public function getScopeEntityByIdentifier($identifier) { // 验证权限是否在权限范围中会调用此方法 // 参数为单个权限标识符 // ...... // 验证成功则返回 ScopeEntityInterface 对象 $scope = new ScopeEntity(); $scope-&gt;setIdentifier($identifier); return $scope; } public function finalizeScopes( array $scopes, $grantType, ClientEntityInterface $clientEntity, $userIdentifier = null ) { // 在创建授权码与访问令牌前会调用此方法 // 用于验证权限范围、授权类型、客户端、用户是否匹配 // 可整合进项目自身的权限控制中 // 必须返回 ScopeEntityInterface 对象可用的 scope 数组 // 示例： // $scope = new ScopeEntity(); // $scope-&gt;setIdentifier('example'); // $scopes[] = $scope; return $scopes; } } class UserRepository implements UserRepositoryInterface { /** * @return UserEntityInterface */ public function getUserEntityByUserCredentials( $username, $password, $grantType, ClientEntityInterface $clientEntity ) { // 验证用户时调用此方法 // 用于验证用户信息是否符合 // 可以验证是否为用户可使用的授权类型($grantType)与客户端($clientEntity) // 验证成功返回 UserEntityInterface 对象 $user = new UserEntity(); $user-&gt;setIdentifier(1); return $user; } } Entities Entities 里是 oauth2-server 处理授权与认证逻辑的类，它为我们提供了 Interfaces 来定义需要实现的方法，同时提供了 Traits 帮助我们实现，可以选择使用，有需要时也可以重写。 class AccessTokenEntity implements AccessTokenEntityInterface { use AccessTokenTrait, TokenEntityTrait, EntityTrait; } class AuthCodeEntity implements AuthCodeEntityInterface { use EntityTrait, TokenEntityTrait, AuthCodeTrait; } class ClientEntity implements ClientEntityInterface { use EntityTrait, ClientTrait; } class RefreshTokenEntity implements RefreshTokenEntityInterface { use RefreshTokenTrait, EntityTrait; } class ScopeEntity implements ScopeEntityInterface { use EntityTrait; // 没有 Trait 实现这个方法，需要自行实现 // oauth2-server 项目的测试代码的实现例子 public function jsonSerialize() { return $this-&gt;getIdentifier(); } } class UserEntity implements UserEntityInterface { use EntityTrait; } Interfaces Repositories League\\OAuth2\\Server\\Repositories\\AccessTokenRepositoryInterface.php League\\OAuth2\\Server\\Repositories\\AuthCodeRepositoryInterface.php League\\OAuth2\\Server\\Repositories\\ClientRepositoryInterface.php League\\OAuth2\\Server\\Repositories\\RefreshTokenRepositoryInterface.php League\\OAuth2\\Server\\Repositories\\ScopeRepositoryInterface.php League\\OAuth2\\Server\\Repositories\\UserRepositoryInterface.php Entities League\\OAuth2\\Server\\Entities\\AccessTokenEntityInterface.php League\\OAuth2\\Server\\Entities\\AuthCodeEntityInterface.php League\\OAuth2\\Server\\Entities\\ClientEntityInterface.php League\\OAuth2\\Server\\Entities\\RefreshTokenEntityInterface.php League\\OAuth2\\Server\\Entities\\ScopeEntityInterface.php League\\OAuth2\\Server\\Entities\\TokenInterface.php League\\OAuth2\\Server\\Entities\\UserEntityInterface.php Traits League\\OAuth2\\Server\\Entities\\Traits\\AccessTokenTrait.php League\\OAuth2\\Server\\Entities\\Traits\\AuthCodeTrait.php League\\OAuth2\\Server\\Entities\\Traits\\ClientTrait.php League\\OAuth2\\Server\\Entities\\Traits\\EntityTrait.php League\\OAuth2\\Server\\Entities\\Traits\\RefreshTokenTrait.php League\\OAuth2\\Server\\Entities\\Traits\\ScopeTrait.php League\\OAuth2\\Server\\Entities\\Traits\\TokenEntityTrait.php 事件 oauth2-server 预设了一些事件，目前官方文档中只有两个，余下的可以在 RequestEvent.php 文件中查看。 client.authentication.failed $server-&gt;getEmitter()-&gt;addListener( 'client.authentication.failed', function (\\League\\OAuth2\\Server\\RequestEvent $event) { // do something } ); 客户端身份验证未通过时触发此事件。你可以在客户端尝试 n 次失败后禁止它一段时间内的再次尝试。 user.authentication.failed $server-&gt;getEmitter()-&gt;addListener( 'user.authentication.failed', function (\\League\\OAuth2\\Server\\RequestEvent $event) { // do something } ); 用户身份验证未通过时触发此事件。你可以通过这里提醒用户重置密码，或尝试 n 次后禁止用户再次尝试。 参考文章 《oauth2-server 官方文档》(https://oauth2.thephpleague.com/) 《理解OAuth 2.0》-阮一峰（http://www.ruanyifeng.com/blog/2014/05/oauth_2_0.html） ","link":"https://cnbailian.github.io/post/oauth2-and-oauth2-package/"},{"title":"Session 与 JWT","content":"关于 REST 与 HTTP 中无状态架构约束的思考之 Session 与 JWT 知识点 无状态 关于无状态，Fielding博士在论文的 3.4.3 与 5.1.3 中有所提及。 《3.4.3 客户-无状态-服务器（Client-Stateless-Server）》 客户-无状态-服务器风格源自客户-服务器风格，并且添加了额外的约束：在服务器组件至上不允许有会话状态（session state） 。从客户发给服务器的每个请求中，都必须包含理解请求所必须的全部信息，不能利用任何保存在服务器上的上下文（content），会话状态应全部保存在客户端。 《5.1.3 无状态》中对于无状态的描述与 3.4.3 一致。 Cookie 说 session 之前先说一下 cookie，因为有太多人将 cookie 与 session 搞混了。 cookie 最初由网景公司开发，是当前识别用户，实现持久会话的最好方式。 可以笼统的将 cookie 分为两类：会话 cookie 和持久 cookie，会话 cookie 是一种临时 cookie，它记录了用户访问站点时的设置和偏好。用户退出浏览器时，会话 cookie 就被删除了。持久 cookie 的生存时间更长一些，它们存储在硬盘上，浏览器退出，计算机重启时它们仍然存在。会话 cookie 和持久 cookie 的唯一区别就是它们的过期时间。 以上是《HTTP权威指南》一书中对于 cookie 的解释。 Session ID 会话 cookie 就是现在人们常说的 session，他的表现形式通常为： 一个很短的由数字和字母组成的字符串键，它和服务器端的某个非常大的数据结构相关联。 恩，上面是《RESTful Web APIs》对于 seesion 的看法，也就是 session ID。这也是我对于这个知识点想说明的，我针对的不是会话 cookie 的运行机制，而是使用字符串来代替会话状态，将会话状态全部存储于服务器端的使用方法。 JWT JSON Web Token（JWT）是一种开放标准（RFC 7519），它定义了一种紧凑且自成一体的方式，使用 JSON 格式安全地传输信息。 这种格式有很多优点，我这里用于与 session ID 来比较的是它的自成一体。 自成一体：这个载体（payload）包含所有的用户请求信息，避免频繁的查询数据库。 是不是似曾相识的描述，没错，无状态也是这个标准：包含理解请求所必须的全部信息。 比较 在我最初的想法里，是没有比较这个概念的，很明显，JWT 是符合无状态架构风格的，且它最常见的使用地点也是在认证授权部分，所以怎么看都是要比 seesion ID 要好用的。于是我去 v2ex 问了一下这个问题，《为什么 session 机制没有被 JWT 所取代?》，很感谢V友们热心回答。经过整理总结，我也理解了 JWT 的不足与 session ID 的优点。 无状态架构的取舍 在说 JWT 与 session ID 的差异之前，先说一下关于无状态架构的取舍。 Fielding博士是通过整理一系列基于网络应用的架构风格，并对其分类评估，而后有所取舍，组合成的 REST。不可能每种架构都只有优点没有缺点，无状态架构也一样，论文的 3.4.3小节 与 5.1.3小节，Fielding博士对于无状态的取舍有着详细的表述。 这一约束产生了可见性，可靠性，可伸缩性三个架构属性，改善了可见性是因为监视系统不必为了确定一个请求的全部性质而去查看该请求之外的多个请求。改善了可靠性是因为它减轻了从局部故障中恢复的任务量。改善了可伸缩性是因为不必在多个请求之间保存状态，从而允许服务器组件迅速释放资源，并进一步简化其实现，因为服务器不必跨多个请求管理资源的使用情况。 与大多数架构抉择一样，无状态这一架构约束反映出了设计上的权衡点。其缺点是：由于不能将状态数据保存在服务器上的共享上下文中，因此增加了在一系列请求中发送的重复数据（每次交互的开销），可能会降低网络性能。此外，将应用状态放在客户端还降低了服务器对于一致的应用行为的控制能力，因为这样一来，应用就得依赖于跨多个客户端版本（semantics across multiple client versions）的语义的正确实现。 顺带一提，原来我认为 cookie 应该是符合无状态的，因为它还是将内容存在客户端的，每次请求都需要携带。但《RESTful Web APIs》中点出了 cookie 的问题，作者认为：客户端一旦接受了一个 cookie，它就应该与随后一定时间内的所有请求一起提交给服务器。服务器也会要求客户端以后不能再发起在接受这个 cookie 之前曾经发起过的请求了。这也违反了无状态原则。 Session ID 与 JWT 的取舍 Session ID 先说 session ID 存在的问题，由于违反了无状态的架构约束，所以很明显的也就没有了可见性，可靠性，可伸缩性。 可见性：客户端不可能在一堆混乱的字符串中得到有用信息，自然没有了可见性。 可靠性：一旦发生故障，可能需要跨越多个请求的结果查询信息，增加了任务量。 可伸缩性：这个词在论文中被提到过很多很多次了，客户-服务器改善了可伸缩性，无状态改善了可伸缩性，缓存改善了可伸缩性等等等等，可见可伸缩性在 Web 中的重要性了。那么 session ID 在哪方面对可伸缩性造成了影响呢？举个简单的例子：负载均衡。这个很常见对吧，做 Web 的应该都有接触，负载均衡就是很常见的横向的可伸缩性。session ID 在这种情况下，就有很明显的问题，他需要每个单元对其进行额外的处理，比如 session 的同步，或者 session 单独剥离出一个额外的单元，还需要为这个额外的单元考虑负载问题。 JWT 再说 JWT 的问题，符合了无状态的架构约束，上面的那三个架构属性都有所改善，问题也正是交互的开销问题，JWT 在有些时候要存储所有完整的会话是不现实的，这就需要存储资源ID，需要进行额外的处理。但是对比 session ID 来说，额外的资源存储要少很多。 额外提一点，这里只讨论了 JWT 的自成一体这一机制，其他不讨论。 取舍 服务器没有了会话状态的保存，涉及到认证授权的一些问题，需要额外进行处理，例如注销，统计在线用户数量等问题。由于有这些问题的存在，处理后的 JWT 很可能变为另一个 session ID，但 session ID 在这方面更加成熟。所以，请根据自身的项目，采取适当的处理方式。 总结 session ID 机制在网站中已经是一套很成熟的系统的。不遵守无状态架构约束所带来的后果也都有着大量的解决方案。JWT 这种符合无状态架构约束的机制，并没有带来很明显的效果，所以很难推广开来。 但是，在 Web API 的领域，session ID 的问题被进一步的凸显，所以应该尽量选择符合无状态架构约束的机制。 ","link":"https://cnbailian.github.io/post/session-and-JWT/"},{"title":"超媒体驱动的 Web API","content":"REST 必须是超媒体驱动的！我在以前关于 RESTful API 的博文中，对于超媒体驱动只有简单的提过，现在来探索这个问题。 超媒体的重要性 超媒体这个概念的基础含义我就不再赘述了，说一说超媒体在 Web 的重要性。 Web（万维网，英文全称 World Wide Web，简称 Web）的成功，很大程度上是因为其软件架构设计满足了拥有互联网规模（Internet-scale）的分布式超媒体系统的需求。 这是Fielding博士在论文中对于 Web 的定义，而 REST 架构风格，正是一直指导着现代 web 架构的设计与开发。这篇论文中提起的所有架构属性，都是基于 互联网规模（Internet-scale） 的 分布式 超媒体 系统的需求来进行分析。 超媒体是 REST 的基点之一。 WEB API 中的超媒体 超媒体对于 web 的重要性显而易见了，但是在 web API 中超媒体却发展的不是很理想，很多不是超媒体驱动的 web API 也自称 RESTful API，Fielding博士在2008年10月20日，发表了一篇文章：《REST APIs must be hypertext-driven》，博士在文章中与下方的评论中阐述了超媒体的重要性。毫无疑问，REST 这个 buzzword 的专利是属于Fielding博士的，嗯，既然不符合标准我以前的博文还是换个 buzzword 吧，改为 HTTP API 好了。 关于超媒体与超文本，以下是Fielding博士在评论中的回复 When I say hypertext, I mean the simultaneous presentation of information and controls such that the information becomes the affordance through which the user (or automaton) obtains choices and selects actions. Hypermedia is just an expansion on what text means to include temporal anchors within a media stream; most researchers have dropped the distinction. 超媒体的作用 论文中给出的正式的超媒体的定义： 超媒体（hypermedia）是由应用控制信息（application control information）来定义的，这些控制信息内嵌在信息的表达（the presentation of information）之中，或者作为信息的表达之上的一层。 超媒体将资源互相连接起来，并以机器可读的方式来描述它们的能力。举个例子，HTML 的 标签： &lt;a href=&quot;http://www.baidu.com/&quot;&gt;点击我将跳转至百度&lt;/a&gt; 这是一个简单的超媒体控件，它向浏览器传达了它可以发起这样的一个请求： GET / HTTP/1.1 Host: www.baidu.com 这就是机器可读的超媒体格式，HTML 还有其他更多更复杂的超媒体控件。 超媒体是服务器用以和客户端进行对话的一种方式，客户端从而可以知道未来将可以向服务器发起什么样的请求。它就是一个由服务器提供的菜单，客户端可以从中自由的进行选择。服务器通常都知道可能发生哪些事，但是客户端将决定实际发生什么。其实这并不是什么新鲜的话题，我们的万维网就是以这种方式工作的，并且我们都想当然地认为它就应该是这样工作的，其他的任何方式都是一种无用的历史的倒退。但是在 API 的世界里，超媒体仍然是一个令人难以理解且富有争议的话题。这也说明了为什么如今的 API 在应对变化时还是显得如此糟糕。 以上出自《RESTful Web APIs》第四章。想一想现在的大部分 web API，是不是就是缺少超媒体，缺少机器可读的方式，只有人类可读的文档。 通过合理地使用超媒体，便可以解决或至少是改善现今 web API 存在的可用性和稳定性问题。 Web API 存在的问题 Web API 经常有大量的阅读文档来告诉你如何为不同的资管构造 URL。这违背了 REST 的连通性与自描述信息的原则。 要集成一个新的 API，不可避免的就要编写新的定制化软件，或者安装别人编写的代码库，哪怕这个新的 API 也叫作 RESTful API，哪怕这个新的 API 是你原来用过的同一个公司的不同产品。 Web API 发生了变化以后，定制化的 API 客户端就不能正常使用了，需要维护者进行一些代码修复。对比来看，当网站重新设计改版，用户可能会抱怨，然后慢慢适应。在这期间，浏览器可不会停止工作。 解决方案 目前已经有了一些超媒体解决方案，《RESTful Web APIs》中有着详细的介绍，我还没有完全理解，先不讲出来误人子弟了。 困惑-Web API的发展方向 目前在我看来，有着超媒体的 web API 以后的发展方向就是：传输数据与自描述消息，但是展现形式却由客户端来定义，如果展现也是服务器给出的话，那 API 客户端也就是另一个浏览器了。可这绝对是不符合大部分 API 提供者的利益，或许这种理想化的情况只能出现在以后的人工智能中吗？ 总结 没有给出超媒体驱动的解决方案，只是讲了我对于超媒体以及 web API 的一些观点，对于实现这里还没有完全理解，日后有所收获了再分享。 ","link":"https://cnbailian.github.io/post/hypertext-friven-web-API/"},{"title":"RESTful API 实践","content":"REST 是Fielding博士在他的论文[1]中提出的一种新的架构风格，被称作表述性状态移交（Representational State Transfer）架构风格，它成为了现代 Web 架构的基础。[2] 符合 REST 原则的应用程序或设计称做 RESTful。 由于不符合超媒体，现在是 HTTP API。 RESTful API 设计原则: 无状态 通信必须在本质上是无状态的.无状态指的是任意一个Web请求必须完全与其他请求隔离，当客户端提出请求时，请求本身包含了这一请求所需的全部信息，会话状态因此要全部保存在客户端。 有状态和无状态与请求本身没有多大关联，重要的是状态信息是由请求方还是响应方负责保存。 对于web的融入 这个API应该能够使用浏览器来进行所有测试，能够方便的使用web功能测试、性能测试等工具进行测试，web类应用也方便将多个RESTful API进行整合。 资源 对于资源的抽象，是设计RESTful API的核心内容，资源就是&quot;Representational State Transfer&quot;这个词组中被省略的主语。 REST 对于信息的核心抽象是资源.任何能够被命名的信息都能够作为一个资源:一份文档或一张图片、一个与时间相关的服务（例如，“洛杉矶今日的天气”）、一个其他资源的集合、一个非虚拟的对象（例如，人）等等。 缓存 应当具有良好的缓存机制，HTTP的缓存机制就是个不错的选择。 低耦合 REST的几个特征保证了RESTful API的低耦合性， 对于资源（Resource）的抽象、统一接口（Uniform Interface）、超文本驱动（Hypertext Driven）[3]。 RESTful API 实践 请求 RESTful 使用HTTP动词操作资源。 常用的HTTP动词有下面四个[4] GET - 用于获取资源信息 POST - 用于新建或修改资源 PUT - 用于新建或修改资源 DELETE - 用于删除资源 幂等性(Idempotent) 幂等性是指: 多次请求所得到的结果与一次请求所得到的结果相同. 上方具有幂等性的方法有GET/PUT/DELETE。 用一个点赞的示例来说明幂等性 &amp; POST与PUT的区别: 当这个点赞一个人只能点赞一次的时候，应使用PUT方法，这个点赞是具有幂等性的，当一个人可以点赞无数次时，应使用POST方法。 PUT方法的幂等性使我们能更好的处理逻辑 常见操作示例 GET /user - 获取用户列表 GET /user/uid - 获取指定用户 GET /user/uid/comments - 获取指定用户的评论 POST /user - 新建一个用户 PUT /user/uid - 修改指定用户 DELETE /user/uid - 删除指定用户 URL的设计原则:资源不能使用动词、路径不宜过深，二三层即可，过深可以使用参数的方式来代表、永远使用最短的路径 不被支持的HTTP动词 有些情况下会只支持GET&amp;POST方法(HTML的FORM标签method属性)，可以在头信息中加入 X-HTTP-Method-Override 来表示当前的HTTP请求或在请求参数中加入 _method 来表示当前请求(laravel框架使用的此方法)。 SSL/TLS 条件允许的情况下，永远使用SSL/TLS! 好处不多说了，再说一点: 不要将HTTP重定向到HTTPS，抛出错误就好，因为第一次的 HTTP 请求就有可能被劫持，导致请求无法到达服务器，从而构成 HTTPS 降级劫持。 文档 文档应简单易懂并具有良好的示例，粘贴至浏览器能直接使用。 这里推荐使用Postman，很好用的调试RESTful API的chrome应用。 版本号 API不会是永远稳定的，版本升级的问题无法避免。 版本号只允许枚举，不允许区间。 关于API的版本号问题，有两种解决方案: 放入URL中. 优点是更加直观些。 放入Header 信息中.URL更加优雅，api.github.com采用此方法。 信息过滤 包括 筛选、排序、分页等。 筛选 GET /users?name=张三 - 筛选出所有 name = 张三 的用户。 为了使接口调用者更加方便，可以将一些常见的查询参数使用别名表示: GET /users/vip - 筛选出所有 vip 用户。 如果业务过于复杂导致普通的查询参数无法胜任，可以试着查询参数json化，虽然不标准，但是已解决问题为主。 限制返回字段 ?fields=id，name 排序 两种解决方案，第一种是拆分为两个参数: ?sortby=level&amp;order=asc 第二种，使用 - 表示倒序，使用 ， 分隔多个排序: ?sort=-type，created_at 分页 常见的分页解决方案有两种，第一种是传统的 offset + limit : ?offset=10 - 偏移量 ?limit=10 - 返回数量 第二种是使用游标分页，需提供 cursor (下一页的游标) 与 limit : ?cursor=2015-01-01 15:20:30 - 使用时间作为游标 ?limit=10 - 返回数量 总结一下传统分页的特点: 传统分页可以进行跳页。 会出现重复数据问题。 当offset数值较大时，效率降低明显。 分页不涉及排序。 我认为使用游标的分页方式受众面比较小，例如想要作为游标的字段有着重复的数据，不能适应负责的排序等.多数情况下，不推荐使用。 在实践中发现 重复数据 的问题有些严重，我的解决方案是增加首次分页的时间作为查询条件，取所有小于这个时间的数据.缺点是会造成后续新增数据只有在刷新后才显示。 HATEOAS HATEOAS(超媒体即应用状态引擎 Hypermedia as the Engine of Application State)， REST的重要原则之一。 在 RESTful 中表现为: 返回结果中提供链接，连向其他API方法，比如，访问 api.github.com : { &quot;current_user_url&quot;: &quot;https://api.github.com/user&quot;， ...... } 理应作为RESTful的设计原则之一，也看了在API中的用处[5]，但是在实践中感觉用处不是很大，碰到业务逻辑比较复杂的地方用起来也很麻烦.还是视情况应用吧。 JSON格式请求 请求头的 Content-Type 设置为 application/json 告诉服务器端消息主体为 json 格式数据 关于优缺点不多做描述， QuQu大神 讲解的很详细了[6]。 相关资源嵌入 API在使用过程中，不可避免的要遇到需要加载相关数据的情况，比如说获取一个用户信息的同时获取这个用户相关的部门信息，这个时候让客户端再请求一次部门的接口是不友好的，也不能在用户信息里加入相关部门信息。 解决方案是在请求参数中加入 embed 来表示相关资源的嵌入。 ?embed=department.name，job 使用 . 表示相关字段， 使用 ， 分割资源列表。 这个请求的返回应该是: { &quot;id&quot; : 1， &quot;name&quot; : &quot;user&quot;， &quot;department&quot; : { &quot;name&quot; : &quot;xxx&quot; }， &quot;job&quot; : { &quot;id&quot; : 1， &quot;name&quot; : &quot;xxxx&quot; } } 没有使用过HTTP2.0协议，不清楚 2.0 的多路复用(Multiplexing) 会不会适用此场景。 限流 用户在一定时间内发出的请求次数要做出限制。 具体算法可以看大神博客[7]。 这里主要说的是 RESTful API 在这方面做出的处理: 在 HTTP status code 中有 429 专用于返回此种错误。 同时应在响应头中提示用户，命名没有一定的规范，但也要遵守基本法，不要胡乱取名，示例: X-Rate-Limit-Limit - 周期内允许请求的总数 X-Rate-Limit-Remaining - 周期内剩余可请求次数 X-Rate-Limit-Reset - 周期剩余时间 stackoverflow 上有关于这个问题的讨论[8]。 不要使用UNIX时间戳[9]。 权限 REST的重要原则之一就是无状态，所以不应该使用 cookie &amp; session ， 而是使用 凭证 来进行权限认证。 对外的接口应使用 OAuth2.0 框架[10]，作为API的权限控制， 对内接口也可使用简化版的 OAuth2.0。 关于 为什么 session 是 不符合REST原则的 而 凭证 又是符合REST原则， 这里的答案可以进行参照[11]。 我的总结: 语义不同。 sessionID 作为一种 标识着某个会话的KEY，给服务端传递请求的语义为:请帮我取出这个信息，在这里，信息是由服务端进行存储的，所以，毫无疑问这是违反REST无状态原则的。 而凭据呢，是服务端期待着客户端传过来的用户验证身份的凭据，是由客户端进行存储的，所以是符合REST原则的。 缓存 HTTP已经为我们提供了很好的解决方案 ETag &amp; Last-Modified。 这里有一片讲解 ETag 的文章， 非常详细[12]。 Last-Modified 基本与 ETag 相同，只是判断依据从 ETag 变为时间。 响应 始终返回适当的HTTP状态码 使用 JSON 作为唯一的返回格式 正确(200 系)时返回所请求的数据即可，不要返回 code = 0 这样的无用信息。 错误时返回具体的错误信息: { 'message': 'Invalid Token' } 业务逻辑较为复杂时，返回业务逻辑错误码: { 'code' : 10010， 'message': 'Insufficient balance' } 业务逻辑错误码应与HTTP状态码不重复 当你认为 HTTP status code 不够用时， 应该想着去使用 业务逻辑码 来解决问题，而不是自定义HTTP status code 。 Enveloping 应考虑到客户端无法获取头信息的情况，这是要将头信息包含在实体中返回(例如 JSONP)。 请求参数 ?enveloping=jsonp callback_function({ status_code: 200， next_page: &quot;https://..&quot;， response: { ... 正常的 JSON 实体 ... } }) 创建 &amp; 更新 创建 &amp; 更新的请求(POST &amp; PUT) 应返回该资源的内容。 常用的 HTTP status codes 200 OK - 请求成功，并且请求结果已返回至响应实体中 201 Created - 请求已经被实现，而且有一个新的资源已经依据请求的需要而创建，其URI会返回在头信息中 202 Accepted - 请求已被接受，但尚未处理，请求有可能会拒绝执行 (例如 异步请求) 204 No Content - 请求成功，但没有内容返回 (例如 DELET 请求) 304 Not Modified - 请求的资源没有修改 适用于请求 缓存 没有变化的资源 400 Bad Request - 请求的格式不正确 客户端不应该重复该请求 401 Unauthorized - 需要客户端提供身份凭据 403 Forbidden - 客户端提供了身份凭据，但权限不足 404 Not Found - 请求的资源不存在 405 Method Not Allowed - 不存在当前请求的HTTP动词 410 Gone - 请求的资源已废弃，并且没有对应新资源 (如果是转到了新的URL，应使用 301 Moved Permanently) 422 Unprocessable Entity - 通常用于请求格式正确，但是用户输入的值不符合服务端的需求 (表单验证) 429 Too Many Requests 请求的速率超过限制 美化返回结果 可以提供一个参数，让返回的结果是格式化后的 JSON 数据，便于使用者的调试。 如果你是一个使用者，并且api没有提供一个这样的参数的情况下，可以使用chrome扩展(搜索 JSON VIEW 此类的关键词)或Postman 来进行调试。 实践出真知 在上面常见的HTTP动词中，我没有提到 PATCH， 是因为我在实际使用中没有碰到要划分 PUT 与 PATH 的情况，所以，觉得这个方法不是那么主要。 客户端要求不能直接返回数组型的 JSON 数据， 要加上 KEY 。有时间可以学一学，做个小APP实践一下。 当你认为你的业务不属于以上HTTP动词的范畴中时 对业务进行深层次解读，或拆分。 例如 : search 我的理解， 某种服务也是资源。 例如 : login 我的理解， 对于一个 API 来说， login 的行为在本质上是对 凭据 这个资源的创建，而且是幂等的。 业务逻辑与代码逻辑冲突时，HTTP动词的选择 跟着业务逻辑走。 例如，业务的删除，在代码里的时间可能是软删除( deleted_at =1 )，这个时候的HTTP动词一定要是 DELETE。 写在最后 在我看来， RESTful 真的很好，简单直观，规范易懂，贴合web，更易于测试 等等等等， 但是 毕竟只是架构风格，过度纠结如何遵守规范反而是违背了设计API的初衷.我在实践中也有很多不遵守的地方，也曾经纠结过很多东西，事后证明大部分不过是浪费时间罢了。 API的设计本身就是要从使用者的角度出发，如果是对外的接口，要尽量做的规范，这样能适应大多数人，对内的接口还是要多听取下使用者的意见，针对于本身的业务来进行调整。 This 本编文章是总结我对于REST与RESTful的理解，与实践结合而成。 《理解 REST 与 RESTful》 一章来源与我看过博士的论文(英语不好，看的中文译本)后，融合网络上的RESTful规范理解而成。 《RESTful API 实践》 一章的来源: 大部分知识点源自我对于 《Best Practices for Designing a Pragmatic RESTful API》的理解，剩余部分是在网络中整理提炼出来的知识，将两者整合，应用于实践后的我的理解。 如果你认为有些地方有问题，或者歧义比较大的话，还望指出，thanks. 因为要整理的知识点很多，也很杂乱，所以写了一个chrome扩展来帮助自己做笔记，没有上架，还在完善中，有兴趣的同学可以去我的github上down来试一试。 终于写完了,断断续续的写了好几天,在以后的工作过程中,如果还有新的内容,也会一并更新在这里 - 2016-12-30 参考文章 Best Practices for Designing a Pragmatic RESTful API (本篇文章的大部分知识点来源于此，对于我理解应用RESTful有着很大的帮助) 架构风格与基于网络应用软件的架构设计（中文修订版） (Fielding博士的论文中文版 因为英语不太好，所以没有强行去看原版) Architectural Styles and the Design of Network-based Software Architectures (Fielding博士论文原版) 理解本真的REST架构风格 (帮助我理解了REST概念) 我所认为的RESTful API最佳实践 (找到的国内RESTful API实践中较好的一篇文章) RESTful API 设计指南 (使我了解了RESTful API，可惜东西讲解的有些少) 理解RESTful架构 (RESTful基础) 理解OAuth 2.0 (理解OAuth2.0， 有这篇文章就够了) 对于REST中无状态(stateless)的一点认识 (加深了我对于 无状态 的理解) 注 Fielding博士论文 英文原版 中文译版 HTTP 我们在互联网工程工作组（IETF）定义了现有的超文本移交协议（HTTP/1.0）， 并且为 HTTP/1.1 和 URI（统一资源标识符）的新规范设计扩展。 在开展这些工作的最初阶段，我就认识到需要建立一个关于 Web 的运转方式的模型。 这个关于整个 Web 应用中的交互的理想化模型，被称作表述性状态移交（REST）架构风格， 它成为了现代 Web 架构的基础。 出自[中文译版] 结论 低耦合性 理解本真的REST架构风格 REST特性讲解 所有的HTTP动词 ALL HATEOAS 在RESTFul API中使用HATEOAS的好处 JSON格式输入 QuQu大神博文 流量控制与令牌桶算法 潘神- 流量控制与令牌桶算法 缓存头信息返回方案 讨论 为什么不要使用UNIX时间戳 为什么不要使用UNIX时间戳 OAuth2.0 大神博客 - 理解OAuth 2.0 REST无状态(stateless)原则 对于REST中无状态(stateless)的一点认识 Etag详解 HTTP缓存 这里主要是讲的Etag ","link":"https://cnbailian.github.io/post/best-practices-for-RESTful-API/"},{"title":"HTTP 缓存小记","content":"参考文章:https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=zh-cn http的缓存有两种方案ETag与Last-Modified 本文讲的是第一种,也就是参考文章里所讲的 ETag缓存方案的实现很简单,服务器返回响应头的时,加上缓存时间与验证令牌(ETag),如有需要还可以加上内容类型、长度等 200 OK Cache-Controller: max-age=60 ETag: &quot;xxxxxxxxxxxxx&quot; Content-Lenght: 1024 上面的示例代码中,服务器告诉了客户端,可以缓存60秒,验证令牌为:xxxxxxxxxxxxx,内容长度是1024 客户端在收到响应头时,可在本地缓存一个60秒的内容,60秒后,重新访问资源,并在请求头中附带If-None-Match值为前文收到的验证令牌. 服务器根据客户端提供的验证令牌,判断资源是否有更新,如果没有更新,则返回304+空的内容,响应头中附带新的可缓存时间,如果资源有更新,则返回200+资源内容,响应头中附带缓存时间与新的验证令牌 资源无更新示例: 304 Not Modified Cache-Controller: max-age=60 资源有更新示例: 200 OK Cache-Controller: max-age=60 ETag: &quot;ooooooooooooo&quot; Content-Lenght: 1024 更详细的缓存方案也是半懂不懂,有时间应该买本《HTTP权威指南》. ","link":"https://cnbailian.github.io/post/http-cache/"}]}