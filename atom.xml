<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://cnbailian.github.io</id>
    <title>白联</title>
    <updated>2021-05-08T03:18:12.427Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://cnbailian.github.io"/>
    <link rel="self" href="https://cnbailian.github.io/atom.xml"/>
    <subtitle>努力前行</subtitle>
    <logo>https://cnbailian.github.io/images/avatar.png</logo>
    <icon>https://cnbailian.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 白联</rights>
    <entry>
        <title type="html"><![CDATA[从 SampleController 项目看 kubernetes controller 的设计——笔记]]></title>
        <id>https://cnbailian.github.io/post/kubernetes-samplecontroller/</id>
        <link href="https://cnbailian.github.io/post/kubernetes-samplecontroller/">
        </link>
        <updated>2021-05-08T03:10:53.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="总结">总结</h2>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glrrw7x90hj31va0ton44.jpg" alt="Kubernetes Informer" loading="lazy"></figure>
]]></summary>
        <content type="html"><![CDATA[<h2 id="总结">总结</h2>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glrrw7x90hj31va0ton44.jpg" alt="Kubernetes Informer" loading="lazy"></figure>
<!--more-->
<h2 id="设计理念">设计理念</h2>
<p>client-go informer</p>
<h3 id="kubernetes-resource-type">Kubernetes Resource Type</h3>
<p><strong>Scheme</strong></p>
<p>Scheme 提供了 Go type 与对应 GVK 的映射。即给定 Go type 就知道对应 GVK，给定 GVK 就知道对应 Go type</p>
<h3 id="informer">Informer</h3>
<p><strong>list/watch 机制</strong></p>
<p>在 kubernetes 的设计中，使用 etcd 存储数据，apiserver 作为统一入口，任何对资源的操作都必须经过 apiserver。</p>
<p>apiserver 对资源提供了 list watch 两个接口。list 基于 HTTP 短链接实现，用于获取资源列表。watch 基于 HTTP 长链接实现，用于获取资源的变更。</p>
<p>watch 基于 HTTP chunked 实现持久链接。服务端每次传输资源的事件信息。</p>
<p>设计理念</p>
<p>通过 list watch 的组合，保证了消息的可靠性，避免因为消息丢失而造成状态不一致场景。</p>
<p>消息必须是实时的，每当 apiserver 产生资源变更事件，都会将事件实时的推送给客户端，保证了消息的实时性。</p>
<p>kubernetes 在每个资源的事件都有一个 resourceVersion 属性，这个属性是递增的数字，所以当客户端并发处理同一资源的事件时，它可以通过对比 resourceVersion 来保证消息的顺序性。</p>
<p>通过 list 获取资源，写入 cache，然后通过 watch 维护缓存，避免了频繁获取资源的性能损耗。通过 resyncPeriod 维护 list，避免发生不一致现象。</p>
<p><strong>informer 工作流程</strong></p>
<ol>
<li>Informer 使用 Reflector 包建立与 apiserver 的连接。Reflector 使用 ListAndWatch 方法监听该分类下所有资源对象，list 首先会将 resourceVersion 设为 0，然后通过 watch 监听该 resourceVersion 之后的所有变化，若中途出现异常，Reflector 会从断开处尝试重现所有变化。当 Reflector watch 到资源对象的事件通知时，会将该事件与它对应的资源对象这个组合（被称为增量 Delta），放入 DeltaFIFO 队列中。</li>
<li>Informer 会 pop 这个 DeltaFIFO 队列中的 Deltas，通过 Indexer 根据事件类型更新缓存。</li>
<li>同时也会去调用事先注册的 ResourceEventHandler 回调函数进行处理。</li>
</ol>
<p><strong>Custom Controller 工作流程</strong></p>
<ol>
<li>在 ResourceEventhandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放在 workqueue 里面</li>
<li>Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑</li>
<li>在 worker 中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中的 resource 的变更都会反映到本地的 cache 中</li>
</ol>
<h2 id="源码">源码</h2>
<p>结合《Kubernetes 源码剖析》和 sampleController 的实际使用来学习 Informer</p>
<h3 id="使用">使用</h3>
<p>通过 <code>k8s.io/client-go/informers</code> 或生成的 <code>informers</code> 调用 <code>NewSharedInformerFactory</code> 创建 InformerFactory。</p>
<pre><code class="language-go">kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)
exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30)
</code></pre>
<p>对具体的 Resources 添加 Events，也就是事件回调函数，正常情况下，在回调中需要添加到 workQueue 中</p>
<p>事件分为三种：Added、Updated、Deleted。那么 kubebuilder 的 generic 是什么...</p>
<pre><code class="language-go">kubeInformerFactory.Apps().V1().Deployments().Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.handleObject,
		UpdateFunc: func(old, new interface{}) {
			newDepl := new.(*appsv1.Deployment)
			oldDepl := old.(*appsv1.Deployment)
			if newDepl.ResourceVersion == oldDepl.ResourceVersion {
				// Periodic resync will send update events for all known Deployments.
				// Two different versions of the same Deployment will always have different RVs.
				return
			}
			controller.handleObject(new)
		},
		DeleteFunc: controller.handleObject,
	})

exampleInformerFactory.Samplecontroller().V1alpha1().Foos().Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: controller.enqueueFoo,
		UpdateFunc: func(old, new interface{}) {
			controller.enqueueFoo(new)
		},
	})


func (c *Controller) enqueueFoo(obj interface{}) {
	var key string
	var err error
	if key, err = cache.MetaNamespaceKeyFunc(obj); err != nil {
		utilruntime.HandleError(err)
		return
	}
	c.workqueue.Add(key)
}
</code></pre>
<p>最后 start，因为 informer 是持久运行的，所以需要通过 channel 来发送结束信号</p>
<pre><code class="language-go">kubeInformerFactory.Start(stopCh)
exampleInformerFactory.Start(stopCh)
</code></pre>
<h3 id="sharedinformerfactory">SharedInformerFactory</h3>
<p><code>informers.NewSharedInformerFactory</code> 函数实例化了 <code>SharedInformerFactory</code> 对象，它接收两个参数：第1个参数 <code>clientset</code> 是用于与Kubernetes API Server交互的客户端，第2个参数 <code>time.Minute</code> 用于设置多久进行一次 resync（重新同步），resync 会周期性地执行 List 操作，将所有的资源存放在 <code>Informer Store</code> 中，如果该参数为0，则禁用 resync 功能。</p>
<pre><code class="language-go">func NewSharedInformerFactory(client kubernetes.Interface, defaultResync time.Duration) SharedInformerFactory {
	return NewSharedInformerFactoryWithOptions(client, defaultResync)
}
func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory {
	factory := &amp;sharedInformerFactory{
		client:           client,
		namespace:        v1.NamespaceAll,
		defaultResync:    defaultResync,
		informers:        make(map[reflect.Type]cache.SharedIndexInformer),
		startedInformers: make(map[reflect.Type]bool),
		customResync:     make(map[reflect.Type]time.Duration),
	}

	// Apply all options
  // 如果不熟悉这种参数传递模式，可以参考 Rob Pike 的文章：https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html
  // 相关文章中文版：https://driverzhang.github.io/post/golang友好的设计api参数可选项/
	for _, opt := range options {
		factory = opt(factory)
	}

	return factory
}
</code></pre>
<p><strong>Informer Shared 机制</strong></p>
<p>从上面的代码中可以看出，我们 New 的是一个 SharedInformerFactory，它是可以被共享使用的。</p>
<p>Shared Informer Factory 可以使同一类资源共享一个 Informer，这样可以节约很多资源。</p>
<pre><code class="language-go">// 实际上是调用 factory 的 InformerFor 方法
kubeInformerFactory.Apps().V1().Deployments().Informer()

func (f *deploymentInformer) Informer() cache.SharedIndexInformer {
	return f.factory.InformerFor(&amp;appsv1.Deployment{}, f.defaultInformer)
}

type sharedInformerFactory struct {
  ......
	informers map[reflect.Type]cache.SharedIndexInformer
  ......
}

// InformerFor 通过 map 数据结构存储 Informer，多次添加也会共享一个 informer。
func (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer {
	f.lock.Lock()
	defer f.lock.Unlock()

	informerType := reflect.TypeOf(obj)
	informer, exists := f.informers[informerType]
	if exists {
		return informer
	}

	resyncPeriod, exists := f.customResync[informerType]
	if !exists {
		resyncPeriod = f.defaultResync
	}

	informer = newFunc(f.client, resyncPeriod)
	f.informers[informerType] = informer

	return informer
}
</code></pre>
<p>上面可以看出，sharedInformerFactory 的 InformerFor 方法会实例化一个 informer 放入 <code>f.informers</code> 中，看下 deployment 传入的 <code>newFunc</code> 是什么：</p>
<pre><code class="language-go">// 可以看到，传入的是 deploymentInformer.defaultInformer 的闭包
func (f *deploymentInformer) Informer() cache.SharedIndexInformer {
	return f.factory.InformerFor(&amp;appsv1.Deployment{}, f.defaultInformer)
}

// defaultInformer 的参数与 NewSharedInformerFactory 的参数一致，用途也一致
func (f *deploymentInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	return NewFilteredDeploymentInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
}

// 实例化 SharedIndexInformer，传入对应资源的 List and Watch
func NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&amp;cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)
			},
		},
		&amp;appsv1.Deployment{},
		resyncPeriod,
		indexers,
	)
}
</code></pre>
<h3 id="sharedindexinformer">SharedIndexInformer</h3>
<p>最后，来看下 informer 的 Start，以及 informer 如何利用 List and Watch。</p>
<pre><code class="language-go">func (f *sharedInformerFactory) Start(stopCh &lt;-chan struct{}) {
	f.lock.Lock()
	defer f.lock.Unlock()

	for informerType, informer := range f.informers {
		if !f.startedInformers[informerType] {
			go informer.Run(stopCh)
			f.startedInformers[informerType] = true
		}
	}
}

// 从上面的代码中可以看到 informer 是传入的 SharedIndexInformer
// ShareIndexInformer 有三个主要组件：
// 第一个 indexed local cache；Indexer
// 第二个是 controller，它使用 ListerWatcher 获取资源，并将其推送到 DeltaFIFO 中
// 同时从 FIFO 中取出 Deltas values，并通过 sharedIndexInformer::HandleDeltas 方法处理
// 每个 Deltas，都会更新 local cache，并将相关通知发送给 sharedProcessor
// 第三个组件就是 sharedProcessor，它会负责转发这些通知给 listeners
func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()

  // deltaFIFO 可以分开理解
  // FIFO 是一个先进先出的队列
  // Delta 代表队列中存储的是 Delta 对象，Delta 是一个资源对象存储，它可以保存资源对象的操作类型
  // 例如 Added、Updated、Deleted、Sync 等操作类型
	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          s.indexer,
		EmitDeltaTypeReplaced: true,
	})

	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    s.listerWatcher,
		ObjectType:       s.objectType,
		FullResyncPeriod: s.resyncCheckPeriod,
		RetryOnError:     false,
		ShouldResync:     s.processor.shouldResync,

		Process:           s.HandleDeltas,
		WatchErrorHandler: s.watchErrorHandler,
	}

	func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()
		// 实例化 controller 组件
		s.controller = New(cfg)
		s.controller.(*controller).clock = s.clock
		s.started = true
	}()

  // 启动 processor 组件
	// Separate stop channel because Processor should be stopped strictly after controller
	processorStopCh := make(chan struct{})
	var wg wait.Group
	defer wg.Wait()              // Wait for Processor to stop
	defer close(processorStopCh) // Tell Processor to stop
	wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)
	wg.StartWithChannel(processorStopCh, s.processor.run)

	defer func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()
		s.stopped = true // Don't want any new listeners
	}()
	s.controller.Run(stopCh)
}
</code></pre>
<h4 id="reflector">Reflector</h4>
<p>controller 组件的主要功能就是通过 reflector 完成。</p>
<p>Reflector 用于监控指定资源，当监控的资源发生变化时，触发相应的变更事件，例如 Added、Updated、Deleted，并将其资源对象存放到 DeltaFIFO 中。</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()
	go func() {
		&lt;-stopCh
		c.config.Queue.Close()
	}()
  // NewReflector 实例化过程中需要传入 ListerWatcher
  // 这是对应资源对象在实例化 NewSharedIndexInformer 时传入的，实现了对应资源的 List and Watch 接口
  // Queue 是上面实例化的 DealtaFIFO
	r := NewReflector(
		c.config.ListerWatcher,
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
	r.ShouldResync = c.config.ShouldResync
	r.WatchListPageSize = c.config.WatchListPageSize
	r.clock = c.clock
	if c.config.WatchErrorHandler != nil {
		r.watchErrorHandler = c.config.WatchErrorHandler
	}

	c.reflectorMutex.Lock()
	c.reflector = r
	c.reflectorMutex.Unlock()

	var wg wait.Group
  // 启动 reflector
	wg.StartWithChannel(stopCh, r.Run)
  // 启动 processor loop
	wait.Until(c.processLoop, time.Second, stopCh)
	wg.Wait()
}
</code></pre>
<p><strong>Reflector run</strong></p>
<pre><code class="language-go">func (r *Reflector) Run(stopCh &lt;-chan struct{}) {
	klog.V(2).Infof(&quot;Starting reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
	wait.BackoffUntil(func() {
		if err := r.ListAndWatch(stopCh); err != nil {
			r.watchErrorHandler(r, err)
		}
	}, r.backoffManager, true, stopCh)
	klog.V(2).Infof(&quot;Stopping reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
}

// ListAndWatch 函数实现可分为两部分：第一部分获取列表数据，第二部分监控资源对象
func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct{}) error {
	klog.V(3).Infof(&quot;Listing and watching %v from %s&quot;, r.expectedTypeName, r.name)
	var resourceVersion string

	options := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}
  // 第一部分：获取列表数据
	if err := func() error {
		initTrace := trace.New(&quot;Reflector ListAndWatch&quot;, trace.Field{&quot;name&quot;, r.name})
		defer initTrace.LogIfLong(10 * time.Second)
		var list runtime.Object
		var paginatedResult bool
		var err error
		listCh := make(chan struct{}, 1)
		panicCh := make(chan interface{}, 1)
		go func() {
			defer func() {
				if r := recover(); r != nil {
					panicCh &lt;- r
				}
			}()
			// Attempt to gather list in chunks, if supported by listerWatcher, if not, the first
			// list request will return the full response.
			pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) {
				return r.listerWatcher.List(opts)
			}))
			switch {
			case r.WatchListPageSize != 0:
				pager.PageSize = r.WatchListPageSize
			case r.paginatedResult:
				// We got a paginated result initially. Assume this resource and server honor
				// paging requests (i.e. watch cache is probably disabled) and leave the default
				// pager size set.
			case options.ResourceVersion != &quot;&quot; &amp;&amp; options.ResourceVersion != &quot;0&quot;:
				// User didn't explicitly request pagination.
				//
				// With ResourceVersion != &quot;&quot;, we have a possibility to list from watch cache,
				// but we do that (for ResourceVersion != &quot;0&quot;) only if Limit is unset.
				// To avoid thundering herd on etcd (e.g. on master upgrades), we explicitly
				// switch off pagination to force listing from watch cache (if enabled).
				// With the existing semantic of RV (result is at least as fresh as provided RV),
				// this is correct and doesn't lead to going back in time.
				//
				// We also don't turn off pagination for ResourceVersion=&quot;0&quot;, since watch cache
				// is ignoring Limit in that case anyway, and if watch cache is not enabled
				// we don't introduce regression.
				pager.PageSize = 0
			}
			// 获取 list 数据，获取资源数据是由 options.ResourcesVersion 参数控制的
      // 如果 ResourceVersion 为 0，则表示获取所有资源数据；如果 ResourceVersion 非0，则表示根据资源版本号继续获取
      // 功能类似于断点续传，当传输过程中遇到网络故障导致中断，下次再连接时，会根据资源版本号继续传输未完成的部分
			list, paginatedResult, err = pager.List(context.Background(), options)
			if isExpiredError(err) || isTooLargeResourceVersionError(err) {
				r.setIsLastSyncResourceVersionUnavailable(true)
				// Retry immediately if the resource version used to list is unavailable.
				// The pager already falls back to full list if paginated list calls fail due to an &quot;Expired&quot; error on
				// continuation pages, but the pager might not be enabled, the full list might fail because the
				// resource version it is listing at is expired or the cache may not yet be synced to the provided
				// resource version. So we need to fallback to resourceVersion=&quot;&quot; in all to recover and ensure
				// the reflector makes forward progress.
				list, paginatedResult, err = pager.List(context.Background(), metav1.ListOptions{ResourceVersion: r.relistResourceVersion()})
			}
			close(listCh)
		}()
		select {
		case &lt;-stopCh:
			return nil
		case r := &lt;-panicCh:
			panic(r)
		case &lt;-listCh:
		}
		if err != nil {
			return fmt.Errorf(&quot;failed to list %v: %v&quot;, r.expectedTypeName, err)
		}

		// We check if the list was paginated and if so set the paginatedResult based on that.
		// However, we want to do that only for the initial list (which is the only case
		// when we set ResourceVersion=&quot;0&quot;). The reasoning behind it is that later, in some
		// situations we may force listing directly from etcd (by setting ResourceVersion=&quot;&quot;)
		// which will return paginated result, even if watch cache is enabled. However, in
		// that case, we still want to prefer sending requests to watch cache if possible.
		//
		// Paginated result returned for request with ResourceVersion=&quot;0&quot; mean that watch
		// cache is disabled and there are a lot of objects of a given type. In such case,
		// there is no need to prefer listing from watch cache.
		if options.ResourceVersion == &quot;0&quot; &amp;&amp; paginatedResult {
			r.paginatedResult = true
		}

		r.setIsLastSyncResourceVersionUnavailable(false) // list was successful
		initTrace.Step(&quot;Objects listed&quot;)
		listMetaInterface, err := meta.ListAccessor(list)
		if err != nil {
			return fmt.Errorf(&quot;unable to understand list result %#v: %v&quot;, list, err)
		}
    // 获取 ResourceVersion，ResourceVersion 非常重要，Kubernetes 中所有的资源都拥有该字段
    // 它标识当前资源对象的版本号。每次修改资源对象时，apiserver 都会更改 ResourceVersion
    // 使得 client-go  执行 watch 时可以根据 ResourceVersion 来确定当前对象资源是否发生变化
		resourceVersion = listMetaInterface.GetResourceVersion()
		initTrace.Step(&quot;Resource version extracted&quot;)
    // 将获取到的资源对象转为列表
		items, err := meta.ExtractList(list)
		if err != nil {
			return fmt.Errorf(&quot;unable to understand list result %#v (%v)&quot;, list, err)
		}
		initTrace.Step(&quot;Objects extracted&quot;)
    // 将资源对象存储至 DeltaFIFO，并会替换已存在的对象
    // 实现是调用传入的 DeltaFIFO 的 Replace 方法
		if err := r.syncWith(items, resourceVersion); err != nil {
			return fmt.Errorf(&quot;unable to sync list result: %v&quot;, err)
		}
		initTrace.Step(&quot;SyncWith done&quot;)
		r.setLastSyncResourceVersion(resourceVersion)
		initTrace.Step(&quot;Resource version updated&quot;)
		return nil
	}(); err != nil {
		return err
	}

  // 额外部分，resync 机制，如果实例化 ShareIndexInformer 时指定了 resyncPeriod
  // 此处就会启动一个 gorutine 来定期强制同步资源，也会同步给 DeltaFIFO
	resyncerrc := make(chan error, 1)
	cancelCh := make(chan struct{})
	defer close(cancelCh)
	go func() {
		resyncCh, cleanup := r.resyncChan()
		defer func() {
			cleanup() // Call the last one written into cleanup
		}()
		for {
			select {
			case &lt;-resyncCh:
			case &lt;-stopCh:
				return
			case &lt;-cancelCh:
				return
			}
			if r.ShouldResync == nil || r.ShouldResync() {
				klog.V(4).Infof(&quot;%s: forcing resync&quot;, r.name)
				if err := r.store.Resync(); err != nil {
					resyncerrc &lt;- err
					return
				}
			}
			cleanup()
			resyncCh, cleanup = r.resyncChan()
		}
	}()

  // 第二部分：监控资源对象
	for {
		// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors
		select {
		case &lt;-stopCh:
			return nil
		default:
		}

		timeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))
		options = metav1.ListOptions{
			ResourceVersion: resourceVersion,
      // typo issue: wachers =&gt; watchers
			// We want to avoid situations of hanging watchers. Stop any wachers that do not
			// receive any events within the timeout window.
			TimeoutSeconds: &amp;timeoutSeconds,
			// To reduce load on kube-apiserver on watch restarts, you may enable watch bookmarks.
			// Reflector doesn't assume bookmarks are returned at all (if the server do not support
			// watch bookmarks, it will ignore this field).
			AllowWatchBookmarks: true,
		}

		// start the clock before sending the request, since some proxies won't flush headers until after the first watch event is sent
		start := r.clock.Now()
    // Watch 实际上调用了对应资源 Client 的 Watch 函数，通过 HTTP 协议与 kube-apiserver 建立长连接
    // Watch 的实现机制是使用 HTTP 的分块传输协议（Chunked Transfer Encoding）
    // Client watch 方法会返回 watcher 的接口实现，交给下文，通过通道读取数据
    // 此处如果是 apiserver 未响应的错误，则会重试
		w, err := r.listerWatcher.Watch(options)
		if err != nil {
			// If this is &quot;connection refused&quot; error, it means that most likely apiserver is not responsive.
			// It doesn't make sense to re-list all objects because most likely we will be able to restart
			// watch where we ended.
			// If that's the case begin exponentially backing off and resend watch request.
			if utilnet.IsConnectionRefused(err) {
				&lt;-r.initConnBackoffManager.Backoff().C()
				continue
			}
			return err
		}
    // watchHandler 负责处理资源的变更事件。当触发 Added、Updated、Deleted 事件时，将对应的资源对象
    // 更新到本地缓存 DeltaFIFO 中并更新 ResourceVersion 资源版本号
		if err := r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh); err != nil {
			if err != errorStopRequested {
				switch {
				case isExpiredError(err):
					// Don't set LastSyncResourceVersionUnavailable - LIST call with ResourceVersion=RV already
					// has a semantic that it returns data at least as fresh as provided RV.
					// So first try to LIST with setting RV to resource version of last observed object.
					klog.V(4).Infof(&quot;%s: watch of %v closed with: %v&quot;, r.name, r.expectedTypeName, err)
				default:
					klog.Warningf(&quot;%s: watch of %v ended with: %v&quot;, r.name, r.expectedTypeName, err)
				}
			}
			return nil
		}
	}
}

// 通过 watcher.ResultChan 得到分段传输的数据，根据资源对象类型 DeltaFIFO 执行相应动作。
func (r *Reflector) watchHandler(start time.Time, w watch.Interface, resourceVersion *string, errc chan error, stopCh &lt;-chan struct{}) error {
	eventCount := 0

	// Stopping the watcher should be idempotent and if we return from this function there's no way
	// we're coming back in with the same watch interface.
	defer w.Stop()

loop:
	for {
		select {
		case &lt;-stopCh:
			return errorStopRequested
		case err := &lt;-errc:
			return err
		case event, ok := &lt;-w.ResultChan():
			if !ok {
				break loop
			}
      ......
			newResourceVersion := meta.GetResourceVersion()
			switch event.Type {
			case watch.Added:
        // DeltaFIFO::Add
				err := r.store.Add(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&quot;%s: unable to add watch event object (%#v) to store: %v&quot;, r.name, event.Object, err))
				}
			case watch.Modified:
        // DeltaFIFO::Update
				err := r.store.Update(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&quot;%s: unable to update watch event object (%#v) to store: %v&quot;, r.name, event.Object, err))
				}
			case watch.Deleted:
				// TODO: Will any consumers need access to the &quot;last known
				// state&quot;, which is passed in event.Object? If so, may need
				// to change this.
        // 
        // DeltaFIFO::Delete
				err := r.store.Delete(event.Object)
				if err != nil {
					utilruntime.HandleError(fmt.Errorf(&quot;%s: unable to delete watch event object (%#v) from store: %v&quot;, r.name, event.Object, err))
				}
			case watch.Bookmark:
				// A `Bookmark` means watch has synced here, just update the resourceVersion
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;%s: unable to understand watch event %#v&quot;, r.name, event))
			}
      ......
		}
	}
  ......
	return nil
}
</code></pre>
<h4 id="deltafifo">DeltaFIFO</h4>
<p>接下来看看 DeltaFIFO 的详细操作都有哪些，上面源码中的 Resync、Add、Update 等操作都做了什么。</p>
<p>现在将代码回到 sharedIndexInformer::Run 方法中:</p>
<pre><code class="language-go">func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct{}) {
	......
	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
    // Indexer 是由对应资源 Informer 实例化 SharedIndexInformer 时传入，这个下面会详细再看
		KnownObjects:          s.indexer,
		EmitDeltaTypeReplaced: true,
	})
  ......
}

// DeltaFIFO 是一个生产者-消费者队列，其中 Reflector 是生产者，消费者是调用 Pop 方法的任何人
// 通过 DeltaFIFO 可以一次处理一个资源对象的所有操作，这主要取决与 DeltaFIFO 的存储结构
// 它通过 queue 字段存储资源对象的 key，该 key 通过 KeyOf 函数计算得到。items 字段使用 map 数据结构
// 的方式存储，key 与 queue 对应，value 存储的是对象的 Deltas 数组
type DeltaFIFO struct {
  ......
	items map[string]Deltas
	queue []string
  ......
}

// Add、Update、Delete 都是生产者方法，产生的都是增量更新，都会调用 queueActionLocked 方法
// 只是传入的 DeltaType 不同
func (f *DeltaFIFO) Add(obj interface{}) error {
	f.lock.Lock()
	defer f.lock.Unlock()
	f.populated = true
	return f.queueActionLocked(Added, obj)
}

func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
  // 取得 key
	id, err := f.KeyOf(obj)
	if err != nil {
		return KeyError{obj, err}
	}
	oldDeltas := f.items[id]
	newDeltas := append(oldDeltas, Delta{actionType, obj})
  // 不只 watch，resync 机制也会改变 items 中的值，所以新的事件进来要进行去重
	newDeltas = dedupDeltas(newDeltas)

	if len(newDeltas) &gt; 0 {
    // 更新 queue 字段
		if _, exists := f.items[id]; !exists {
			f.queue = append(f.queue, id)
		}
    // 更新 items
		f.items[id] = newDeltas
    // 唤醒被阻塞的 goroutine
		f.cond.Broadcast()
	} else {
		// This never happens, because dedupDeltas never returns an empty list
		// when given a non-empty list (as it is here).
		// If somehow it happens anyway, deal with it but complain.
		if oldDeltas == nil {
			klog.Errorf(&quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; ignoring&quot;, id, oldDeltas, obj)
			return nil
		}
		klog.Errorf(&quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; breaking invariant by storing empty Deltas&quot;, id, oldDeltas, obj)
		f.items[id] = newDeltas
		return fmt.Errorf(&quot;Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; broke DeltaFIFO invariant by storing empty Deltas&quot;, id, oldDeltas, obj)
	}
	return nil
}

// 消费者方法
func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) {
	f.lock.Lock()
	defer f.lock.Unlock()
	for {
    // 如果队列为空则阻塞
		for len(f.queue) == 0 {
			// When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
			// When Close() is called, the f.closed is set and the condition is broadcasted.
			// Which causes this loop to continue and return from the Pop().
			if f.closed {
				return nil, ErrFIFOClosed
			}
			// 阻塞，可被 f.cond.Broadcast 唤醒
			f.cond.Wait()
		}
    // 取出头部资源对象 key
		id := f.queue[0]
    // 已加锁，可以直接更新队列
		f.queue = f.queue[1:]
		if f.initialPopulationCount &gt; 0 {
			f.initialPopulationCount--
		}
    // 根据 key 取出 deltas
		item, ok := f.items[id]
		if !ok {
			// This should never happen
			klog.Errorf(&quot;Inconceivable! %q was in f.queue but not f.items; ignoring.&quot;, id)
			continue
		}
		delete(f.items, id)
    // process 是传入的回调方法，由上层消费者（controller）传入
    // 这正是第三个组件，sharedProcessor，DeltaFIFO 会以此通知 listeners
		err := process(item)
    // 如果回调函数出错，就将资源重新存入队列
		if e, ok := err.(ErrRequeue); ok {
			f.addIfNotPresent(id, item)
			err = e.Err
		}
		// Don't need to copyDeltas here, because we're transferring
		// ownership to the caller.
		return item, err
	}
}
</code></pre>
<p><strong>DeltaFIFO Resync</strong></p>
<p>Resync 与 kubebuilder  的 retry 是一样的功能吗？</p>
<p>Deleted object 如何给出完整资源呢？</p>
<p>上文在 <code>Reflector::ListAndWatch</code> 中可以看到启动了一个 goroutine 用于定时同步，调用的就是 <code>DeltaFIFO::Resync</code> 方法</p>
<p>Resync 的作用是将 indexer 中的资源对象同步至 DeltaFIFO 中，以便于让处理失败的事件再次处理</p>
<pre><code class="language-go">// Resync 实际上是添加了一个 Sync 类型的 delta
func (f *DeltaFIFO) Resync() error {
	f.lock.Lock()
	defer f.lock.Unlock()

  // knownObject 接口用于列出所有已知资源对象，实际传入的就是 indexer
  // 在 sharedIndexInformer::Run 方法中实例化 DeltaFIFO 时传入
	if f.knownObjects == nil {
		return nil
	}

	keys := f.knownObjects.ListKeys()
	for _, k := range keys {
		if err := f.syncKeyLocked(k); err != nil {
			return err
		}
	}
	return nil
}

// Resync 的作用是将 indexer 中的资源对象同步至 DeltaFIFO 中，并将同步过去的资源对象设为 Sync 类型
func (f *DeltaFIFO) syncKeyLocked(key string) error {
	obj, exists, err := f.knownObjects.GetByKey(key)
  ......

	// If we are doing Resync() and there is already an event queued for that object,
	// we ignore the Resync for it. This is to avoid the race, in which the resync
	// comes with the previous value of object (since queueing an event for the object
	// doesn't trigger changing the underlying store &lt;knownObjects&gt;.
	id, err := f.KeyOf(obj)
	if err != nil {
		return KeyError{obj, err}
	}
	if len(f.items[id]) &gt; 0 {
		return nil
	}

	if err := f.queueActionLocked(Sync, obj); err != nil {
		return fmt.Errorf(&quot;couldn't queue object: %v&quot;, err)
	}
	return nil
}
</code></pre>
<p><strong>Replace</strong></p>
<p>在 <code>sharedIndexInformer::ListAndWatch</code> 中，List 部分会在通过 <code>DeltaFIFO::Replace</code> 方法替换 DeltaFIFO 中的资源，此方法用于首次 List 的数据处理或连接中断后的数据同步</p>
<pre><code class="language-go">func (f *DeltaFIFO) Replace(list []interface{}, resourceVersion string) error {
  ......
	for _, item := range list {
		key, err := f.KeyOf(item)
		if err != nil {
			return KeyError{item, err}
		}
		keys.Insert(key)
		if err := f.queueActionLocked(action, item); err != nil {
			return fmt.Errorf(&quot;couldn't enqueue object: %v&quot;, err)
		}
	}
	......
	// Detect deletions not already in the queue.
	knownKeys := f.knownObjects.ListKeys()
	queuedDeletions := 0
	for _, k := range knownKeys {
		if keys.Has(k) {
			continue
		}
    ......
    // 根据 Indexer 检测已删除的资源
		if err := f.queueActionLocked(Deleted, DeletedFinalStateUnknown{k, deletedObj}); err != nil {
			return err
		}
	}
  ......
}
</code></pre>
<h4 id="indexer">Indexer</h4>
<p>Indexer 上面也介绍了，是负责 local cache 的组件，它用来存储资源对象并自带索引功能。Indexer 中的数据会与 Etcd 集群中的数据保持一致，这主要通过 Reflector 实现。</p>
<p>在实例化 <code>sharedIndexInformer</code> 时需要在参数中传入参数，跟着一起实例化，下面是 deployment Informer 的代码：</p>
<pre><code class="language-go">func (f *deploymentInformer) defaultInformer(client kubernetes.Interface, resyncPeriod time.Duration) cache.SharedIndexInformer {
	return NewFilteredDeploymentInformer(client, f.namespace, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}, f.tweakListOptions)
}

func NewSharedIndexInformer(lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer {
	realClock := &amp;clock.RealClock{}
	sharedIndexInformer := &amp;sharedIndexInformer{
    ......
		indexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),
    ......
	}
	return sharedIndexInformer
}

func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {
  // cache 使用 cacheStorage 进行存储，自身在其基础上封装了用于索引的方法，便于使用
	return &amp;cache{
    // cacheStorage 使用的 ThreadSafeMap 是线程安全的存储
		cacheStorage: NewThreadSafeStore(indexers, Indices{}),
    // 用于获取 key 的闭包
		keyFunc:      keyFunc,
	}
}

func NewThreadSafeStore(indexers Indexers, indices Indices) ThreadSafeStore {
	return &amp;threadSafeMap{
    // 存储
		items:    map[string]interface{}{},
    // 索引器，map 类型，key 是索引器的名称，value 是对应索引器函数，由对应资源的 informer 传入
    // 索引器函数被定义为接收一个资源对象，返回检索结果列表
		indexers: indexers,
    // 索引存储器，map 类型，将名称与 index 对应
    // index 被定义为存储的缓存数据，通过 set 结构存储，go 语言没有 set 结构，所以是通过 map 实现 set 的去重
		indices:  indices,
	}
}

func (c *threadSafeMap) Add(key string, obj interface{}) {
  // 通过锁保证数据一致性
	c.lock.Lock()
	defer c.lock.Unlock()
	oldObject := c.items[key]
	c.items[key] = obj
  // 更新索引
	c.updateIndices(oldObject, obj, key)
}
</code></pre>
<p>想要理解 indexer 还是需要通过 example：</p>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glqkusf9nvj30su10odib.jpg" alt="exalpme" loading="lazy"></figure>
<h4 id="processor">Processor</h4>
<p>最后一个组件 sharedProcessor，processor 作为回调函数在 DeltaFIFO 的中被调用，现在将代码回到 <code>Controller::Run</code> 中：</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
  ......
	var wg wait.Group
  // 上面讲了 Reflector:Run:
	wg.StartWithChannel(stopCh, r.Run)
  // 消费 DeltaFIFO 数据
	wait.Until(c.processLoop, time.Second, stopCh)
	wg.Wait()
}

func (c *controller) processLoop() {
	for {
    // 传入确保类型正确的回调函数，c.config.Process 由 sharedIndexInformer 传入
		obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process))
		if err != nil {
			if err == ErrFIFOClosed {
				return
			}
      // DeltaFIFO::Pop 只会对 ErrRequeue 类型错误进行重试，此处会处理所有类型错误
			if c.config.RetryOnError {
				// 这是个安全的方法，如果队列中已存在，则不会重复添加
				c.config.Queue.AddIfNotPresent(obj)
			}
		}
	}
}

// 寻找 config.Process
func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct{}) {
  ......
	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    s.listerWatcher,
		ObjectType:       s.objectType,
		FullResyncPeriod: s.resyncCheckPeriod,
		RetryOnError:     false,
		ShouldResync:     s.processor.shouldResync,
    // here
		Process:           s.HandleDeltas,
		WatchErrorHandler: s.watchErrorHandler,
	}
  ......
}

// Informer DeltaFIFO 回调函数
func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error {
	s.blockDeltas.Lock()
	defer s.blockDeltas.Unlock()

	// from oldest to newest
	for _, d := range obj.(Deltas) {
		switch d.Type {
    // 更新 local cache
		case Sync, Replaced, Added, Updated:
			s.cacheMutationDetector.AddObject(d.Object)
			if old, exists, err := s.indexer.Get(d.Object); err == nil &amp;&amp; exists {
				if err := s.indexer.Update(d.Object); err != nil {
					return err
				}

				isSync := false
				switch {
				case d.Type == Sync:
					// Sync events are only propagated to listeners that requested resync
					isSync = true
				case d.Type == Replaced:
					if accessor, err := meta.Accessor(d.Object); err == nil {
						if oldAccessor, err := meta.Accessor(old); err == nil {
							// Replaced events that didn't change resourceVersion are treated as resync events
							// and only propagated to listeners that requested resync
							isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()
						}
					}
				}
        // 发送给 sharedProcessor 组件
				s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
			} else {
				if err := s.indexer.Add(d.Object); err != nil {
					return err
				}
				s.processor.distribute(addNotification{newObj: d.Object}, false)
			}
    // 从 local cache 中删除对象资源
		case Deleted:
			if err := s.indexer.Delete(d.Object); err != nil {
				return err
			}
			s.processor.distribute(deleteNotification{oldObj: d.Object}, false)
		}
	}
	return nil
}
</code></pre>
<p><strong>sharedProcessor</strong></p>
<pre><code class="language-go">// sharedProcessor 在实例化 SharedIndexInformer 时一起实例化
func NewSharedIndexInformer(lw ListerWatcher, exampleObject runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer {
	realClock := &amp;clock.RealClock{}
	sharedIndexInformer := &amp;sharedIndexInformer{
		processor:                       &amp;sharedProcessor{clock: realClock},
		indexer:                         NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers),
		listerWatcher:                   lw,
		objectType:                      exampleObject,
		resyncCheckPeriod:               defaultEventHandlerResyncPeriod,
		defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod,
		cacheMutationDetector:           NewCacheMutationDetector(fmt.Sprintf(&quot;%T&quot;, exampleObject)),
		clock:                           realClock,
	}
	return sharedIndexInformer
}

// 在调用 SharedIndexInformer::AddEventHandler 的方法时，会为 sharedProcessor 添加 listener
func NewController(
	kubeclientset kubernetes.Interface,
	sampleclientset clientset.Interface,
	deploymentInformer appsinformers.DeploymentInformer,
	fooInformer informers.FooInformer) *Controller {
  ......
  deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc: controller.handleObject,
    UpdateFunc: func(old, new interface{}) {
      newDepl := new.(*appsv1.Deployment)
      oldDepl := old.(*appsv1.Deployment)
      if newDepl.ResourceVersion == oldDepl.ResourceVersion {
        // Periodic resync will send update events for all known Deployments.
        // Two different versions of the same Deployment will always have different RVs.
        return
      }
      controller.handleObject(new)
    },
    DeleteFunc: controller.handleObject,
  })
  ......
}

func (s *sharedIndexInformer) AddEventHandler(handler ResourceEventHandler) {
	s.AddEventHandlerWithResyncPeriod(handler, s.defaultEventHandlerResyncPeriod)
}

// 为 processor 添加 listener
func (s *sharedIndexInformer) AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) {
	......
	listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize)
	.......
	s.processor.addListener(listener)
	for _, item := range s.indexer.List() {
		listener.add(addNotification{newObj: item})
	}
}

// listener 添加后会直接启动
func (p *sharedProcessor) addListener(listener *processorListener) {
	p.listenersLock.Lock()
	defer p.listenersLock.Unlock()

	p.addListenerLocked(listener)
	if p.listenersStarted {
		p.wg.Start(listener.run)
		p.wg.Start(listener.pop)
	}
}

// processorListener::run 方法会等待 pop 通知，并根据通知的类型，调用相应回调函数
func (p *processorListener) run() {
	stopCh := make(chan struct{})
	wait.Until(func() {
		for next := range p.nextCh {
			switch notification := next.(type) {
			case updateNotification:
				p.handler.OnUpdate(notification.oldObj, notification.newObj)
			case addNotification:
				p.handler.OnAdd(notification.newObj)
			case deleteNotification:
				p.handler.OnDelete(notification.oldObj)
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;unrecognized notification: %T&quot;, next))
			}
		}
		// the only way to get here is if the p.nextCh is empty and closed
		close(stopCh)
	}, 1*time.Second, stopCh)
}

// processorListener::pop 会接收通知，并发送给 run 等待的 channel 中
func (p *processorListener) pop() {
	defer utilruntime.HandleCrash()
	defer close(p.nextCh) // Tell .run() to stop

	var nextCh chan&lt;- interface{}
	var notification interface{}
	for {
		select {
		case nextCh &lt;- notification:
			// Notification dispatched
			var ok bool
			notification, ok = p.pendingNotifications.ReadOne()
			if !ok { // Nothing to pop
				nextCh = nil // Disable this select case
			}
		case notificationToAdd, ok := &lt;-p.addCh:
			if !ok {
				return
			}
			if notification == nil { // No notification to pop (and pendingNotifications is empty)
				// Optimize the case - skip adding to pendingNotifications
				notification = notificationToAdd
				nextCh = p.nextCh
			} else { // There is already a notification waiting to be dispatched
				p.pendingNotifications.WriteOne(notificationToAdd)
			}
		}
	}
}
</code></pre>
<h2 id="workqueue">WorkQueue</h2>
<p>WorkQueue 称为工作队列，Kubernetes 的 WorkQueue 队列与普通的 FIFO 队列相比，实现略显复杂，它的主要功能在于标记和去重，并支持如下特性：</p>
<ul>
<li><strong>有序</strong>：按照添加顺序处理元素</li>
<li><strong>去重</strong>：相同元素在同一时间不会被重复处理，例如一个元素在处理之前被添加了多次，它只会被处理一次</li>
<li><strong>并发性</strong>：多生产者和多消费者</li>
<li><strong>标记机制</strong>：支持标记功能，标记一个元素是否被处理，也允许元素在处理时重新排队</li>
<li><strong>通知机制</strong>：ShutDown 方法通过信号量通知队列不再接收新的元素，并通知 metric goroutine 退出</li>
<li><strong>延迟</strong>：支持延迟队列，延迟一段时间后再将元素存入队列</li>
<li><strong>限速</strong>：支持限速队列，元素存入队列时进行速率限制，限制一个元素被重新排队的次数</li>
<li><strong>Metric</strong>：支持 metric 监控指标</li>
<li><strong>Interface</strong>：FIFO 队列接口，先进先出，并支持去重机制</li>
<li><strong>DelayingInterface</strong>：延迟队列接口，基于 Interface 接口封装，延迟一段时间后再将元素存入队列</li>
<li><strong>RateLimitingInterface</strong>：限速队列接口，基于 DelayingInterface 接口封装，支持元素存入队列时进行速率限制</li>
</ul>
<h3 id="源码-2">源码</h3>
<p>SampleController 在实例化 Controller 时实例化了 <code>NamedRateLimitingQueue</code>，也就是命名的限速队列。</p>
<pre><code class="language-go">func NewController(
	kubeclientset kubernetes.Interface,
	sampleclientset clientset.Interface,
	deploymentInformer appsinformers.DeploymentInformer,
	fooInformer informers.FooInformer) *Controller {
  ......
	controller := &amp;Controller{
    ......
    // DefaultControllerRateLimiter 是默认的、基于令牌桶机制的速率限制器
		workqueue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;Foos&quot;),
    ......
	}
  fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    // 在回调函数中将 delta 加入 workQueue
		AddFunc: controller.enqueueFoo,
		UpdateFunc: func(old, new interface{}) {
			controller.enqueueFoo(new)
		},
	})
  ......
}

func NewNamedRateLimitingQueue(rateLimiter RateLimiter, name string) RateLimitingInterface {
	return &amp;rateLimitingType{
    // 同时实例化延迟队列
		DelayingInterface: NewNamedDelayingQueue(name),
		rateLimiter:       rateLimiter,
	}
}

func NewNamedDelayingQueue(name string) DelayingInterface {
	return NewDelayingQueueWithCustomClock(clock.RealClock{}, name)
}

func NewDelayingQueueWithCustomClock(clock clock.Clock, name string) DelayingInterface {
  // NewNamed 会实例化一个基础的 FIFOWorkQueue
	return newDelayingQueue(clock, NewNamed(name), name)
}

func newDelayingQueue(clock clock.Clock, q Interface, name string) *delayingType {
	ret := &amp;delayingType{
    // 基础队列
		Interface:       q,
		clock:           clock,
    // 心跳确保等待时间不超过 maxWait，const maxWait = 10 * time.Second
		heartbeat:       clock.NewTicker(maxWait),
		stopCh:          make(chan struct{}),
		waitingForAddCh: make(chan *waitFor, 1000),
		metrics:         newRetryMetrics(name),
	}
  // 处理延迟元素
	go ret.waitingLoop()
	return ret
}

// 以上是实例化的过程，下面看添加
func (c *Controller) enqueueFoo(obj interface{}) {
	var key string
	var err error
  // obj 为传入的资源，在此处转为 key 存入 WorkQueue
  // 为什么转为 key，而不是将 object 整个存入 workQueue？
  // 暂时的思考： object 不利于去重，而且数据过大，还有如果存入 object
  // Reconciler 还是需要 Get 来判断资源是否已删除
	if key, err = cache.MetaNamespaceKeyFunc(obj); err != nil {
		utilruntime.HandleError(err)
		return
	}
  // RateLimiting 和 Delaying 都是基于基础的 workQueue 封装
  // 所以此处调用的是 Interface::Add()
	c.workqueue.Add(key)
}

// 现在回来看看 NewNamed 实例化的是什么
func NewNamed(name string) *Type {
	rc := clock.RealClock{}
	return newQueue(
		rc,
		globalMetricsFactory.newQueueMetrics(name, rc),
		defaultUnfinishedWorkUpdatePeriod,
	)
}
func newQueue(c clock.Clock, metrics queueMetrics, updatePeriod time.Duration) *Type {
	t := &amp;Type{
		clock:                      c,
		dirty:                      set{},
		processing:                 set{},
		cond:                       sync.NewCond(&amp;sync.Mutex{}),
		metrics:                    metrics,
		unfinishedWorkUpdatePeriod: updatePeriod,
	}
	go t.updateUnfinishedWorkLoop()
	return t
}

func (q *Type) Add(item interface{}) {
	q.cond.L.Lock()
	defer q.cond.L.Unlock()
	if q.shuttingDown {
		return
	}
  // 去重，这里也不利于 object 整个传入
	if q.dirty.has(item) {
		return
	}

	q.metrics.add(item)

	q.dirty.insert(item)
  // 集中处理正在处理的元素
	if q.processing.has(item) {
		return
	}
  
  // 将需要处理的元素加入到队列中，队列中应该只包含已处理完成的元素，不应该有 processing 的元素
	q.queue = append(q.queue, item)
	q.cond.Signal()
}

// 现在来看下如何消费 workQueue 中的内容
// 在 Controller::Run() 中，会根据给定线程数启动 x 个 Controller::runWorkker()
func (c *Controller) Run(threadiness int, stopCh &lt;-chan struct{}) error {
	......
	for i := 0; i &lt; threadiness; i++ {
		go wait.Until(c.runWorker, time.Second, stopCh)
	}
  ......
}

// 这是一个持久运行的方法，它会不断的消费 workQueue 中的元素
func (c *Controller) runWorker() {
	for c.processNextWorkItem() {
	}
}

// 这里的处理过于简单，可以看看 kubebuilder 的源码了
func (c *Controller) processNextWorkItem() bool {
  // 取出元素
	obj, shutdown := c.workqueue.Get()
  ......
	err := func(obj interface{}) error {
		......
    // Controller::syncHandler 等同于 Reconciler
		if err := c.syncHandler(key); err != nil {
			// 处理失败则放回等待再次处理
			c.workqueue.AddRateLimited(key)
			return fmt.Errorf(&quot;error syncing '%s': %s, requeuing&quot;, key, err.Error())
		}
		// 如果处理成功则删除元素
		c.workqueue.Forget(obj)
    ......
	}(obj)
  ......
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[我们为什么要使用 Kubernetes 自定义资源]]></title>
        <id>https://cnbailian.github.io/post/why-do-use-kubernetes-crd/</id>
        <link href="https://cnbailian.github.io/post/why-do-use-kubernetes-crd/">
        </link>
        <updated>2021-05-08T03:09:27.000Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes 提供了 CRD(CustomResourceDefinitions) 这种扩展方式满足了用户增强 Kubernetes 功能的需求，我们熟悉的 Kubernetes  Operator 也是基于这一机制而实现。</p>
<p>本文想讨论的是我们要在什么时候使用 CRD 以及为什么要使用 CRD。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Kubernetes 提供了 CRD(CustomResourceDefinitions) 这种扩展方式满足了用户增强 Kubernetes 功能的需求，我们熟悉的 Kubernetes  Operator 也是基于这一机制而实现。</p>
<p>本文想讨论的是我们要在什么时候使用 CRD 以及为什么要使用 CRD。</p>
<!--more-->
<h2 id="我是否应该向我的-kubernetes-集群添加定制资源">我是否应该向我的 Kubernetes 集群添加定制资源？</h2>
<p>表格是 <a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#%E6%88%91%E6%98%AF%E5%90%A6%E5%BA%94%E8%AF%A5%E5%90%91%E6%88%91%E7%9A%84-kubernetes-%E9%9B%86%E7%BE%A4%E6%B7%BB%E5%8A%A0%E5%AE%9A%E5%88%B6%E8%B5%84%E6%BA%90">Kubernetes 官网</a>列出的选择 CRD 的场景，其中最重要的，也是难以理解的应该是<strong>声明式 API</strong>这一概念。</p>
<table>
<thead>
<tr>
<th>考虑 API 聚合的情况</th>
<th>优选独立 API 的情况</th>
</tr>
</thead>
<tbody>
<tr>
<td>你的 API 是<a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis">声明式的</a>。</td>
<td>你的 API 不符合<a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis">声明式</a>模型。</td>
</tr>
<tr>
<td>你希望可以是使用 <code>kubectl</code> 来读写你的新资源类别。</td>
<td>不要求 <code>kubectl</code> 支持。</td>
</tr>
<tr>
<td>你希望在 Kubernetes UI （如仪表板）中和其他内置类别一起查看你的新资源类别。</td>
<td>不需要 Kubernetes UI 支持。</td>
</tr>
<tr>
<td>你在开发新的 API。</td>
<td>你已经有一个提供 API 服务的程序并且工作良好。</td>
</tr>
<tr>
<td>你有意愿取接受 Kubernetes 对 REST 资源路径所作的格式限制，例如 API 组和名字空间。（参阅 <a href="https://kubernetes.io/zh/docs/concepts/overview/kubernetes-api/">API 概述</a>）</td>
<td>你需要使用一些特殊的 REST 路径以便与已经定义的 REST API 保持兼容。</td>
</tr>
<tr>
<td>你的资源可以自然地界定为集群作用域或集群中某个名字空间作用域。</td>
<td>集群作用域或名字空间作用域这种二分法很不合适；你需要对资源路径的细节进行控制。</td>
</tr>
<tr>
<td>你希望复用 <a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#common-features">Kubernetes API 支持特性</a>。</td>
<td>你不需要这类特性。</td>
</tr>
</tbody>
</table>
<h2 id="声明式">声明式</h2>
<p>声明式指的是这么一种软件设计理念和做法：<strong>让我们的动作更偏向于描述，而不是命令</strong>。</p>
<p>声明式（Declarative）通常是与命令式（Imperative）作对比，两者的侧重点不同。命令式编程会详细的命令工具怎么（How）去处理一件事情以达到你想要的结果（What）；声明式编程则是只告诉工具想要的结果（What），由工具自行决定怎么做（How）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gq8n3f5etmj30f008374t.jpg" alt="img"  />
<p>以生活中打车作为例子，我们在大多数时候并不会指挥司机师傅：走哪条街，前行多少米，在哪个路口转向；而是直接告诉师傅，我要去 XXX 地点。上述例子能看出命令式与声明式在生活中的体现，在编程中，我们大多数人首先接触到的都是命令式的编程语言，这就导致我们对声明式会有一些不理解。下面就用声明式在编程领域中的两个比较重要的成果来说明声明式的意义。</p>
<h3 id="dsl">DSL</h3>
<p>DSL 是 Domain Specific Language 的缩写，中文翻译为<strong>领域特定语言</strong>。与 DSL 相对的是 GPPL（General Purpose Programming Language，通用目的编程语言），也就是我们非常熟悉的 Java、C、Go 等编程语言。</p>
<p>DSL 的定义并不是很明确，我们可以简单的理解为“为了解决某一类任务而专门设计的计算机语言”。最常见的 DSL 包括 SQL、HTML 和 CSS 等。</p>
<p>所有的 DSL 都是声明式的，你写出一条 SQL 语句，只是告诉数据库想要的结果是什么，数据库会帮我们设计获取这个结果集的执行路径，并返回结果集。众所周知，使用 SQL 语言获取数据，要比自行编写处理过程去获取数据容易的多。</p>
<pre><code class="language-sql">SELECT * from user WHERE user_name = Ben
</code></pre>
<p>Go 伪代码：</p>
<pre><code class="language-go">users := get_users()
for row, value := range users {
  if value.user_name = &quot;Ben&quot; {
    print(&quot;find&quot;)
    break
  }
}
</code></pre>
<h4 id="内部-dsl">内部 DSL</h4>
<p>上面提到的 SQL、HTML 和 CSS 等，属于外部 DSL。外部 DSL 是自我包含的语言，他有自己特定语法、解析器和词法分析器等等。与之相对的是内部 DSL，它使用的是宿主语言的抽象能力，更像是一种别称，代表着一类特别的 API 及使用模式。</p>
<p>比如说 LINQ（C#）、 Ruby on Rails（Ruby）、jQuery（JavaScript）。它们共同的特点是，它们其实只是一系列 API，但是你可以“假装”它们是一种 DSL。不过，这种 DSL 模糊了框架和 DSL 的边界，因为两者看起来没有什么区别，我们也没有必要争论哪些是框架，哪些是 DSL，因为这些争论并没有什么意义。</p>
<p><em>就我个人体验而言，如果脱离框架转而使用宿主语言实现同样功能时会感觉到不适应，那么可能就证明了这个框架拥有内部 DSL 的性质。</em></p>
<h3 id="函数式编程">函数式编程</h3>
<p>函数式编程就是声明式的另一个重要成果，它的编程形式更倾向于描述而不是执行命令，下面这个例子是 React 的声明式构建 UI：</p>
<pre><code class="language-javascript">// 普通的 DOM API 构建 UI
const div = document.createElement('div')
const p = document.createElement('p')
p.textContent = 'hello world'
const UI = div.append(p)

// React 构建 UI
const h = React.craeteElement
const UI = h('div', null, h('p', null, 'hello world'))
</code></pre>
<p>React 依托于 JavaScript，并不是完全的函数式编程语言，不过 Haskell 等函数式语言我也没有接触，所以并不能很好的理解。分享两篇文章，希望能一起学习。</p>
<p><a href="http://blog.zhaojie.me/2010/05/trends-and-future-directions-in-programming-languages-by-anders-3-functional-programming-and-fsharp.html">编程语言的发展趋势及未来方向（3）：函数式编程</a></p>
<p><a href="https://lutaonan.com/blog/declarative-programming-is-the-future/">未来属于声明式编程</a></p>
<h2 id="kubernetes-声明式-api">Kubernetes 声明式 API</h2>
<p>通过上述的例子，我们已经明白声明式的理念。Kubernetes 的声明式 API 正是使用了这种方法，<strong>我们向其描述我们想要让一个事物达到的期望状态，由 Kubernetes 内部去自行实现，令这个事物达成实际状态</strong>。</p>
<p>声明式 API 基于 RESTful 的设计风格，将想要描述的事物抽象为资源，通过 CRUD 风格的操作方法修改资源对象的状态。这也正是 REST 的本质：<strong>资源表述性状态转移</strong>，通俗的讲就是：资源以某种表现形式进行状态转移。在 Kubernetes 中，自定义资源的表现形式是由 CRD 来定义。</p>
<p><em>表现形式包含表示的格式，也包含表示的属性。格式在 Kubernetes 中有着统一的定义，所以我们在 CRD 中主要配置的是表示的属性，也就是对象的配置信息，我们想要对象达成的期望状态的相关属性。</em></p>
<h3 id="关于-replace-和-apply">关于 replace 和 apply</h3>
<p>通过上面对声明式和声明式 API 的理解，我们也就能更好的理解极客时间中张磊老师课程里所说的 replace 和 apply 的区别。replace 的语义主要体现在删除重建的命令上，而 apply 是对资源对象期望状态的更新。</p>
<p>根据课程中的例子来更好的理解：</p>
<pre><code class="language-yaml"># nginx.yaml 将 Nginx 容器镜像改为1.7.9
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
</code></pre>
<pre><code class="language-shell"># 这个命令所表达的语义，是要将 nginx 资源强制替换为修改后的资源
# 明确表示了执行过程：先删除，然后重建
$ kubectl replace -f nginx.yaml

# 而 apply 则只表明更新 nginx 资源的期望状态，具体的实现过程，由其自行处理
$ kubectl apply -f nginx.yaml
</code></pre>
<p>在实际的使用过程中，我们也要尽量避免使用 <code>replace -f</code> 命令，同时避免更新有上层抽象控制的底层资源对象。</p>
<h3 id="声明式-api-特点">声明式 API 特点</h3>
<p>现在我们也能更好的理解 <a href="https://kubernetes.io/zh/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis">Kubernetes 官网</a>中对于声明式 API 的一些说明，附带上一些我的理解：</p>
<ul>
<li>你的 API 包含相对而言为数不多的、尺寸较小的对象（资源）。
<ul>
<li><em>声明式重要的点在于描述，描述可以详细，但不应用于存储具体数据，应该描述其元数据。</em></li>
</ul>
</li>
<li>对象定义了应用或者基础设施的配置信息。</li>
<li>对象更新操作频率较低。</li>
<li>通常需要人来读取或写入对象。</li>
<li>对象的主要操作是 CRUD 风格的（创建、读取、更新和删除）。</li>
<li>不需要跨对象的事务支持：API 对象代表的是期望状态而非确切实际状态。
<ul>
<li><em>也就是说我们在设计抽象资源时，如果该资源的创建需要依赖其他资源的实际状态，那么应该考虑将其归属于所依赖的资源。</em></li>
</ul>
</li>
</ul>
<p>也能更好的理解什么不是声明式 API：</p>
<ul>
<li>客户端发出“做这个操作”的指令，之后在该操作结束时获得同步响应。
<ul>
<li><em>声明式 API 的一个特点，声明的永远是期望状态，不能即时得到处理成功的响应。对实时性要求很高的场景是不合适的。</em></li>
</ul>
</li>
<li>客户端发出“做这个操作”的指令，并获得一个操作 ID，之后需要检查一个 Operation（操作） 对象来判断请求是否成功完成。
<ul>
<li><em>我们要相信我们期望的状态是能达到的，并且不能在状态达成后才需要处理一些其他逻辑，如果是这样，应该考虑将这些逻辑放入声明式 API，或是放弃使用。</em></li>
</ul>
</li>
<li>你会将你的 API 类比为远程过程调用（Remote Procedure Call，RPCs）。
<ul>
<li><em>这很明显，过程调用强调的是过程，如果你的 API 非常注重过程的处理，那就不适合声明式 API</em></li>
</ul>
</li>
<li>直接存储大量数据；例如每个对象几 kB，或者存储上千个对象。</li>
<li>需要较高的访问带宽（长期保持每秒数十个请求）。</li>
<li>存储有应用来处理的最终用户数据（如图片、个人标识信息（PII）等）或者其他大规模数据。</li>
<li>在对象上执行的常规操作并非 CRUD 风格。
<ul>
<li><em>对于声明式 API 而言，我们对资源对象的操作是有限的，仅能对其进行状态转移，这也就局限为 CRUD 操作。如果一项操作不能抽象为状态的改变，那么就证明不适合声明式 API。</em></li>
</ul>
</li>
<li>API 不太容易用对象来建模。</li>
<li>你决定使用操作 ID 或者操作对象来表现悬决的操作。
<ul>
<li><em>这里“悬决的操作”英文原文为&quot;pending operations&quot;，表达的应该是悬而未决的意思。然而需要挂起，就表示你知道这个操作在可控的范围内需要依赖于其他操作的完成，这是不符合声明式 API 要求的。</em></li>
</ul>
</li>
</ul>
<h2 id="控制器模式">控制器模式</h2>
<p>从上面可以了解到，声明式 API 让我们可以描述资源对象的期望状态，那么 Kubernetes 内部是如何将期望状态转为实际状态的呢？答案就是 Kubernetes 的控制器模式。这是 kubernetes 的核心机制，也叫 Control Loop 或是 Reconcile Loop。</p>
<p>以下是 <a href="https://kubernetes.io/zh/docs/concepts/architecture/controller/">Kubernetes 官网</a>对于 Control Loop 的解释，很详细：</p>
<blockquote>
<p>在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。</p>
<p>这是一个控制环的例子：房间里的温度自动调节器。</p>
<p>当你设置了温度，告诉了温度自动调节器你的<em>期望状态（Desired State）</em>。 房间的实际温度是<em>当前状态（Current State）</em>。 通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。</p>
</blockquote>
<p>控制器模式指的就是这样一个控制循环，Kubernetes 中的控制器通过 “List&amp;Watch 机制” 实现对于 Kubernetes 中相关资源变化的关注，从而触发控制器逻辑的处理，完成最终用户的期望，并且实时更新资源的状态来告知用户。Kubernetes 自身的固有资源也都是通过这种形式来实现的。</p>
<p>这个控制循环确保了实际状态与期望状态的一致性，而实际状态向期望状态逐渐转换的这个过程，叫做 Reconcile，所以控制循环也叫做调谐循环（Reconcile Loop）。正是由于 Reconcile 的存在，它不断的执行“检查 -&gt; Diff -&gt; 更新实际状态”这样一个过程，才使得这个系统能够始终对系统当前状态与期望状态对比差异并采取必要的行动。</p>
<h4 id="期望状态与实际状态">期望状态与实际状态</h4>
<blockquote>
<p>Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。</p>
<p>在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。 这意味着很可能集群永远不会达到稳定状态。</p>
<p>只要集群中的控制器在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。</p>
</blockquote>
<h3 id="关于控制器的实现原理">关于控制器的实现原理</h3>
<p>限于篇幅，不讲了。感兴趣的可以看 Tower 上关于 Controller 原理和源码的笔记：<a href="/posts/kubernetes-samplecontroller">《从 SampleController 项目看 kubernetes controller 的设计》</a>。</p>
<h2 id="声明式的优点">声明式的优点</h2>
<h3 id="可读性">可读性</h3>
<p>声明式的描述通常比一连串的命令更具有可读性。</p>
<h4 id="dsl-2">DSL</h4>
<p>对于在 DSL 上的体现来说，DSL 通常比伪代码更接近自然语言，并且非程序员更容易学习。包括内部 DSL，通常也会比宿主语言实现同样功能的命令更加易读。</p>
<h4 id="函数式编程-2">函数式编程</h4>
<p>函数式编程也同样具有更高的可读性，因为所有的状态都是不可变的。你声明一个状态，但是不能改变这个状态。由于你无法改变它，所以在函数式编程中不需要变量。对函数式编程的讨论也更像是数学、公式，而不像是程序语句。</p>
<pre><code class="language-c">x = x + 1
</code></pre>
<p>如果你把这行代码交给一个数学家去看，他会认为这是一个不成立的等式。如果用函数式编程的形式：</p>
<pre><code class="language-c">y = x + 1
</code></pre>
<p>这个数学家就会明白 y 的值是 x + 1 的计算结果。并且它不会被改变，被声明之后，y 就永远代表的 x + 1。</p>
<h4 id="声明式-api">声明式 API</h4>
<p>面向终态的声明式 API 的可读性是毋庸置疑的，我们关注的就是对象最终的运行状态，现在可以通过对象的描述直接了解，而不用根据过程进行推算。</p>
<h3 id="简单">简单</h3>
<p>一段代码越简单，就越容易看懂并发现错误，也就越容易对系统进行修改。所以我们鼓励采用有意义的变量名，清晰的代码结构，整洁的系统架构等等。基于同样的理由，DSL 的本质就是<strong>通过简单来换取在某一领域内的高效</strong>。DSL 的简单体现在其有限的表达性上，它不需要做到万能，只相反，DSL 只需要解决系统某一领域内的问题。只有在这个领域内，DSL 才有用，也更推荐使用。</p>
<h3 id="幂等性">幂等性</h3>
<p>由于我们面向的最终状态，对状态修改的操作一定是幂等性的。因为没有副作用，所以对于重复操作的效果是稳定的，也就能更好的处理分布式环境和并发等问题。</p>
<h3 id="可交换性">可交换性</h3>
<p>上面也提到了，声明式 API 不需要跨对象的事务支持。换句话说，<strong>声明式 API 不需要事务中固定的执行顺序</strong>。因为我们描述的总是期望状态，所以在多个对象协作的场景中，对每个对象的创建或状态转移都是不需要保证执行顺序的。</p>
<h3 id="关于控制器模式的优点">关于控制器模式的优点</h3>
<p>当我们自己设计的 API 也经过良好的抽象，对外的表现形式与声明式 API 的表现形式一致时，我们为什么还要用 CRD 呢？</p>
<p>这就需要我们对控制器模式的一些思考，控制器模式对比命令式的执行模型有哪些优点。</p>
<p>在一次性的命令执行过程中，指令的执行失败是很难处理的，通常是响应错误后需要记录日志、报警及回滚等一系列操作。调用方在接收到响应错误时，也很难把握对象当前的状态，后续的处理也会很困难。</p>
<p>而控制器模式是一个永不终止的循环，在这个循环中，控制器会通过观察对象状态，不断尝试调谐（Reconcile）以达成实际状态和期望状态的一致。这个过程是包含错误处理的流程，不需要调用方费心。调用方也可以通过对象的 status 字段实时查看对象的当前状态，以便于辅助处理。</p>
<p><em>所以我认为当你的 API 足够声明式的时候，CRD 永远是首选项。</em></p>
<h2 id="相关链接">相关链接</h2>
<p><a href="https://skyao.io/learning-cloudnative/declarative/">声明式设计</a></p>
<p><a href="http://blog.zhaojie.me/2010/04/trends-and-future-directions-in-programming-languages-by-anders-2-declarative-programming-and-dsl.html">编程语言的发展趋势及未来方向（2）：声明式编程与DSL</a></p>
<p><a href="https://www.toptal.com/software/declarative-programming">Declarative Programming: Is It A Real Thing?</a></p>
<p><a href="https://www.cnblogs.com/lisperl/archive/2011/11/21/2257360.html">浅析函数式编程与命令式编程的区别（一）计算模型的区别</a></p>
<p><a href="https://draveness.me/dsl/">谈谈 DSL 以及 DSL 的应用（以 CocoaPods 为例）</a></p>
<p><a href="https://i.cloudnative.to/oam/event">【网络研讨会】GitOps 及 OAM 的落地实践</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes 网络笔记]]></title>
        <id>https://cnbailian.github.io/post/kubernetes-network-notes/</id>
        <link href="https://cnbailian.github.io/post/kubernetes-network-notes/">
        </link>
        <updated>2021-02-07T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>集群网络系统是 Kubernetes 的核心部分，其中 Pod 之间的通信的部分 Kubernetes 没有自己实现，而是交给了外部组件进行处理。Kubernetes 对这部分网络模型的要求是：节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。这就需要一个跨主机的容器网络。</p>
<p>本篇笔记前半部分记录了 VXLAN 技术。VXLAN 全称是 <code>Virtual eXtensible Local Area Network</code>，虚拟可扩展的局域网。它是一种 Overlay 技术，通过三层网络来搭建的二层网络。在笔记的后半部分，通过学习 <a href="https://github.com/coreos/flannel">Flannel</a> 的源码手动搭建跨主机容器网络示例。</p>
<p>笔记中 vxlan 内容学习自 <a href="https://cizixs.com/about/">cizixs</a> 的两篇博客，一篇<a href="https://cizixs.com/2017/09/25/vxlan-protocol-introduction/">介绍协议原理</a>，一篇<a href="https://cizixs.com/2017/09/28/linux-vxlan/">结合实践</a>。文章写的很详细，而且深入浅出适合学习，建议读者在 vxlan 部分直接看原文。</p>
]]></summary>
        <content type="html"><![CDATA[<p>集群网络系统是 Kubernetes 的核心部分，其中 Pod 之间的通信的部分 Kubernetes 没有自己实现，而是交给了外部组件进行处理。Kubernetes 对这部分网络模型的要求是：节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。这就需要一个跨主机的容器网络。</p>
<p>本篇笔记前半部分记录了 VXLAN 技术。VXLAN 全称是 <code>Virtual eXtensible Local Area Network</code>，虚拟可扩展的局域网。它是一种 Overlay 技术，通过三层网络来搭建的二层网络。在笔记的后半部分，通过学习 <a href="https://github.com/coreos/flannel">Flannel</a> 的源码手动搭建跨主机容器网络示例。</p>
<p>笔记中 vxlan 内容学习自 <a href="https://cizixs.com/about/">cizixs</a> 的两篇博客，一篇<a href="https://cizixs.com/2017/09/25/vxlan-protocol-introduction/">介绍协议原理</a>，一篇<a href="https://cizixs.com/2017/09/28/linux-vxlan/">结合实践</a>。文章写的很详细，而且深入浅出适合学习，建议读者在 vxlan 部分直接看原文。</p>
<!--more-->
<h2 id="vxlan-协议原理">VXLAN 协议原理</h2>
<p>上面提到 vxlan 是 overlay 技术，overlay 网络是建立在已有物理网络（underlay）上的虚拟网络，具有独立的控制和转发平面，对于连接到 overlay 的设备来说，物理网络是透明的。</p>
<p>那么 vxlan 这类的 Overlay 网络解决了那么些问题？</p>
<ul>
<li>传统的 VLAN 技术满足不了虚拟化场景下的数据中心规模，VLAN 最多只支持 4096 个网络上限。</li>
<li>数据中心需要提供多租户功能，不同用户之间需要独立的分配 IP 和 MAC 地址</li>
<li>云计算业务需要高灵活性，虚拟机可能会大规模迁移，并保证网络一直可用。</li>
</ul>
<p>vxlan 实现原理就是使用 VTEP 设备对服务器发出和收到的数据包进行二次封装和解封。所以 vxlan 这类隧道网络对原有的网络架构影响小，原来的网络不需要做任何改动，在原有网络上架设一层新的网络。</p>
<h3 id="vxlan-模型">VXLAN 模型</h3>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi3vmfk4nmj30g808ft9m.jpg" alt="vxlan" loading="lazy"></figure>
<p>物理网络上可以创建多个 vxlan 网络，这些 vxlan 网络可以认为是一个隧道，不同节点的虚拟机能够通过隧道直连。在每个端点上都有一个 vtep 负责 vxlan 协议的封包和解包，也就是在虚拟报文上封装 vtep 通信的报文头部。每个 vxlan 网络由唯一的 VNI 标识，不同的 vxlan 可以不互相影响。</p>
<ul>
<li>VTEP（VXLAN Tunnel Endpoints）：vxlan 网络的边缘设备，用来进行 vxlan 报文的处理（封包和解包）。vtep 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机）。</li>
<li>VNI（VXLAN Network Identifier）：VNI 是每个 vxlan 的标识，是个 24 位整数，一共有 2^24 = 16,777,216（一千多万），一般每个 VNI 对应一个租户，也就是说使用 vxlan 搭建的公有云可以理论上可以支撑千万级别的租户。</li>
<li>Tunnel：隧道是一个逻辑上的概念，在 vxlan 模型中并没有具体的物理实体想对应。隧道可以看做是一种虚拟通道，vxlan 通信双方（图中的虚拟机）认为自己是在直接通信，并不知道底层网络的存在。从整体来说，每个 vxlan 网络像是为通信的虚拟机搭建了一个单独的通信通道，也就是隧道。</li>
</ul>
<h3 id="vxlan-报文解析">VXLAN 报文解析</h3>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi3w2klximj30vn0f040c.jpg" alt="post-img" loading="lazy"></figure>
<p>白色部分是虚拟机发送的原始报文（二层帧，包含了 MAC 头部、IP 头部和传输层头部的报文），前面加上了 vxlan 头部用于保存 vxlan 相关内容，再前面是标准的 UDP 协议头部（UDP 头部、IP 头部和 MAC 头部）用来在底层网络上传输报文。</p>
<p>最外层的 UDP 协议用来在底层网络上传输，也就是 vtep 之间互相通信的基础。中间是 VXLAN 头部，vetp 接到报文后，根据这部分内容处理 vxlan 逻辑，主要是根据 VNI 发送到最终的虚拟机。最里面是原始报文，也就是虚拟机看到的报文内容。</p>
<p>报文各部分意义如下：</p>
<ul>
<li>VXLAN header：8 字节
<ul>
<li>VXLAN flags：标志位</li>
<li>Reserved：保留位</li>
<li>VNID：24 位的 VNID 标识</li>
<li>Reserved：保留位</li>
</ul>
</li>
<li>UDP 头部：8 字节
<ul>
<li>UDP：UDP 通信双方是 vtep 应用，IANA 分配的 vxlan 端口是 4789</li>
</ul>
</li>
<li>IP 头部：20 字节
<ul>
<li>目的地址：是由虚拟机所在地址宿主机的 vtep 的 IP 地址</li>
</ul>
</li>
<li>MAC 头部：14 字节
<ul>
<li>MAC 地址：主机之间通信的 MAC 地址</li>
</ul>
</li>
</ul>
<p>可以看出 vxlan 协议比原始报文多出 50 字节的内容，这会降低网络链路传输有效数据的比例。</p>
<h2 id="实现-vxlan">实现 VXLAN</h2>
<p>Linux 在 3.7.0 版本才开始支持 vxlan，请尽量使用比较新版本的 kernel，以免因为内核版本太低导致功能或性能出现问题。</p>
<p>我的实验环境是 2 台 AWS Debian 系统实例：</p>
<pre><code class="language-shell">$ uname -r
4.19.0-14-cloud-amd64
$ echo ${HOST1_IP}
172.16.3.142
$ echo ${HOST2_IP}
172.16.2.21
</code></pre>
<p>同时为了实验容器网络，会保证每台主机上都有 network namespace（net0）与 bridge（br0） 的连接关系。创建过程在上一篇笔记。</p>
<pre><code class="language-shell">$ ip netns
net0 (id: 0)
$ ip link
veth1
br0
$ ip netns exec net0 ip addr # host1
veth0
  link/ether 4e:3d:fd:29:55:38
  inet 192.168.2.11/24 scope global veth0
$ ip netns exec net0 ip addr # host2
veth0
  link/ether 46:63:12:3e:fa:da
  inet 192.168.2.12/24 scope global veth0
</code></pre>
<h3 id="点对点-vxlan">点对点 VXLAN</h3>
<p>首先创建 host1 的点对点的 VXLAN 设备，点对点设备是指创建 vxlan 时指定了 <code>remote</code> 参数的设备：</p>
<pre><code class="language-shell">$ ip link add type vxlan id 1 dstport 4789 dev eth0 remote 172.16.2.21
</code></pre>
<p><code>id 1</code> 表示 VNI，在点对点的设备中需要双方保持一致。</p>
<p><code>dstport 4789</code> 是IANA 分配的 vxlan 端口是 4789，Linux 默认使用 8472，所以这里显式分配。</p>
<p><code>dev eth0</code> 表示当前节点用于通信的网络设备，用于获取 IP，与 <code>local 172.16.3.142</code> 参数等效。</p>
<p><code>remote 172.16.2.21</code> 显示指定了 vxlan 的对口 IP，所以只会发往这个地址，类似点对点协议。</p>
<p>host2 主机同样需要创建，注意 <code>id &amp; dspport</code> 参数要保持一致，<code>remote</code> 参数要指定 host1 IP：</p>
<pre><code class="language-shell">$ ip link add type vxlan id 1 dstport 4789 dev eth0 remote 172.16.3.142
</code></pre>
<p>在两台主机上将 vxlan 设备挂载至 bridge，并启动：</p>
<pre><code class="language-shell">$ ip link set vxlan0 master br0
$ ip link set up vxlan0
</code></pre>
<p>尝试 ping:</p>
<pre><code class="language-shell">$ ip netns exec net0 ping -c1 192.168.2.12
PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data.
64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=1.31 ms

--- 192.168.2.12 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<h3 id="vxlan-网络">VXLAN 网络</h3>
<p>点对点设备只能两两通信，实际用处不大。我们需要组成 vxlan 网络，在 vxlan 网络中有着一个问题：vtep 如何感知彼此的存在并选择正确路径传输报文？从上面的封装的报文中来看，有两个地址在发送时是不确定的：</p>
<ol>
<li>对方 vtep 的 IP 地址
<ul>
<li>在 IP 头部，需要的是双方 vtep 的 IP 地址，源地址可以很简单确定，目的地址是要发往的<strong>虚拟机所在地址的宿主机的 vtep 的 IP 地址</strong>，而我们在发送时只知道对方虚拟机 IP 的地址。</li>
</ul>
</li>
<li>对方虚拟机 MAC 地址
<ul>
<li>在内部报文中，通信双方是知道对方 IP 地址的，但如果是同一网段的通信，还需要知道对方<strong>虚拟机的 MAC 地址</strong>。</li>
</ul>
</li>
</ol>
<p>那么在点对点的 vxlan 设备上为什么没有这个问题呢？</p>
<p>在点对点的设备中，对方 vtep IP 地址在创建 vxlan 设备由 <code>remote</code> 参数指定。由于是同网段，vxlan 设备将 ARP 请求也发送到了对点的 vtep 上，所以能够直接获得对方的 ARP 响应。</p>
<p><a href="https://cizixs.com/2017/09/25/vxlan-protocol-introduction/">《vxlan 协议原理简介》</a>中提出了两个解决方案：多播和分布式控制中心；多播需要底层网络设备的配合，有一定局限性，而且多播方式会带来报文的浪费，在实际生产中很少用到。而分布式控制的 vxlan 是一种典型的 <a href="https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C/9117977">SDN</a> 架构，也是目前使用最广泛的方式。</p>
<h3 id="分布式控制中心">分布式控制中心</h3>
<p>多播的解决方案是在发送报文前以广播的方式自动学习地址，可是这太浪费了。所以分布式控制中心的解决方案就是提前知道地址信息，直接告诉 vtep，这就不需要多播了。</p>
<p>一般情况下，在每个 vtep 所在的节点都会有一个 agent，它会和控制中心通信，获取 vtep 需要的信息以某种方式告知 vtep。不止告知的方式不同，告知的时间也有区别。一般有两种方式：常见的是一旦知道信息就立刻告知 vtep，即使它可能用不上，一般这时候第一次通信还没有发生；另一种方式是在第一次通信时 vtep 以某种方式通知 agent，然后 agent 才告诉 vtep 这些信息。</p>
<h4 id="arp-和-fdb">ARP 和 FDB</h4>
<p>先解释一下 ARP 表和 FDB（二层转发表）表。</p>
<p>ARP 表是由三层设备（路由器，三层交换机，服务器，电脑）用来存储 ip 地址和 mac 地址对应关系的一张表。</p>
<p>FDB 是二层转发表，它是由2层设备（二层交换机）用来存储mac地址和交换机接口地址对应关系的一张表，用于帮助交换机指明 MAC 帧应从哪个端口发出去。Linux vxlan 设备的 FDB 表与上面说的交换机的 FDB 表略有不同，vxlan 设备的 FBD 表保存的是 mac 地址与其他 vxlan 设备的 vtep 地址。</p>
<h4 id="手动维护-fdb-表">手动维护 FDB 表</h4>
<p>在多播中以广播的形式获取宿主机的 IP 地址。如果提前知道目的虚拟机的 MAC 地址和它所在的主机的 IP 地址，可以通过更新 FDB 表项来减少广播报文的数量。这就能解决第一个问题。</p>
<pre><code class="language-shell">$ ip link add type vxlan id 1 dstport 4789 dev eth0 nolearning
</code></pre>
<p>添加 <code>nolearning</code> 参数告诉 vtep 不要通过收到的报文来学习 FDB 表项的内容，因为我们会手动维护这个列表。</p>
<pre><code class="language-shell">$ bridge fdb append 4e:3d:fd:29:55:38 dev vxlan0 dst 172.16.3.142 # host1 netns 与宿主机 IP 映射
$ bridge fdb append 46:63:12:3e:fa:da dev vxlan0 dst 172.16.2.21 # host2 netns 与宿主机 IP 映射
</code></pre>
<p>通过这个映射表，在发送报文时，vtep 搜索 FDB 表项就知道应该发送到哪个对应的 vtep 上了。需要注意的是，还需要一个默认的表项，以便 vtep 在不知道对应关系时可以通过默认方式发送 ARP 报文去查询对方的 MAC 地址。</p>
<pre><code class="language-shell">$ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 172.16.3.142
$ bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 172.16.2.21
</code></pre>
<h4 id="手动维护-arp-表">手动维护 ARP 表</h4>
<p>单独维护 FDB 表并没有作用，因为在不知道对方虚拟机 MAC 地址的情况下还是会广播大量的 ARP 报文。所以 ARP 表也需要手动维护。这能解决第二个问题。</p>
<p>但 ARP 表的维护不同于 FDB 表，因为最终通信的双方是容器。到每个容器里面去更新对应的 ARP 表，是件工作量很大的事情，而且容器的创建和删除还是动态的。Linux 提供了一个解决方案，vtep 可以作为 ARP 代理，回复 ARP 请求，也就是说只要 vtep interface 知道对应的 <code>IP - MAC</code> 关系，在接收到容器发来的 ARP 请求时可以直接做出应答。我们只需要更新 vtep interface 上的 ARP 表项就行了。</p>
<pre><code class="language-shell">$ ip link add type vxlan id 1 dstport 4789 dev eth0 nolearning proxy
</code></pre>
<p>添加 <code>proxy</code> 参数告知 vtep 承担 ARP 代理的功能。如果收到 ARP 请求，并且自己知道结果就直接作出应答。</p>
<pre><code class="language-shell">$ ip neigh add 192.168.2.11 lladdr 4e:3d:fd:29:55:38 dev vxlan0
$ ip neigh add 192.168.2.12 lladdr 46:63:12:3e:fa:da dev vxlan0
</code></pre>
<p>在要通信的所有节点配置完之后，容器就能相互 ping 通。当容器要访问彼此，并且第一次发送 ARP 请求时，这个请求并不会发送给所有的 vtep，而是由当前的 vtep 作出应答，大大减少了网络上的报文。</p>
<pre><code class="language-shell">$ ip netns exec net0 ping -c1 192.168.2.12
PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data.
64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=1.15 ms

--- 192.168.2.12 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<p>这里要注意的是前面的示例中 netns 都是在同一网段 <code>192.168.2.0/24</code>，实际的项目会需要更大的网段，而跨网段就需要走网关。</p>
<h4 id="动态维护-arp-和-fdb-表">动态维护 ARP 和 FDB 表</h4>
<p>尽管通过手动维护 FDB 表和 ARP 表可以避免多余的网络报文，但是还有一个问题：为了能让所有的容器正常工作，所有可能会通信的容器都必须提前添加到 ARP 和 FDB 表项中。但并不是网络上所有的容器都会相互通信，所以添加的有些表项是用不到的。</p>
<p>Linux 提供了一种方法，内核能够动态地通知节点要和哪个容器通信，应用程序可以订阅这些事件，如果内核发现需要的 ARP 或者 FDB 表项不存在，会发送事件给订阅的应用程序，这样应用程序可以从控制中心拿到这些信息来更新表项，做到更精确的控制。</p>
<pre><code class="language-shell">$ ip link add vxlan0 type vxlan id 1 dstport 4789 dev eth0 nolearning proxy l2miss l3miss
</code></pre>
<p>这次多了两个参数 <code>l2miss</code> 和 <code>l3miss</code>：</p>
<ul>
<li><code>l2miss</code>：如果设备找不到 MAC 地址需要的 vtep 地址，就会发送通知事件</li>
<li><code>l3miss</code>：如果设备找不到 IP 地址需要的 MAC 地址，就会发送通知事件</li>
</ul>
<p><code>ip monitor</code> 命令可以监听某个 interface 的事件：</p>
<pre><code class="language-shell">$ ip monitor all dev vxlan0
</code></pre>
<p>如果从本节点容器 ping 另外一个节点的容器，就先发生 l3 miss：</p>
<pre><code class="language-shell">$ ipmonitor all dev vxlan0
[nsid current]miss 10.20.1.3  STALE
</code></pre>
<p><code>l3miss</code> 是说这个 IP 地址，vtep 不知道它对应的 MAC 地址，因此要手动添加 ARP 记录：</p>
<pre><code class="language-shell">$ ip neigh add 192.168.2.12 lladdr 46:63:12:3e:fa:da dev vxlan0 nud reachable
</code></pre>
<p><code>nud reachable</code> 参数代表系统发现其无效一段时间后会自动删除。</p>
<p>添加 ARP 表项后还是不能正常通信，接着会出现 l2miss 的通知事件：</p>
<pre><code class="language-shell">$ ip monitor all dev vxlan0
[nsid current]miss lladdr 46:63:12:3e:fa:da STALE
</code></pre>
<p>这个事件是说不知道这个容器的 MAC 地址在哪个节点上，所以要手动添加 FDB 记录：</p>
<pre><code class="language-shell">$ bridge fdb append 46:63:12:3e:fa:da dev vxlan0 dst 172.16.2.21
</code></pre>
<h2 id="flannel">Flannel</h2>
<p>Flannel 是 CoreOS 为 Kubernetes 设计的网络插件，实现简单且容易配置，但社区不怎么活跃，不过用来学习还是很好的。</p>
<h3 id="some-design-notes-and-history">Some design notes and history</h3>
<p>Flannel 对于网络的实现有不同的 <code>backend</code>，vxlan 的实现在 <code>backend/vxlan</code> 中， 源码文件 <code>vxlan.go</code> 的注释中记载了一些修改历史：</p>
<ol>
<li>
<p>Flannel 的第一个版本，l3miss 学习，通过查找 ARP 表 MAC 完成的。 l2miss 学习，通过获取 VTEP 上的 public ip 完成的。</p>
</li>
<li>
<p>Flannel 的第二个版本，移除了 l3miss 学习的需求，当远端主机上线，只是直接添加对应的 ARP 表项即可，不用查找学习了。</p>
</li>
<li>
<p>Flannel的最新版本，移除了 l2miss 学习的需求，不再监听 netlink 消息。</p>
<p>它的工作模式：</p>
<ol>
<li>创建 vxlan 设备，不再监听任何 l2miss 和 l3miss 事件消息</li>
<li>为远端的子网创建路由</li>
<li>为远端主机创建静态 ARP 表项</li>
<li>创建 FDB 转发表项，包含 VTEP MAC 和远端 Flannel 的 public IP</li>
<li>同一个 VNI 下每一台 Host 主机仅包含 1 route，1 arp entry and 1 FDB entry。</li>
<li>还有一个选项是跳过对位于同一子网的主机使用vxlan，这被称为“directRouting”</li>
</ol>
</li>
</ol>
<p>l2miss 和 l3miss 方案缺陷</p>
<ol>
<li>每一台 Host 需要配置所有需要互通 Guest 路由，路由记录会膨胀，不适合大型组网</li>
<li>通过 netlink 通知学习路由的效率不高</li>
<li>Flannel Daemon 异常后无法持续维护 ARP 和 FDB 表，从而导致网络不通</li>
</ol>
<p>在最新的方案中，有选项可以跳过对同一子网上的主机使用vxlan，称为“directRouting（直达路由）”。</p>
<h3 id="源码分析">源码分析</h3>
<pre><code>func main() {
	// 创建 SubnetManager 用于管理子网。sm 有两种模式，通过 kube-subnet-mgr 划分。kubeSubnetMgr 使用 Kubernetes 管理子网；etcdSubnetMgr 使用 etcd 管理子网。
	sm, err := newSubnetManager()
	if err != nil {
		log.Error(&quot;Failed to create SubnetManager: &quot;, err)
		os.Exit(1)
	}
	log.Infof(&quot;Created subnet manager: %s&quot;, sm.Name())

	// 创建 BackendManager，随后根据类型获取 BackendNetwork，用于在 Node 上创建网络。backends 通过 init 函数在 backend.Register 中注册，BackendManager 通过 GetBackend 获得对应类型的 backend，类型通过 Flannel config 文件 BackendType 字段获取。
	// Create a backend manager then use it to create the backend and register the network with it.
	bm := backend.NewManager(ctx, sm, extIface)
	be, err := bm.GetBackend(config.BackendType)
	if err != nil {
		log.Errorf(&quot;Error fetching backend: %s&quot;, err)
		cancel()
		wg.Wait()
		os.Exit(1)
	}

	// 获得 backend 后，使用 RegisterNetwork 方法创建主机网络。
	bn, err := be.RegisterNetwork(ctx, &amp;wg, config)
	if err != nil {
		log.Errorf(&quot;Error registering network: %s&quot;, err)
		cancel()
		wg.Wait()
		os.Exit(1)
	}

	// Start &quot;Running&quot; the backend network. This will block until the context is done so run in another goroutine.
	log.Info(&quot;Running backend.&quot;)
	wg.Add(1)
	go func() {
		// 监听子网事件，通过 handleSubnetEvents 为主机创建静态路由、ARP表项、FDB表项。
		// kubeSubnetManager 在 newKubeSubnetManager 时通过 informer 监听 Node 事件，发送给 events 不同的 object，然后进行处理
		bn.Run(ctx)
		wg.Done()
	}()

	daemon.SdNotify(false, &quot;READY=1&quot;)

	// Kube subnet mgr doesn't lease the subnet for this node - it just uses the podCidr that's already assigned.
	if !opts.kubeSubnetMgr {
		err = MonitorLease(ctx, sm, bn, &amp;wg)
		if err == errInterrupted {
			// The lease was &quot;revoked&quot; - shut everything down
			cancel()
		}
	}
}
</code></pre>
<p><code>vxlan.go RegisterNetwork()</code></p>
<pre><code>func (be *VXLANBackend) RegisterNetwork(ctx context.Context, wg *sync.WaitGroup, config *subnet.Config) (backend.Network, error) {
  // 通过 config 文件中 Backend 字段获取配置。设置 vxlanDeviceAttrs，使用 VNI 作为 name。
  devAttrs := vxlanDeviceAttrs{
		vni:       uint32(cfg.VNI),
		name:      fmt.Sprintf(&quot;flannel.%v&quot;, cfg.VNI),
		vtepIndex: be.extIface.Iface.Index,
		vtepAddr:  be.extIface.IfaceAddr,
		vtepPort:  cfg.Port,
		gbp:       cfg.GBP,
		learning:  cfg.Learning,
	}
  // 使用 vxlanDeviceAttrs 设置 vxlanDevice
  // newVXLANDevice 函数通过 github.com/vishvananda/netlink 包创建 vxlan 设备，然后设置 net/ipv6/conf/${device_name}/accept_ra 的配置。
	dev, err := newVXLANDevice(&amp;devAttrs)
	if err != nil {
		return nil, err
	}
	dev.directRouting = cfg.DirectRouting

  // 通过 newSubnetAttrs 函数获取配置，使用 subnetMgr 设置子网并得到 Lease。
	subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr())
	if err != nil {
		return nil, err
	}

	lease, err := be.subnetMgr.AcquireLease(ctx, subnetAttrs)
	switch err {
	case nil:
	case context.Canceled, context.DeadlineExceeded:
		return nil, err
	default:
		return nil, fmt.Errorf(&quot;failed to acquire lease: %v&quot;, err)
	}

	// Ensure that the device has a /32 address so that no broadcast routes are created.
	// This IP is just used as a source address for host to workload traffic (so
	// the return path for the traffic has an address on the flannel network to use as the destination)
  // 配置 vxlan 设备 addr，然后启动设备。设置 vxlan 设备为子网中的 /32 地址
	if err := dev.Configure(ip.IP4Net{IP: lease.Subnet.IP, PrefixLen: 32}); err != nil {
		return nil, fmt.Errorf(&quot;failed to configure interface %s: %s&quot;, dev.link.Attrs().Name, err)
	}

	return newNetwork(be.subnetMgr, be.extIface, dev, ip.IP4Net{}, lease)
}
</code></pre>
<h3 id="linux-实现-flannel-网络">Linux 实现 flannel 网络</h3>
<p>Flannel 网络配置不需要维护过多的表项，在同一个 VNI 下的每台主机仅需要配置一个路由、一个 ARP 表项、一个 FDB 表项。配置的表项变少，解决了手动维护 FDB 表和 ARP 表所带来的过多的无用表项问题，但相应的也会增加报文的发送，这也是 flannel 在实现上的取舍问题。</p>
<h4 id="环境">环境</h4>
<p>Flannel 配置：</p>
<pre><code class="language-json">{
  &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
  &quot;Backend&quot;: {
    &quot;Type&quot;: &quot;vxlan&quot;
  }
}
</code></pre>
<p>Flannel 使用 <code>/16</code> CIDR，为每个节点分配一个 <code>/24</code> 的子网，所以此时的 network namespace 变为：</p>
<pre><code class="language-shell">$ ip netns exec net0 ip addr # host1
veth0:
  inet 10.244.0.2/24 scope global veth0
$ ip netns exec net0 ip addr # host2
veth0:
  inet 10.244.1.2/24 scope global veth0
</code></pre>
<p>因为跨网段，所以为 br0 设置 IP 地址，并修改路由表做为网关：</p>
<pre><code class="language-shell">$ ip netns exec net0 ip route add 10.244.0.0/16 via 10.244.0.1 dev veth0 onlink # host1
$ ip addr
br0:
  inet 10.244.0.1/24 scope global br0
$ ip netns exec net0 ip route add 10.244.0.0/16 via 10.244.1.1 dev veth0 onlink # host2
$ ip addr
br0:
  inet 10.244.1.1/24 scope global br0
</code></pre>
<h4 id="示例">示例</h4>
<p>配置 vxlan 设备：</p>
<pre><code class="language-shell">$ ip link add vxlan0 type vxlan id 1 dstport 4789 dev eth0 nolearning
$ ip link set up vxlan0
$ ip link # host1
vxlan0:
  link/ether c2:cb:69:f5:a6:e4
$ ip link # host2
vxlan0:
  link/ether 66:8e:33:ac:7a:22
</code></pre>
<p>设置路由表：</p>
<pre><code class="language-shell">$ ip addr add 10.244.0.0 dev vxlan0 # 本机 vxlan IP
$ ip route add 10.244.1.0/24 via 10.244.1.0 dev vxlan0 onlink # 在 host1 设置 host2 路由表
# $ ip addr add 10.244.1.0 dev vxlan0
# $ ip route add 10.244.0.0/24 via 10.244.0.0 dev vxlan0 onlink
</code></pre>
<p>设置 FDB 表：</p>
<pre><code class="language-shell">$ bridge fdb append 66:8e:33:ac:7a:22 dev vxlan0 dst 172.16.2.21 # host2 主机的 vxlan MAC 地址与主机 IP
# bridge fdb append c2:cb:69:f5:a6:e4 dev vxlan0 dst 172.16.3.142
</code></pre>
<p>设置 ARP 表：</p>
<pre><code class="language-shell">$ ip neigh add 10.244.1.0 dev vxlan0 lladdr 66:8e:33:ac:7a:22 # host2 vxlan MAC 与 vxlan IP
# $ ip neigh add 10.244.0.0 dev vxlan0 lladdr c2:cb:69:f5:a6:e4
</code></pre>
<p>测试 ping：</p>
<pre><code class="language-shell">$ ip netns exec net0 ping -c1 10.244.1.2
PING 10.244.1.2 (10.244.1.2) 56(84) bytes of data.
64 bytes from 10.244.1.2: icmp_seq=1 ttl=62 time=1.11 ms

--- 10.244.1.2 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<p>如果需要，别忘记设置内核 ip forward 参数：</p>
<pre><code class="language-shell">$ sysctl -w net.ipv4.ip_forward=1
</code></pre>
<h4 id="总结">总结</h4>
<p>Flannel 基于每台节点一个 <code>/24</code> 的网段，大大减少了维护 ARP 和 FDB 表项的工作，所增加的只是数据包达到目的地主机后的少量 ARP 请求，每次容器的增减也不需要触发维护。对比完全手动维护的方案来说，要好得多。</p>
<h2 id="参考文章">参考文章</h2>
<p><a href="https://cizixs.com/2017/09/25/vxlan-protocol-introduction/">vxlan 协议原理简介</a></p>
<p><a href="https://cizixs.com/2017/09/28/linux-vxlan/">linux 上实现 vxlan 网络</a></p>
<p><a href="https://zdyxry.github.io/2020/01/03/%e4%b8%ba%e4%bb%80%e4%b9%88-flannel-1-%e4%b8%a2%e5%a4%b1%e5%90%8e%e4%b8%8d%e4%bc%9a%e8%87%aa%e5%8a%a8%e9%87%8d%e5%bb%ba/">为什么 flannel.1 丢失后不会自动重建</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[容器网络学习笔记]]></title>
        <id>https://cnbailian.github.io/post/container-netwrok-notes/</id>
        <link href="https://cnbailian.github.io/post/container-netwrok-notes/">
        </link>
        <updated>2021-02-04T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>容器技术涉及的知识点很多，包括进程隔离、容器网络、分层存储等等，我对其中容器网络部分很感兴趣，并有较为深入的学习。此篇文章用于记录我的学习笔记。</p>
]]></summary>
        <content type="html"><![CDATA[<p>容器技术涉及的知识点很多，包括进程隔离、容器网络、分层存储等等，我对其中容器网络部分很感兴趣，并有较为深入的学习。此篇文章用于记录我的学习笔记。</p>
<!--more-->
<h2 id="概述">概述</h2>
<p>提到容器技术，大家可能知道容器通过 Linux Namespace 技术实现资源隔离。Namespace 是 kernel 对全局系统资源的一种封装隔离，比如 PID、User、Network 等等，改变 namespace 中被隔离的系统资源，只会影响当前 namespace 中的进程，对其他 namespace 中的进程没有影响。</p>
<p>Network namespace 就是本文主要涉及的一个 namespace，它被用来隔离网络设备、IP 地址端口等，每个 namespace 都有自己独立的网络协议栈、IP 路由表、防火墙规则、sockets等。</p>
<p>有了不同的 network namespace 之后，也就有了网络隔离，但一个完全被隔离的网络环境没有实际用处，这就需要通过 Linux 的虚拟网络设备为其插上“网卡”，以连通更多的网络。Linux 虚拟网络设备很多，这里主要介绍的是构建容器网络要用到的 Veth 与 Bridge，前者可以连接两个被隔离的 network namespace，后者则可以让更多的 network namespace 加入进来。</p>
<h2 id="linux-veth">Linux Veth</h2>
<h3 id="linux-网络设备">Linux 网络设备</h3>
<p>Linux 的网络设备就像一个双向的管道，数据从一端进，就会从另一端出，关键要看这两端是什么。用常见的 eth0 举例，eth0 设备的一端连接网络协议栈，另一端连接网卡。用户通过 socket api 调用，经过 Linux 网络协议栈，进入 eth0 网络设备，最后发送到网卡。</p>
<pre><code>+-------------------------------------------+
|                                           |
|        +-------------------+              |
|        | User Application  |              |
|        +-------------------+              |   
|                 |                         |     
|.................|.........................|
|                 ↓                         |     
|           +----------+                    |     
|           | socket   |                    |     
|           +----------+                    |     
|                 |                         |     
|.................|.........................|
|                 ↓                         |     
|      +------------------------+           |     
|      | Newwork Protocol Stack |           |     
|      +------------------------+           |     
|                 |                         |     
|.................|.........................|
|                 ↓                         |     
|        +----------------+                 |     
|        |      eth0      |                 |     
|        +----------------+                 |     
|                 |                         |
|                 |                         |
|                 |                         |
+-----------------|-------------------------+
                  ↓
          Physical Network
</code></pre>
<h3 id="veth-pair">Veth Pair</h3>
<p>Veth 作为 Linux 的虚拟网络设备，它总是成对（pair）出现，它的一端连接着网络协议栈，另一端两个设备彼此相连。这个特性使得一个设备收到协议栈的数据请求后，会将数据发送到另一个设备上去。</p>
<pre><code>+----------------------------------------------------------------+
|                                                                |
|       +------------------------------------------------+       |
|       |             Newwork Protocol Stack             |       |
|       +------------------------------------------------+       |
|              ↑               ↑               ↑                 |
|..............|...............|...............|.................|
|              ↓               ↓               ↓                 |
|        +----------+    +-----------+   +-----------+           |
|        |   eth0   |    |   veth0   |   |   veth1   |           |
|        +----------+    +-----------+   +-----------+           |
|              ↑               ↑               ↑                 |
|              |               +---------------+                 |
|              |         192.168.2.11     192.168.2.1            |
+--------------|-------------------------------------------------+
               ↓
         Physical Network
</code></pre>
<p>可以通过这个特性，实现两个 network namespace 网络的互通。</p>
<h4 id="示例">示例</h4>
<p>通过示例创建 network namespace 与 veth pair，并实现网络互通。</p>
<pre><code class="language-shell"># 创建 network namespace
root@ubuntu:~$ ip netns add net0
root@ubuntu:~$ ip netns add net1
# 创建 veth pair
# 因为未指定名称，会默认生成 veth0 和 veth1，如果有其他 veth 设备序号会顺延
# 如果想指定名字：ip link add vethfoo type veth peer name vethbar
root@ubuntu:~$ ip link add type veth
# 将 veth0 设备转给 net0 namespace
root@ubuntu:~$ ip link set dev veth0 netns net0
# 将 veth1 设备转给 net1 namespace
root@ubuntu:~$ ip link set dev veth1 netns net1
# 分别设置设备 IP
# ip netns exec 命令是进入 network namespace 内执行指令
root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0
root@ubuntu:~$ ip netns exec net1 ip addr add 192.168.2.1/24 dev veth1
# 启动 veth pair
root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up
root@ubuntu:~$ ip netns exec net1 ip link set dev veth1 up
# 尝试 ping
root@ubuntu:~$ ip netns exec net0 ping -c1 192.168.2.1
PING 192.168.2.1 (192.168.2.1) from 192.168.2.11 veth0: 56(84) bytes of data.
64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=0.032 ms

--- 192.168.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<h3 id="network-namespace">Network namespace</h3>
<p>多记一些 network namespace 相关的知识点。</p>
<p>每个新的 network namespace 创建之后默认会有一个 lo 设备，除此之外的其他网络设备就需要创建或移动过来。注意 lo 设备默认是关闭的，需要自己手动启动。</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns add net0
root@ubuntu:~$ ip netns exec net0 ip link
lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</code></pre>
<p>上面的示例中将 veth pair 设备分别给了两个 namespace，但被标记为“local device”的设备不能被移动，比如 loopback、bridge、ppp 等。可以通过 <code>ethtool -k</code> 命令查看设备的 <code>netns-local</code> 属性：</p>
<pre><code class="language-shell">root@ubuntu:~$ ethtool -k lo|grep netns-local
netns-local: on [fixed]
root@ubuntu:~$ ethtool -k veth0|grep netns-local
netns-local: off [fixed]
</code></pre>
<h2 id="linux-bridge">Linux Bridge</h2>
<p>虽然 veth pair 可以实现两个 network namespace 之间的通信，但是当多个 namespace 需要通信的时候，就需要 bridge 了。bridge 同样是 Linux 虚拟网络设备，具有网络设备的特征，可以配置 IP、MAC 地址等，但 bridge 同时也是一个虚拟交换机，和物理交换机有类似的功能。</p>
<p>对于普通的网络设备来说，只有两个端口，从一端进来的数据会从另一端出去。而 bridge 不同，bridge 有多个端口，数据可以从任何端口进来，进来之后从哪个端口出去和物理交换机的原理差不多，要看 MAC 地址。</p>
<p>所以，要想实现多 network namespace 的网络通信，就需要 bridge 这个虚拟交换机。</p>
<h3 id="使用-bridge-连接不同的-namespace">使用 bridge 连接不同的 namespace</h3>
<p>首先创建并启动 bridge，将其取名为 br0：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip link add name br0 type bridge
root@ubuntu:~$ ip link set br0 up
root@ubuntu:~$ ip link
br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 9a:a8:84:37:d4:56 brd ff:ff:ff:ff:ff:ff
</code></pre>
<p>同样，network namespace 也要准备好：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns add net0
root@ubuntu:~$ ip netns add net1
</code></pre>
<h4 id="示例-2">示例</h4>
<p>现在两个网络环境与虚拟交换机都已准备好，接下来将使用 veth pair 进行连接互通：</p>
<pre><code class="language-shell"># 创建 net0 使用的 veth pair
root@ubuntu:~$ ip link add type veth
# 将 veth0 移至 net0
root@ubuntu:~$ ip link set dev veth0 netns net0
# 设置 IP 并启动
root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0
root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up
# 将其对应的另一个设备 attach 到 bridge 上并启动
root@ubuntu:~$ ip link set dev veth1 master br0
root@ubuntu:~$ ip link set dev veth1 up
# net1 同理
root@ubuntu:~$ ip link add type veth
root@ubuntu:~$ ip link set dev veth0 netns net1
root@ubuntu:~$ ip netns exec net1 ip addr add 192.168.2.1/24 dev veth0
root@ubuntu:~$ ip netns exec net1 ip link set dev veth0 up
root@ubuntu:~$ ip link set dev veth2 master br0
root@ubuntu:~$ ip link set dev veth2 up
</code></pre>
<p>测试 ping：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns exec net0 ping -c1 192.168.2.1
PING 192.168.2.1 (192.168.2.1) 56(84) bytes of data.
64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=0.045 ms

--- 192.168.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<p>Veth pair 在此时的作用就相当于网线，一头（veth0）连着容器（network namespace），另一头（veth1）连着交换机（bridge）。bridge 作为交换机，当有设备 attach 到 bridge，就相当于交换机上插了一个新网线。当有请求到达 bridge 设备时，就可以通过报文中的 MAC 地址进行广播、转发、丢弃处理。</p>
<h3 id="给-bridge-配上-ip">给 bridge 配上 IP</h3>
<p>Bridge 与现实世界的二层交换机有一个区别：数据可以直接被发到 bridge 上，而不是从一个端口接受。这种情况可以看做 bridge 自己有一个 MAC 可以主动发送报文，或者说 bridge 自带了一个隐藏端口和寄主 Linux 系统自动连接，Linux 上的程序可以直接从这个端口向 bridge 上的其他端口发数据。</p>
<p>由此带来一个有意思的事情是，bridge 可以设置 IP 地址。通常来讲 IP 地址是三层协议的内容，不应该出现在二层设备 bridge 上，但 bridge 是虚拟交换机，属于通用网络设备的抽象的一种，只要是网络设备就能够设定 IP 地址。</p>
<p>当一个 bridge 拥有 IP 后，Linux 便可以通过路由表或者 IP 表规则在三层定位 bridge，此时相当于 Linux 拥有了另外一个隐藏的虚拟网卡和 bridge 的隐藏端口相连，这个网卡就是名为 br0 的通用网络设备，IP 可以看成是这个网卡的。当有符合此 IP 的数据到达 br0 时，内核协议栈认为收到了一包目标为本机的数据，此时应用程序可以通过 socket 接收到它。</p>
<h4 id="示例-3">示例</h4>
<p>接上文环境，为 bridge 配置 IP：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip addr add 192.168.2.12/24 dev br0
</code></pre>
<p>在主机上尝试 ping net0：</p>
<pre><code class="language-shell">root@ubuntu:~$ ping -I br0 -c1 192.168.2.11
PING 192.168.2.11 (192.168.2.11) from 192.168.2.12 br0: 56(84) bytes of data.
64 bytes from 192.168.2.11: icmp_seq=1 ttl=64 time=0.057 ms

--- 192.168.2.11 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<p>在 net1 中尝试 ping br0:</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns exec net1 ping -c1 192.168.2.12
PING 192.168.2.12 (192.168.2.12) 56(84) bytes of data.
64 bytes from 192.168.2.12: icmp_seq=1 ttl=64 time=0.061 ms

--- 192.168.2.12 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<h2 id="与外部网络通信">与外部网络通信</h2>
<p>上文给 bridge 配置 IP 后，network namespace 已经可以通过 br0 与宿主机的网络协议栈通信，但我们还需要与外部的网络通信。</p>
<p>其中的一种方法是将物理网卡设备 eth0 也 attach 到 br0 上。br0 根本不区分 attach 的是物理设备还是虚拟设备，对它来说都一样，都是网络设备，这就相当于 br0 拥有了一条连接外部物理设备的网线。此时连接到 br0 的 network namespace 都可以通过 br0 访问外部网络。但由于我是使用的云主机，通过 ssh 连接，无法很方便的调试，所以没有试过这种方法。</p>
<p>上一种方法不需要经过宿主机网络协议栈，直接就可以通过 eth0 设备发送数据。而第二种方法，可以不接入 eth0 设备，而是通过 IP forward 将数据转发。同时由于 network namespace 是分配的内网 IP，所以一般在发出去之前还需要经过 NAT 转换。</p>
<h3 id="ip-forward">IP forward</h3>
<p>“IP forwarding” 和 “routing” 是同义词，因为属于 Linux 内核的特性，所以也被叫做 “kernel IP forwarding”。所谓转发的概念就是 Linux 内核实现了路由器的功能，根据数据包的 IP 地址将数据从一个网络发送到另一个网络，该网络根据路由表配置继续发送数据包。</p>
<p>出于安全考虑，Linux 默认是禁止数据包转发的。如果想要启用，需要修改内核参数 <code>net.ipv4.ip_forward</code>。这个参数的值指定了是否启用转发功能；为 0 时禁用，为 1 时表示启用。</p>
<pre><code class="language-shell">root@ubuntu:~$ sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 0
# 也可以通过 /proc 查看
root@ubuntu:~$ cat /proc/sys/net/ipv4/ip_forward
0
</code></pre>
<h4 id="修改内核参数">修改内核参数</h4>
<p><strong>临时生效</strong></p>
<pre><code class="language-shell">root@ubuntu:~$ sysctl -w net.ipv4.ip_forward=1
net.ipv4.ip_forward = 1
# 或直接修改 /proc 文件
root@ubuntu:~$ echo 1 &gt; /proc/sys/net/ipv4/ip_forward
</code></pre>
<p><strong>永久生效</strong></p>
<p>修改 <code>sysctl.conf</code> 文件，找到 <code>net.ipv4.ip_forward</code> 配置项，修改为 1：</p>
<pre><code class="language-shell">root@ubuntu:~$ vi /etc/sysctl.conf
# 需要在当前环境中刷新更改
root@ubuntu:~$ sysctl -p /etc/sysctl.conf
</code></pre>
<h3 id="nat">NAT</h3>
<p><strong>网络地址转换</strong> NAT（Network Address Translation）的作用是将数据包中的 network namespace 内网 IP 转为主机所拥有的公网 IP。</p>
<p>NAT 根据数据流向可以分为两种：SNAT 是源 IP 转换，将发送的数据包中的源 IP 转为公网 IP；DNAT 是目标 IP 转换，将接收到的数据包中的公网 IP 转为 network namespace 的内网 IP。</p>
<h3 id="netfilteriptables">netfilter/iptables</h3>
<p>无论是 IP forward 还是 NAT，在 Linux 系统上都可以通过 netfilter/iptables 配置规则。netfilter 和 iptables 可以拆开来说，netfilter 指的是整个<a href="https://www.netfilter.org">项目</a>，在这个项目中 netfilter 特指内核中的 netfilter 框架，而我们更熟悉的 iptables 则是用户空间的配置工具，用于与 netfilter 框架打交道。</p>
<h4 id="netfilter-框架">netfilter 框架</h4>
<p>netfilter 在内核协议栈的 IP 层添加了几个钩子（hooks），允许内核模块在这些钩子的地方注册回调函数，这样经过钩子的所有数据包都会被注册在相应钩子上的函数所处理，包括修改数据包内容或者丢弃数据包等等。</p>
<p>netfilter 框架负责维护钩子上注册的处理函数或者模块，以及它们的优先级。</p>
<h4 id="iptables">iptables</h4>
<p>iptables 是用户空间的一个程序，与内核的 neifilter 框架打交道，根据规则在钩子上配置回调函数。</p>
<p>iptables 用表（table）来分类管理它的规则（rule），根据 rule 的作用可以分类为几个表，比如用于过滤数据的 filter 表，用于处理 NAT 规则的 nat 表等等。</p>
<h4 id="conntrack">conntrack</h4>
<p>onntrack 是 netfilter 实现 NAT 的基础，当加载内核模块 <code>nf_conntrack</code> 后，connection tracking 机制就开始工作，它工作在 <code>NF_IP_PRE_ROUTING</code> 和 <code>NF_IP_LOCAL_OUT</code> 这两个钩子处。它会追踪每个数据包（被 raw 表中的 rule 标记过的除外），并生成 conntrack 条目用于追踪此连接，对于后续通过的数据包，内核会判断若此数据包属于某个连接，则会更新对应的 conntrack 条目。</p>
<p>所有的 conntrack 条目都存放在一张表里，称为连接跟踪表。可以用 <code>cat /proc/net/nf_conntrack</code> 来查看当前的所有连接。下面是所有的连接状态：</p>
<ul>
<li>NEW：当检测到一个不和任何现有连接关联的新包时，如果该包是一个合法的建立连接的数据包，一个新的连接将会被保存，并且标记为状态 NEW。</li>
<li>ESTABLISHED：对于状态是 NEW 的连接，当检测到一个相反方向的包时，连接的状态将会由 NEW 变成 ESTABLISHED，表示连接成功建立。对于TCP连接，意味着收到了一个 SYN/ACK 包， 对于 UDP 和 ICMP，任何反方向的包都可以。</li>
<li>RELATED：数据包不属于任何现有的连接，但它跟现有的状态为 ESTABLISHED 的连接有关系，对于这种数据包，将会创建一个新的连接，且状态被标记为 RELATED。这种连接一般是辅助连接，比如 FTP 的数据传输连接（FTP 有两个连接，另一个是控制连接），或者和某些连接有关的ICMP报文。</li>
<li>INVALID：数据包不和任何现有连接关联，并且不是一个合法的建立连接的数据包，对于这种连接，将会被标记为 INVALID，一般这种都是垃圾数据包，比如收到一个 TCP 的 RST 包，但实际上没有任何相关的 TCP 连接，或者别的地方误发过来的 ICMP 包。</li>
<li>UNTRACKED：被 raw 表里面的 rule 标记为不需要 tracking 的数据包，这种连接将会标记成 UNTRACKED。</li>
</ul>
<h3 id="示例-4">示例</h3>
<p>创建 bridge，并配置 IP：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip link add br0 type bridge
root@ubuntu:~$ ip link set dev br0 up
root@ubuntu:~$ ip addr add 192.168.2.1/24 dev br0
</code></pre>
<p>创建 network namespace 并与 bridge 相连：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns add net0
root@ubuntu:~$ ip link add type veth
root@ubuntu:~$ ip link set veth0 netns net0
root@ubuntu:~$ ip netns exec net0 ip link set dev veth0 up
root@ubuntu:~$ ip link set veth1 up
root@ubuntu:~$ ip link set veth1 master br0
root@ubuntu:~$ ip netns exec net0 ip addr add 192.168.2.11/24 dev veth0
</code></pre>
<p>修改 net0 路由表，默认网关设置为 br0：</p>
<pre><code class="language-shell">root@ubuntu:~$ ip netns exec net0 ip route add 0.0.0.0/0 via 192.168.2.1 dev veth0 onlink
</code></pre>
<p>注意 IP forward 配置：</p>
<pre><code class="language-shell">root@ubuntu:~$ sysctl -w net.ipv4.ip_forward=1
net.ipv4.ip_forward = 1
</code></pre>
<p>屏蔽环境干扰，先默认不允许转发：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -P FORWARD DROP
</code></pre>
<p>开始配置 iptables rules，首先设置 bridge 转发规则，此条规则的意思是允许 br0 转发给 eth0：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -A FORWARD -i br0 -o eth0 -j ACCEPT
</code></pre>
<p>接下来配置 SNAT 规则：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -t nat -A POSTROUTING -s 192.168.2.0/24 -j SNAT --to # to eth0 ip
# 也可以直接配置在 eth0 上
root@ubuntu:~$ # iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
</code></pre>
<p>netfilter 通过 conntrack 来实现 NAT 转换，所以我们要对 <code>RELATED,ESTABLISHED</code> 状态的包予以通行：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -A FORWARD -o br0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
</code></pre>
<p>通过上面的配置，conntrack 状态监测到是回包的数据包，都给予通行，而后回包经过 conntrack 表会变为原始 IP 关系，相当于 DNAT 转换。</p>
<p>在 network namespace 中使用 ping 来测试访问外部网络：</p>
<pre><code class="language-bash">root@ubuntu:~$ ip netns exec net0 ping -c1 110.242.68.4 # 百度的一个 IP
PING 110.242.68.4 (110.242.68.4) 56(84) bytes of data.
64 bytes from 110.242.68.4: icmp_seq=1 ttl=34 time=56.7 ms

--- 110.242.68.4 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
</code></pre>
<h3 id="端口转发">端口转发</h3>
<p>上面的示例是从 network namespace 内部访问外部网络，可以利用 conntrack 来替代 DNAT，如果想让外部请求访问内部服务，就需要配置 DNAT 的映射规则。可映射是一对一的，一个宿主机 IP 对应一个 network namespace 的内网 IP，当我们有多个内部服务想要暴露给公网，就需要配置 NAPT 规则。</p>
<h4 id="napt">NAPT</h4>
<p>网络地址与端口号转换 NAPT (Network Address andPort Translation) 就是使用端口号的 NAT，有端口号的配置，就能实现内网 IP 的多对一映射，只是映射到不同的端口上。</p>
<table>
<thead>
<tr>
<th style="text-align:left">内网 IP</th>
<th style="text-align:left">公网 IP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">192.168.2.11:80</td>
<td style="text-align:left">x.x.x.x:8080</td>
</tr>
<tr>
<td style="text-align:left">192.168.2.1:80</td>
<td style="text-align:left">x.x.x.x:8081</td>
</tr>
</tbody>
</table>
<h4 id="示例-5">示例</h4>
<p>除 iptables rules 外规则不变，首先是在 network namespace 中启动一个 http server：</p>
<pre><code class="language-shell"># 注意：这会暴露当前目录下的文件
root@ubuntu:~$ ip netns exec net0 python -m SimpleHTTPServer 80
</code></pre>
<p>添加 DNAT 规则，设置主机端口为 8080，映射 net0 的 80：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to 192.168.2.11:80
</code></pre>
<p>添加 ip forward：</p>
<pre><code class="language-shell">root@ubuntu:~$ iptables -A FORWARD -i eth0 -d 192.168.2.0/24 -o br0 -p tcp --dport 80 -j ACCEPT
</code></pre>
<p>现在就可以通过宿主机的 IP 访问了。</p>
<h2 id="写在最后">写在最后</h2>
<p>上述例子用于学习需要，与真实的容器配置不同，但所用的基础技术都是一样的。笔记内容主要学习和参考自 Segmentfault 用户 <a href="https://segmentfault.com/u/public0821">public0821</a> 的 Linux 专栏文章，还有网络上的一些相关文章。</p>
<p>接下来会继续学习跨主机的容器网络搭建，这次会结合实际项目 Flannel。</p>
<h3 id="参考文章">参考文章</h3>
<ol>
<li><a href="https://segmentfault.com/a/1190000009251098">Linux虚拟网络设备之veth</a></li>
<li><a href="https://segmentfault.com/a/1190000009491002">Linux虚拟网络设备之bridge(桥)</a></li>
<li><a href="https://segmentfault.com/a/1190000009043962">netfilter/iptables简介</a></li>
<li><a href="http://xstarcd.github.io/wiki/Linux/iptables_forward_internetshare.html">通过iptables实现端口转发和内网共享上网</a></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次 Traefik 无法代理 MySQL 问题]]></title>
        <id>https://cnbailian.github.io/post/traefik-cannot-proxy-mysql/</id>
        <link href="https://cnbailian.github.io/post/traefik-cannot-proxy-mysql/">
        </link>
        <updated>2020-04-13T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>Traefik 从 2.0 版本开始支持 TCP route，我也使用 Traefik 作为 kubernetes 集群的 Ingress，但是在使用过程中，发现 Traefik 为 MySQL 创建的 TCP route 无法正常工作，经过排查搜索后发现了官方人员关于这个疑惑的<a href="https://community.containo.us/t/v2-tcp-router-with-tls-example/2664">解答</a>，以下截取片段：</p>
<blockquote>
<p>But be careful: not all protocols based on TCP and using TLS supports the SNI routing or the passthrough. It requires the protocol supporting SNI (for instance MySQL doesn't) and doing a TLS handshake (if it is a STARTTLS, then it does not work).</p>
</blockquote>
<p>虽然找到了问题是由于 MySQL 不支持，但也勾起了我的好奇心，什么是 SNI？Traefik 为什么要使用 <code>HostSNI</code> 创建 TCP route 呢？为什么 MySQL 不支持 SNI 呢？于是带着这些问题，我开始寻找答案。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Traefik 从 2.0 版本开始支持 TCP route，我也使用 Traefik 作为 kubernetes 集群的 Ingress，但是在使用过程中，发现 Traefik 为 MySQL 创建的 TCP route 无法正常工作，经过排查搜索后发现了官方人员关于这个疑惑的<a href="https://community.containo.us/t/v2-tcp-router-with-tls-example/2664">解答</a>，以下截取片段：</p>
<blockquote>
<p>But be careful: not all protocols based on TCP and using TLS supports the SNI routing or the passthrough. It requires the protocol supporting SNI (for instance MySQL doesn't) and doing a TLS handshake (if it is a STARTTLS, then it does not work).</p>
</blockquote>
<p>虽然找到了问题是由于 MySQL 不支持，但也勾起了我的好奇心，什么是 SNI？Traefik 为什么要使用 <code>HostSNI</code> 创建 TCP route 呢？为什么 MySQL 不支持 SNI 呢？于是带着这些问题，我开始寻找答案。</p>
<!--more-->  
<h2 id="tls-extensions-sni">TLS Extensions —— SNI</h2>
<p>首先从了解 SNI 开始，SNI 是 TLS 的一个扩展协议。</p>
<h3 id="什么是-tls-extensions">什么是 TLS Extensions？</h3>
<p>TLS 扩展于 2003 年以一个独立的规范（<a href="https://tools.ietf.org/html/rfc3546">RFC 3546</a>）被提出，经过不断的发展：<a href="https://tools.ietf.org/html/rfc4366">RFC 4366</a>、<a href="https://tools.ietf.org/html/rfc6066">RFC 6066</a> 等，先后被加入到 TLS1.1、TLS1.2、TLS1.3 中。它能让 Client 和 Server 在不更新 TLS 的基础上，获得新的功能。</p>
<p>Client 在 ClientHello 中声明多个自己可以支持的 Extensions，Server 收到 ClientHello 以后，依次解析 Extensions，有些如果需要立即回应，就在 ServerHello 中作出回应，有些不需要回应，或者 Server 不支持的 Extensions 就不用响应，忽略不处理。</p>
<p>在 ClientHello 中，Extension 字段位于 Compression Methods 字段之后，通过 Wireshark 工具进行查看：</p>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge84vtby39j31nf0u0wt5.jpg" alt="github-wireshark" loading="lazy"></figure>
<h3 id="什么是-sni-扩展">什么是 SNI 扩展？</h3>
<p>我们知道，在 Nginx 中可以通过指定不同的 <code>server_name</code> 来配置多个站点。HTTP/1.1 协议请求头中的 <code>Host</code> 字段可以标识出当前请求属于哪个站点。但是在 TLS 协议中，没有提供一种机制来告诉 Server 它正在建立连接的 Server 的名称，那么对于在同一个地址，并且还使用不同证书的情况下，Server 怎么知道该发送哪个证书？</p>
<p>于是为了解决这个问题，SNI 应运而生。SNI 全称是 Server Name Indication，<a href="https://tools.ietf.org/html/rfc3546#page-8">最初是 2003 年标准化的</a>，在 <a href="https://tools.ietf.org/html/rfc6066#page5">RFC 6066</a> 中有更新。它允许 Server 在同一个网络地址上托管多个启用了 TLS 的服务，要求 Client 在初始 TLS 握手期间指定要连接到哪个服务。</p>
<pre><code class="language-c">struct {
  NameType name_type;
  select (name_type) {
  	case host_name: HostName;
  } name;
} ServerName;

enum {
	host_name(0), (255)
} NameType;

opaque HostName&lt;1..2^16-1&gt;;

struct {
	ServerName server_name_list&lt;1..2^16-1&gt;
} ServerNameList;
</code></pre>
<p>Extension type 是 <code>server_name</code>，点开上图 Wireshark 中 <code>server_name</code> 一行，查看更详细信息：</p>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge85t4pu0uj31n80u019m.jpg" alt="server_name" loading="lazy"></figure>
<p><code>ServerNameList</code> 不能包含多个具有相同 <code>ServerNameType</code> 的名称，当前 <code>ServernameType</code> 只有 <code>host_name</code> 一种，在以后可能会添加更多类型，<code>host_name</code> 包含标准的 DNS hostname 且不含结尾点。如果 Server 支持 SNI 扩展，但不能识别 <code>server_name</code>，则应该发送 <code>fatal-level unrecognized_name(112)</code> 来终止握手或继续握手。</p>
<p><em>更多详细的规范内容可以到 <a href="https://tools.ietf.org/html/rfc6066#page5">RFC 6066</a> 中查看。<a href="https://www.iana.org/assignments/tls-extensiontype-values/tls-extensiontype-values.xhtml">这里</a> 有一个扩展协议列表。</em></p>
<h2 id="traefik-的-tcp-路由与-sni">Traefik 的 TCP 路由与 SNI</h2>
<p>Traefik 从 2.0 开始支持 TCP 路由，也支持在相同的 <code>entryPoints</code>（traefik 中的入口端口） 中定义不同的 TCP 路由，但是我们都知道，TCP 是传输层协议，没有任何 SNI 类的机制来保证同一地址入口可以处理不同的服务。那么，Traefik 是怎么做的呢？</p>
<h3 id="部署基于-tls-的-tcp-路由">部署基于 TLS 的 TCP 路由</h3>
<p>答案很简单，Traefik 支持通过 SNI 在每台主机上进行路由，因为这是通过 TCP 进行路由的惟一标准方法，但是 TCP 本身没有 SNI，因此必须使用 TLS。部署配置：</p>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: IngressRouteTCP
metadata:
  name: example
spec:
  entryPoints:
    - web
  routes:
  - match: HostSNI(`web.example.com`)
    services:
    - name: example-service-name
      port: 80
  tls: 
    secretName: traefik-tls-certs
</code></pre>
<p><code>HostSNI</code> 中的值对应 SNI 扩展中 <code>server_name</code> 的值，Traefik 以此来进行路由，并找到对应证书。还需要注意的是 <code>entryPoints</code> 部分由部署的 Traefik 配置中的 <code>entryPoints</code> 参数决定，此处的 <code>web</code> 是我们指定的一个 <code>entryPoints</code> 名称，端口地址对应为 80 端口：</p>
<pre><code class="language-yaml">......
- image: traefik:2.1.1
  name: traefik
  ports:
  - name: web
    containerPort: 80
    hostPort: 80
  args:
  - --entryPoints.web.address=:80
......
</code></pre>
<p>此处使用 <code>hostPort</code> 的方式暴露入口点，是为了能够通过 Traefik 部署的节点的入口点端口直接访问到 backend service。</p>
<h3 id="部署非-tls-的-tcp-路由">部署非 TLS 的 TCP 路由</h3>
<p>如果有不支持 SNI/TLS 协议的应用客户端，Traefik 也可以部署 “plain TCP”，也就是标准的通过端口进行路由。此时虽然 <code>metch</code> 还是使用 <code>HostSNI</code>，但需要指定为通配符 <code>*</code>：</p>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: IngressRouteTCP
metadata:
  name: example
spec:
  entryPoints:
    - web
  routes:
  - match: HostSNI(`*`)
    services:
    - name: example-service-name
      port: 80
</code></pre>
<h3 id="其他">其他</h3>
<p>使用 Traefik 代理 TLS 服务时，backend service 可不设置 TLS 相关，由 Traefik 负责全部相关机制。如果 backend service 有需要加密后的数据时，可通过 <code>passthrough</code> 参数配置，Traefik 将发送加密后的数据给 backend service：</p>
<pre><code class="language-yaml">......
  tls: 
    secretName: traefik-tls-certs
    passthrough: true
</code></pre>
<h2 id="为什么不能为-mysql-代理">为什么不能为 MySQL 代理</h2>
<p>当我明白 SNI 协议以及 Traefik 如何使用 SNI/TLS 为 TCP 创建路由时，我开始研究为什么 MySQL 不能使用 SNI 扩展，甚至在 2016 年就有人提出过这个问题，但可惜一直没有人跟进：https://bugs.mysql.com/bug.php?id=82872。这让我有些疑惑，毕竟 MySQL 已经实现了 TLS 功能，为什么在有用户有需求的情况下不加上 SNI 扩展呢？毕竟这又不是过于复杂的功能。</p>
<p>在寻找到答案之前，让我们先简单复习下 TLS 协议的标准流程：首先是 TCP 的三次握手，随后开始 TLS 的握手，如果是 TLS1.2 或之前需要四次握手，如果是 TLS1.3 则需要三次握手，最后开始传输加密数据。</p>
<p>下面来看看 MySQL 的流程，输入命令：<code>mysql -hmysql.example.com -P3306 -uroot -pmysql --ssl-mode=REQUIRED</code>，使用 wireshark 查看：</p>
<p><em>MySQL 对于 TCP 连接已经默认使用 tls，如果不想使用需要修改参数为 <code>--ssl-mode=DISABLED</code>，同时对于 localhost 默认使用 soket 连接，强制使用 TCP 连接需要增加参数: <code>--protocol tcp</code>。</em></p>
<figure data-type="image" tabindex="3"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge97ghpl3ej31ja0u07t9.jpg" alt="mysql" loading="lazy"></figure>
<p>上图中可以看出，在 TCP 握手后，Server 会发送 MySQL 协议 HandShake Paket：<code>Server Greeting proto=10 version=5.7.29</code>，开始 MySQL 协议的握手流程，随后 Client 发送 Auth Paket，图中为开启 TLS 认证的流程，所以并未显示 <code>user</code> 的内容，如果设置 MySQL Client 参数为 <code>--ssl-mode=DISABLED</code>，将显示认证的用户名，并且 Server 会在随后发送 <code>Auth Switch Request</code> 包继续认证流程，此处不再赘述，有兴趣的可以自己抓包看一下。</p>
<p>看到这里其实就已经很清晰了，MySQL 在连接时会将自定义协议握手流程置于 TLS 协议握手之前，以至于 Traefik 无法通过 TLS SNI 找到对应 backend service，也就无法发送 MySQL 的 HandShake Paket。对于 MySQL Client 来说，如果是有超时机制，会响应 <code>waiting for initial communication packet</code> 或类似的错误，如果没有超时机制，就会一直等待。</p>
<p>这点对于 Traefik 来说也很无奈，MySQL 自定义协议中也没有 SNI 的机制，而 TLS 又在 MySQL 协议握手之后发生，导致它完全没办法进行路由，只好期望 MySQL 能尽快修改这部分的流程。<a href="https://github.com/containous/traefik/issues/5155">这里</a>有官方对于这件事的一些回复：https://github.com/containous/traefik/issues/5155</p>
<h2 id="其他常见数据库">其他常见数据库</h2>
<p>了解到了 MySQL 的问题，不禁让我好奇，其他的常见数据库是否也拥有相同问题，于是我又去看了 MongoDB 和 Redis。</p>
<h3 id="mongodb">MongoDB</h3>
<p>使用命令进行连接：<code>mongo --host mongo.example.com --port 27017 --ssl</code></p>
<figure data-type="image" tabindex="4"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge98i03x3ej31pq0u0wsp.jpg" alt="mongodb" loading="lazy"></figure>
<p>非常标准的流程，也支持 SNI 扩展，Traefik 可以顺利的进行路由。</p>
<h3 id="redis">Redis</h3>
<p>Redis 从 6.0 开始支持 SSL/TLS，但 6.0 正在处于 RC（Release　Candidate） 阶段，如果想要测试，可以下载代码后自行编译。TLS 特性是个可选特性，需要在编译时使用参数确认使用：<code>make BUILD_TLS=yes</code>。</p>
<p><em>相关官方文档：https://redis.io/topics/encryption</em></p>
<p>编译后尝试连接 Traefik 代理的地址：<code>./redis-cli --tls -h testtcp.ohuna.cloud -p 6379</code>，却发现 Traefik 响应了 fatal level error： <code>Unknown CA</code>：</p>
<figure data-type="image" tabindex="5"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge99xitc2zj31j50u0qlj.jpg" alt="redis" loading="lazy"></figure>
<p>很明显是因为 redis 没有使用 SNI 扩展，但文档中又没有提及，所以我去 redis 源码中寻找答案。在 <code>tls.h</code> 中了解到 redis 使用了 openssl：</p>
<pre><code class="language-c">......
#ifdef USE_OPENSSL

#include &lt;openssl/ssl.h&gt;
#include &lt;openssl/err.h&gt;
#include &lt;openssl/rand.h&gt;
</code></pre>
<p>于是通过 openssl 设置 SNI 的函数 <code>SSL_set_tlsext_host_name</code> 进行查找：</p>
<pre><code class="language-c">#redis-cli.c
    if (config.sni &amp;&amp; !SSL_set_tlsext_host_name(ssl, config.sni)) {
        *err = &quot;Failed to configure SNI&quot;;
        SSL_free(ssl);
        return REDIS_ERR;
    }
......
  #ifdef USE_OPENSSL
        } else if (!strcmp(argv[i],&quot;--tls&quot;)) {
            config.tls = 1;
        } else if (!strcmp(argv[i],&quot;--sni&quot;) &amp;&amp; !lastarg) {
            config.sni = argv[++i];
......
</code></pre>
<p>发现可以通过 <code>--sni</code> 参数进行指定，通过 <code>redis-cli --help</code> 能查看到相关说明：</p>
<pre><code class="language-bash">redis-cli 5.9.103

Usage: redis-cli [OPTIONS] [cmd [arg [arg ...]]]
......
	--tls              Establish a secure TLS connection.
  --sni &lt;host&gt;       Server name indication for TLS.
</code></pre>
<p>由于粗心大意，导致耽误了时间去寻找 SNI 的设置方法，不过 redis 需要必须手动设置 SNI 的方式也是很奇怪。重新使用带有 <code>--sni</code> 参数的命令进行连接：<code>./redis-cli --tls -h redis.example.com -p 6379 --sni redis.example.com</code>，这次成功连接，查看 TLS ClientHello 中也带有 <code>server_name</code>：</p>
<figure data-type="image" tabindex="6"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ge9amao5j7j31ji0u01a4.jpg" alt="redis-success" loading="lazy"></figure>
<h2 id="扩展阅读esni">扩展阅读——ESNI</h2>
<p>虽然关于 Traefik 与 MySQL 的问题告一段落，但 SNI 本身还有其他可学习的内容。</p>
<h4 id="sni-的安全问题">SNI 的安全问题</h4>
<p>由于 SNI 扩展是在 TLS 握手期间通过 ClientHello 进行发送，在此时 Client 和 Server 还未共享加密密钥，因此 ClientHello 消息未被加密发送。这就意味着如果有中间人，是可以拦截明文的 ClientHello 消息，并知道 Client 将要访问的网址。</p>
<h4 id="esni">ESNI</h4>
<p>当前有一项草案正在试图解决这个问题，也就是 <a href="https://tools.ietf.org/html/draft-rescorla-tls-esni-00">ESNI（Encrypted Server Name Indication）</a>。</p>
<p>对于加密 SNI 内容这种先有鸡还是先有蛋的问题，ESNI 通过引入 DNS 来解决。服务器在已知的 DNS 记录上发布一个公钥，客户端可以在连接 Server 之前获得该公钥。然后，客户端将 ClientHello 中的 SNI 扩展替换为 ESNI，也就是使用获得的公钥对 SNI 信息对称加密。</p>
<p>ESNI 必须要基于 TLS1.3 版本，因为 TLS1.3 使用了 Deffie-Hellman 算法进行密钥交换，DH 算法可以使通信的双方能在非安全的信道中安全的交换密钥。否则，就算加密了 SNI，也可以通过明文证书进行验证。</p>
<p>如果仅仅使用 DNS 也不行，因为 DNS 默认是为加密的，所以需要使用的 DNS 支持 DNS over TLS（DoT）或 DNS over HTTPS（DoH）特性。</p>
<p><em>简单的学习下 ESNI，更多详细内容可以通过 Cloudflare 的<a href="https://blog.cloudflare.com/zh/encrypted-sni-zh/">文章</a>或<a href="https://tools.ietf.org/html/draft-rescorla-tls-esni-00">草案</a>进行了解。</em></p>
<h2 id="参考和致谢">参考和致谢</h2>
<p>学习过程中碰到了诸多问题，幸好互联网上有着众多的学习资料，感谢以下文档与博客：</p>
<p><a href="https://www.qikqiak.com/post/traefik-2.1-101/">一文搞懂 Traefik2.1 的使用</a></p>
<p><a href="https://harttle.land/2018/03/25/https-protocols.html">HTTPS 交互过程分析</a></p>
<p><a href="https://imququ.com/post/sth-about-switch-to-https-2.html">关于启用 HTTPS 的一些经验分享（二）</a></p>
<p><a href="https://halfrost.com/https-extensions/">HTTPS 温故知新（六） —— TLS 中的 Extensions</a></p>
<p><a href="https://tools.ietf.org/html/rfc6066">RFC 6066</a></p>
<p><a href="%5Bhttps://www.callmejiagu.com/2018/10/26/WireShark-%E5%88%86%E6%9E%90MySQL%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8C%85%EF%BC%88%E4%BA%8C%EF%BC%89/%5D(https://www.callmejiagu.com/2018/10/26/WireShark-%E5%88%86%E6%9E%90MySQL%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8C%85%EF%BC%88%E4%BA%8C%EF%BC%89/)">实现自己的数据库驱动——WireShark分析MySQL网络协议中的数据包（二）</a></p>
<p><a href="https://blog.cloudflare.com/zh/encrypted-sni-zh/">不加密，无隐私：加密SNI工作原理</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Cluster Autoscaler]]></title>
        <id>https://cnbailian.github.io/post/kubernetes-cluster-autoscaler/</id>
        <link href="https://cnbailian.github.io/post/kubernetes-cluster-autoscaler/">
        </link>
        <updated>2020-03-31T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>当我们使用 Kubernetes 部署应用后，会发现如果用户增长速度超过预期，以至于计算资源不够时，你会怎么做呢？Kubernetes 给出的解决方案就是：自动伸缩（auto-scaling），通过自动伸缩组件之间的配合，可以 7*24 小时的监控着你的集群，动态变化负载，以适应你的用户需求。</p>
]]></summary>
        <content type="html"><![CDATA[<p>当我们使用 Kubernetes 部署应用后，会发现如果用户增长速度超过预期，以至于计算资源不够时，你会怎么做呢？Kubernetes 给出的解决方案就是：自动伸缩（auto-scaling），通过自动伸缩组件之间的配合，可以 7*24 小时的监控着你的集群，动态变化负载，以适应你的用户需求。</p>
<!--more-->	
<h2 id="自动伸缩组件">自动伸缩组件</h2>
<p><strong>水平自动伸缩（Horizontal Pod Autoscaler，HPA）</strong></p>
<p>HPA 可以基于实时的 CPU 利用率自动伸缩 Replication Controller、Deployment 和 Replica Set 中的 Pod 数量。也可以通过搭配 Metrics Server 基于其他的度量指标。</p>
<p><strong>垂直自动伸缩（Vertical Pod Autoscaler，VPA）</strong></p>
<p>VPA 可以基于 Pod 的使用资源来自动设置 Pod 所需资源并且能够在运行时自动调整资源。</p>
<p><strong>集群自动伸缩（Cluster Autoscaler，CA）</strong></p>
<p>CA 是一个可以自动伸缩集群 Node 的组件。如果集群中有未被调度的 Pod，它将会自动扩展 Node 来使 Pod 可用，或是在发现集群中的 Node 资源使用率过低时，删除 Node 来节约资源。</p>
<p><strong>插件伸缩（Addon Resizer）</strong></p>
<p>这是一个小插件，它以 Sidecar 的形式来垂直伸缩与自己同一个部署中的另一个容器，目前唯一的策略就是根据集群中节点的数量来进行线性扩展。通常与 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/metrics-server/metrics-server-deployment.yaml#L66">Metrics Server</a> 配合使用，以保证其可以负担不断扩大的整个集群的 metrics API 服务。</p>
<p>通过 HPA 伸缩无状态应用，VPA 伸缩有状态应用，CA 保证计算资源，它们的配合使用，构成了一个完整的自动伸缩解决方案。</p>
<h2 id="cluster-autoscaler-详细介绍">Cluster Autoscaler 详细介绍</h2>
<p>上面介绍的四个组件中，HPA 是在 kubernetes 代码仓库中的，随着 kubernetes 的版本进行更新发布，不需要部署，可以直接使用。其他的三个组件都在官方社区维护的<a href="https://github.com/kubernetes/autoscaler">仓库</a>中，Cluster Autoscaler 的 v1.0(GA) 版本已经随着 kubernetes 1.8 一起发布，剩下两个则还是 beta 版本。</p>
<h3 id="部署">部署</h3>
<p>Cluster Autoscaler 通常需要搭配云厂商使用，它提供了 <code>Cloud Provider</code> 接口供各个云厂商接入，云厂商通过伸缩组（Scaling Group）或节点池（Node Pool）的功能对 ECS 类产品节点进行增加删除等操作。</p>
<p>目前（v1.18.1）已接入的云厂商：</p>
<p>**Alicloud：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/alicloud/README.md</p>
<p>**Aws：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</p>
<p>**Azure：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md</p>
<p>**Baiducloud：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/baiducloud/README.md</p>
<p>**Digitalocean：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/digitalocean/README.md</p>
<p>**GoogleCloud GCE：**https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#upgrading-google-compute-engine-clusters</p>
<p>**GoogleCloud GKE：**https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler</p>
<p>**OpenStack Magnum：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/magnum/README.md</p>
<p>**Packet：**https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/packet/README.md</p>
<p>启动参数列表：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca</p>
<h3 id="工作原理">工作原理</h3>
<p>Cluster Autoscaler 抽象出了一个 <code>NodeGroup</code> 的概念，与之对应的是云厂商的伸缩组服务。Cluster Autoscaler 通过 <code>CloudProvider</code> 提供的 <code>NodeGroup</code> 计算集群内节点资源，以此来进行伸缩。</p>
<p>在启动后，Cluster Autoscaler 会定期（默认 10s）检查未调度的 Pod 和 Node 的资源使用情况，并进行相应的 <code>Scale UP</code> 和 <code>Scale Down</code> 操作。</p>
<h4 id="scale-up">Scale UP</h4>
<p>当 Cluster Autoscaler 发现有 Pod 由于资源不足而无法调度时，就会通过调用 <code>Scale UP</code> 执行扩容操作。</p>
<p>在 <code>Scale UP</code> 中会只会计算在 <code>NodeGroup</code> 中存在的 Node，我们可以将 Worker Node 统一交由伸缩组进行管理。并且由于伸缩组非同步加入的特性，也会考虑到 Upcoming Node。</p>
<p>为了业务需要，集群中可能会有不同规格的 Node，我们可以创建多个 <code>NodeGroup</code>，在扩容时会根据 <code>--expander</code> 选项配置指定的策略，选择一个扩容的节点组，支持如下<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders">五种策略</a>：</p>
<ul>
<li>**random：**随机选择一个 <code>NodeGroup</code>。如果未指定，则默认为此策略。</li>
<li>**most-pods：**选择能够调度最多 Pod 的 <code>NodeGroup</code>，比如有的 Pod 未调度是因为 <code>nodeSelector</code>，此策略会优先选择能满足的 <code>NodeGroup</code> 来保证大多数的 Pod 可以被调度。</li>
<li>**least-waste：**为避免浪费，此策略会优先选择能满足 Pod 需求资源的最小资源类型的 <code>NodeGroup</code>。</li>
<li>**price：**根据 <code>CloudProvider</code> 提供的价格模型，选择最省钱的 <code>NodeGroup</code>。</li>
<li>**priority：**通过配置优先级来进行选择，用起来比较麻烦，需要额外的配置，可以看<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md">文档</a>。</li>
</ul>
<p>如果有需要，也可以平衡相似 <code>NodeGroup</code> 中的	Node 数量，避免 <code>NodeGroup</code> 达到 <code>MaxSize</code> 而导致无法加入新 Node。通过 <code>--balance-similar-node-groups</code> 选项配置，默认为 <code>false</code>。</p>
<p>再经过一系列的操作后，最终计算出要扩容的 Node 数量及 <code>NodeGroup</code>，使用 <code>CloudProvider</code> 执行 <code>IncreaseSize</code> 操作，增加云厂商的伸缩组大小，从而完成扩容操作。</p>
<p><em>文字表达能力不足，如果有不清晰的地方，可以参考下面的 <a href="#ScaleUP%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90">ScaleUP 源码解析</a>。</em></p>
<h4 id="scale-down">Scale Down</h4>
<p>缩容是一个可选的功能，通过 <code>--scale-down-enabled</code> 选项配置，默认为 <code>true</code>。</p>
<p>在 Cluster Autoscaler 监控 Node 资源时，如果发现有 Node 满足以下三个条件时，就会标记这个 Node 为 <code>unneeded</code>：</p>
<ul>
<li>Node 上运行的所有的 Pod 的 Cpu 和内存之和小于该 Node 可分配容量的 50%。可通过 <code>--scale-down-utilization-threshold</code> 选项改变这个配置。</li>
<li>Node 上所有的 Pod 都可以被调度到其他节点。</li>
<li>Node 没有表示不可缩容的 annotaition。</li>
</ul>
<p>如果一个 Node 被标记为 <code>unneeded</code> 超过 10 分钟（可通过 <code>--scale-down-unneeded-time</code> 选项配置），则使用 <code>CloudProvider</code> 执行 <code>DeleteNodes</code> 操作将其删除。一次最多删除一个 <code>unneeded Node</code>，但空 Node 可以批量删除，每次最多删除 10 个（通过 <code>----max-empty-bulk-delete</code> 选项配置）。</p>
<p>实际上并不是只有这一个判定条件，还会有其他的条件来阻止删除这个 Node，比如 <code>NodeGroup</code> 已达到 <code>MinSize</code>，或在过去的 10 分钟内有过一次 <code>Scale UP</code> 操作（通过 <code>--scale-down-delay-after-add</code> 选项配置）等等，更详细可查看<a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work">文档</a>。</p>
<p>Cluster Autoscaler 的工作机制很复杂，但其中大部分都能通过 flags 进行配置，如果有需要，请详细阅读文档：https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md</p>
<h2 id="如何实现-cloudprovider">如何实现 CloudProvider</h2>
<p>如果使用上述中已实现接入的云厂商，只需要通过 <code>--cloud-provider</code> 选项指定来自哪个云厂商就可以，如果想要对接自己的 IaaS 或有特定的业务逻辑，就需要自己实现 <code>CloudProvider Interface</code> 与 <code>NodeGroupInterface</code>。并将其注册到 <code>builder</code> 中，用于通过 <code>--cloud-provider</code> 参数指定。</p>
<p><code>builder</code> 在 <code>cloudprovider/builder</code> 中的 <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/builder/builder_all.go">builder_all.go</a> 中注册，也可以在其中新建一个自己的 <code>build</code>，通过 go 文件的 <code>+build</code> 编译参数来指定使用的 <code>CloudProvider</code>。</p>
<p><code>CloudProvider</code> 接口与 <code>NodeGroup</code> 接口在 <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/cloud_provider.go">cloud_provider.go</a> 中定义，其中需要注意的是 <code>Refresh</code> 方法，它会在每一次循环（默认 10 秒）的开始时调用，可在此时请求接口并刷新 <code>NodeGroup</code> 状态，通常的做法是增加一个 <code>manager</code> 用于管理状态。有不理解的部分可参考其他 <code>CloudProvider</code> 的实现。</p>
<pre><code>type CloudProvider interface {
	// Name returns name of the cloud provider.
	Name() string

	// NodeGroups returns all node groups configured for this cloud provider.
	// 会在一此循环中多次调用此方法，所以不适合每次都请求云厂商服务，可以在 Refresh 时存储状态
	NodeGroups() []NodeGroup

	// NodeGroupForNode returns the node group for the given node, nil if the node
	// should not be processed by cluster autoscaler, or non-nil error if such
	// occurred. Must be implemented.
	// 同上
	NodeGroupForNode(*apiv1.Node) (NodeGroup, error)

	// Pricing returns pricing model for this cloud provider or error if not available.
	// Implementation optional.
	// 如果不使用 price expander 就可以不实现此方法
	Pricing() (PricingModel, errors.AutoscalerError)

	// GetAvailableMachineTypes get all machine types that can be requested from the cloud provider.
	// Implementation optional.
	// 没用，不需要实现
	GetAvailableMachineTypes() ([]string, error)

	// NewNodeGroup builds a theoretical node group based on the node definition provided. The node group is not automatically
	// created on the cloud provider side. The node group is not returned by NodeGroups() until it is created.
	// Implementation optional.
	// 通常情况下，不需要实现此方法，但如果你需要 ClusterAutoscaler 创建一个默认的 NodeGroup 的话，也可以实现。
	// 但其实更好的做法是将默认 NodeGroup 写入云端的伸缩组
	NewNodeGroup(machineType string, labels map[string]string, systemLabels map[string]string,
		taints []apiv1.Taint, extraResources map[string]resource.Quantity) (NodeGroup, error)

	// GetResourceLimiter returns struct containing limits (max, min) for resources (cores, memory etc.).
	// 资源限制对象，会在 build 时传入，通常情况下不需要更改，除非在云端有显示的提示用户更改的地方，否则使用时会迷惑用户
	GetResourceLimiter() (*ResourceLimiter, error)

	// GPULabel returns the label added to nodes with GPU resource.
	// GPU 相关，如果集群中有使用 GPU 资源，需要返回对应内容。 hack: we assume anything which is not cpu/memory to be a gpu.
	GPULabel() string

	// GetAvailableGPUTypes return all available GPU types cloud provider supports.
	// 同上
	GetAvailableGPUTypes() map[string]struct{}

	// Cleanup cleans up open resources before the cloud provider is destroyed, i.e. go routines etc.
	// CloudProvider 只会在启动时被初始化一次，如果每次循环后有需要清除的内容，在这里处理
	Cleanup() error

	// Refresh is called before every main loop and can be used to dynamically update cloud provider state.
	// In particular the list of node groups returned by NodeGroups can change as a result of CloudProvider.Refresh().
	// 会在 StaticAutoscaler RunOnce 中被调用
	Refresh() error
}
// NodeGroup contains configuration info and functions to control a set
// of nodes that have the same capacity and set of labels.
type NodeGroup interface {
	// MaxSize returns maximum size of the node group.
	MaxSize() int

	// MinSize returns minimum size of the node group.
	MinSize() int

	// TargetSize returns the current target size of the node group. It is possible that the
	// number of nodes in Kubernetes is different at the moment but should be equal
	// to Size() once everything stabilizes (new nodes finish startup and registration or
	// removed nodes are deleted completely). Implementation required.
	// 响应的是伸缩组的节点数，并不一定与 kubernetes 中的节点数保持一致
	TargetSize() (int, error)

	// IncreaseSize increases the size of the node group. To delete a node you need
	// to explicitly name it and use DeleteNode. This function should wait until
	// node group size is updated. Implementation required.
	// 扩容的方法，增加伸缩组的节点数
	IncreaseSize(delta int) error

	// DeleteNodes deletes nodes from this node group. Error is returned either on
	// failure or if the given node doesn't belong to this node group. This function
	// should wait until node group size is updated. Implementation required.
	// 删除的节点一定要在该节点组中
	DeleteNodes([]*apiv1.Node) error

	// DecreaseTargetSize decreases the target size of the node group. This function
	// doesn't permit to delete any existing node and can be used only to reduce the
	// request for new nodes that have not been yet fulfilled. Delta should be negative.
	// It is assumed that cloud provider will not delete the existing nodes when there
	// is an option to just decrease the target. Implementation required.
	// 当 ClusterAutoscaler 发现 kubernetes 节点数与伸缩组的节点数长时间不一致，会调用此方法来调整
	DecreaseTargetSize(delta int) error

	// Id returns an unique identifier of the node group.
	Id() string

	// Debug returns a string containing all information regarding this node group.
	Debug() string

	// Nodes returns a list of all nodes that belong to this node group.
	// It is required that Instance objects returned by this method have Id field set.
	// Other fields are optional.
	// This list should include also instances that might have not become a kubernetes node yet.
	// 返回伸缩组中的所有节点，哪怕它还没有成为 kubernetes 的节点
	Nodes() ([]Instance, error)

	// TemplateNodeInfo returns a schedulernodeinfo.NodeInfo structure of an empty
	// (as if just started) node. This will be used in scale-up simulations to
	// predict what would a new node look like if a node group was expanded. The returned
	// NodeInfo is expected to have a fully populated Node object, with all of the labels,
	// capacity and allocatable information as well as all pods that are started on
	// the node by default, using manifest (most likely only kube-proxy). Implementation optional.
	// ClusterAutoscaler 会将节点信息与节点组对应，来判断资源条件，如果是一个空的节点组，那么就会通过此方法来虚拟一个节点信息。
	TemplateNodeInfo() (*schedulernodeinfo.NodeInfo, error)

	// Exist checks if the node group really exists on the cloud provider side. Allows to tell the
	// theoretical node group from the real one. Implementation required.
	Exist() bool

	// Create creates the node group on the cloud provider side. Implementation optional.
	// 与 CloudProvider.NewNodeGroup 配合使用
	Create() (NodeGroup, error)

	// Delete deletes the node group on the cloud provider side.
	// This will be executed only for autoprovisioned node groups, once their size drops to 0.
	// Implementation optional.
	Delete() error

	// Autoprovisioned returns true if the node group is autoprovisioned. An autoprovisioned group
	// was created by CA and can be deleted when scaled to 0.
	Autoprovisioned() bool
}
</code></pre>
<h2 id="scaleup-源码解析">ScaleUP 源码解析</h2>
<pre><code>func ScaleUp(context *context.AutoscalingContext, processors *ca_processors.AutoscalingProcessors, clusterStateRegistry *clusterstate.ClusterStateRegistry, unschedulablePods []*apiv1.Pod, nodes []*apiv1.Node, daemonSets []*appsv1.DaemonSet, nodeInfos map[string]*schedulernodeinfo.NodeInfo, ignoredTaints taints.TaintKeySet) (*status.ScaleUpStatus, errors.AutoscalerError) {
	
	......
	// 验证当前集群中所有 ready node 是否来自于 nodeGroups，取得所有非组内的 node
	nodesFromNotAutoscaledGroups, err := utils.FilterOutNodesFromNotAutoscaledGroups(nodes, context.CloudProvider)
	if err != nil {
		return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, err.AddPrefix(&quot;failed to filter out nodes which are from not autoscaled groups: &quot;)
	}

	nodeGroups := context.CloudProvider.NodeGroups()
	gpuLabel := context.CloudProvider.GPULabel()
	availableGPUTypes := context.CloudProvider.GetAvailableGPUTypes()

	// 资源限制对象，会在 build cloud provider 时传入
	// 如果有需要可在 CloudProvider 中自行更改，但不建议改动，会对用户造成迷惑
	resourceLimiter, errCP := context.CloudProvider.GetResourceLimiter()
	if errCP != nil {
		return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errors.ToAutoscalerError(
			errors.CloudProviderError,
			errCP)
	}

	// 计算资源限制
	// nodeInfos 是所有拥有节点组的节点与示例节点的映射
	// 示例节点会优先考虑真实节点的数据，如果 NodeGroup 中还没有真实节点的部署，则使用 Template 的节点数据
	scaleUpResourcesLeft, errLimits := computeScaleUpResourcesLeftLimits(context.CloudProvider, nodeGroups, nodeInfos, nodesFromNotAutoscaledGroups, resourceLimiter)
	if errLimits != nil {
		return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errLimits.AddPrefix(&quot;Could not compute total resources: &quot;)
	}

	// 根据当前节点与 NodeGroups 中的节点来计算会有多少节点即将加入集群中
	// 由于云服务商的伸缩组 increase size 操作并不是同步加入 node，所以将其统计，以便于后面计算节点资源
	upcomingNodes := make([]*schedulernodeinfo.NodeInfo, 0)
	for nodeGroup, numberOfNodes := range clusterStateRegistry.GetUpcomingNodes() {
		......
	}
	klog.V(4).Infof(&quot;Upcoming %d nodes&quot;, len(upcomingNodes))

	// 最终会进入选择的节点组
	expansionOptions := make(map[string]expander.Option, 0)
	......
	// 出于某些限制或错误导致不能加入新节点的节点组，例如节点组已达到 MaxSize
	skippedNodeGroups := map[string]status.Reasons{}
	// 综合各种情况，筛选出节点组
	for _, nodeGroup := range nodeGroups {
	......
	}
	if len(expansionOptions) == 0 {
		klog.V(1).Info(&quot;No expansion options&quot;)
		return &amp;status.ScaleUpStatus{
			Result:					status.ScaleUpNoOptionsAvailable,
			PodsRemainUnschedulable: getRemainingPods(podEquivalenceGroups, skippedNodeGroups),
			ConsideredNodeGroups:	nodeGroups,
		}, nil
	}

	......
	// 选择一个最佳的节点组进行扩容，expander 用于选择一个合适的节点组进行扩容，默认为 RandomExpander，flag: expander
	// random 随机选一个，适合只有一个节点组
	// most-pods 选择能够调度最多 pod 的节点组，比如有 noSchedulerPods 是有 nodeSelector 的，它会优先选择此类节点组以满足大多数 pod 的需求
	// least-waste 优先选择能满足 pod 需求资源的最小资源类型的节点组
	// price 根据价格模型，选择最省钱的
	// priority 根据优先级选择
	bestOption := context.ExpanderStrategy.BestOption(options, nodeInfos)
	if bestOption != nil &amp;&amp; bestOption.NodeCount &gt; 0 {
	......
		newNodes := bestOption.NodeCount

		// 考虑到 upcomingNodes, 重新计算本次新加入节点
		if context.MaxNodesTotal &gt; 0 &amp;&amp; len(nodes)+newNodes+len(upcomingNodes) &gt; context.MaxNodesTotal {
			klog.V(1).Infof(&quot;Capping size to max cluster total size (%d)&quot;, context.MaxNodesTotal)
			newNodes = context.MaxNodesTotal - len(nodes) - len(upcomingNodes)
			if newNodes &lt; 1 {
				return &amp;status.ScaleUpStatus{Result: status.ScaleUpError}, errors.NewAutoscalerError(
					errors.TransientError,
					&quot;max node total count already reached&quot;)
			}
		}

		createNodeGroupResults := make([]nodegroups.CreateNodeGroupResult, 0)
	
		// 如果节点组在云服务商端处不存在，会尝试创建根据现有信息重新创建一个云端节点组
		// 但是目前所有的 CloudProvider 实现都没有允许这种操作，这好像是个多余的方法
		// 云服务商不想，也不应该将云端节点组的创建权限交给 ClusterAutoscaler
		if !bestOption.NodeGroup.Exist() {
			oldId := bestOption.NodeGroup.Id()
			createNodeGroupResult, err := processors.NodeGroupManager.CreateNodeGroup(context, bestOption.NodeGroup)
		......
		}

		// 得到最佳节点组的示例节点
		nodeInfo, found := nodeInfos[bestOption.NodeGroup.Id()]
		if !found {
			// This should never happen, as we already should have retrieved
			// nodeInfo for any considered nodegroup.
			klog.Errorf(&quot;No node info for: %s&quot;, bestOption.NodeGroup.Id())
			return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, errors.NewAutoscalerError(
				errors.CloudProviderError,
				&quot;No node info for best expansion option!&quot;)
		}

		// 根据 CPU、Memory及可能存在的 GPU 资源（hack: we assume anything which is not cpu/memory to be a gpu.），计算出需要多少个 Nodes
		newNodes, err = applyScaleUpResourcesLimits(context.CloudProvider, newNodes, scaleUpResourcesLeft, nodeInfo, bestOption.NodeGroup, resourceLimiter)
		if err != nil {
			return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, err
		}

		// 需要平衡的节点组
		targetNodeGroups := []cloudprovider.NodeGroup{bestOption.NodeGroup}
		// 如果需要平衡节点组，根据 balance-similar-node-groups flag 设置。
		// 检测相似的节点组，并平衡它们之间的节点数量
		if context.BalanceSimilarNodeGroups {
		......
		}
		// 具体平衡策略可以看 (b *BalancingNodeGroupSetProcessor) BalanceScaleUpBetweenGroups 方法
		scaleUpInfos, typedErr := processors.NodeGroupSetProcessor.BalanceScaleUpBetweenGroups(context, targetNodeGroups, newNodes)
		if typedErr != nil {
			return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, typedErr
		}
		klog.V(1).Infof(&quot;Final scale-up plan: %v&quot;, scaleUpInfos)
		// 开始扩容，通过 IncreaseSize 扩容
		for _, info := range scaleUpInfos {
			typedErr := executeScaleUp(context, clusterStateRegistry, info, gpu.GetGpuTypeForMetrics(gpuLabel, availableGPUTypes, nodeInfo.Node(), nil), now)
			if typedErr != nil {
				return &amp;status.ScaleUpStatus{Result: status.ScaleUpError, CreateNodeGroupResults: createNodeGroupResults}, typedErr
			}
		}
		......
	}
	......
}


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes 的 Dynamic Provisioning 实现]]></title>
        <id>https://cnbailian.github.io/post/dynamic-provisioning-of-kubernetes/</id>
        <link href="https://cnbailian.github.io/post/dynamic-provisioning-of-kubernetes/">
        </link>
        <updated>2020-03-11T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>存储一直是容器运行的关键部分，Kubernetes 为此做了很多努力，从一开始的 Pod Volumes、PV(Persistent Volumes) 与 PVC(Persistent Volume Claim)，到 StorageClass 与 Dynamic Provisioning，再到现在 “out-of-tree” 的 CSI(Container Storage Interface)，Kubernetes 社区一直在演进存储的实现。</p>
<p>前面基础的就不讲了，我们从 StorageClass 与 Dynamic Provisioning 开始了解。</p>
]]></summary>
        <content type="html"><![CDATA[<p>存储一直是容器运行的关键部分，Kubernetes 为此做了很多努力，从一开始的 Pod Volumes、PV(Persistent Volumes) 与 PVC(Persistent Volume Claim)，到 StorageClass 与 Dynamic Provisioning，再到现在 “out-of-tree” 的 CSI(Container Storage Interface)，Kubernetes 社区一直在演进存储的实现。</p>
<p>前面基础的就不讲了，我们从 StorageClass 与 Dynamic Provisioning 开始了解。</p>
<!--more-->  
<h2 id="关于-storageclass-与-dynamic-provisioning">关于 StorageClass 与 Dynamic Provisioning</h2>
<p>StorageClass 为存储提供了“类”的概念，使得 PVC 可以申请不同类别的 PV，以满足用户不同质量、不同策略要求的存储需求。但仅仅是这样还不够，我们还需要手动去创建存储，创建 PV 并与之绑定。所以 StorageClass 还有一个功能就是<strong>动态卷供应（Dynamic Provisioning）</strong>，通过它，Kubernetes 可以根据用户的需求，自动创建其需要的存储。</p>
<h3 id="如何使用">如何使用</h3>
<p>我们需要创建 StorageClass 对象，通过 <code>provisioner</code> 属性指定所用的动态供应的种类：</p>
<pre><code class="language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
</code></pre>
<p>创建好以后，所有指定这个 StorageClass 的 PVC 都会动态分配 PV：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example
  namespace: default
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: standard
</code></pre>
<p>当然，也需要些其他的配置，比如 aws-ebs 需要在启动参数中加入 <code>--cloud-provider=aws</code>。Glusterfs 需要在集群节点中预先安装好分布式存储等。具体请参考官方手册或 Google，这里不赘述了。</p>
<h3 id="external-provisioner">External provisioner</h3>
<p>官方提供了许多 Provisioner 的实现：AWSElasticBlockStore、AzureFile、Glusterfs <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">等等</a>，这些都是 “in-tree” 的，所以官方也在实验一些 external provisioner 的实现方式。在 <strong><a href="https://github.com/kubernetes-incubator/external-storage">kubernetes-incubator/external-storage</a></strong> 这个仓库中，就有一些孵化中的项目，不过随着 CSI 的出现，应该已经孵死了。官方也正在将 “in-tree” 的存储实现迁移到 CSI 上。</p>
<h2 id="如何实现">如何实现</h2>
<p>我们根据 external-storage 仓库中的项目，简单的分析一下如何自定义一个 Dynamic Provisioner。</p>
<p>其实这个仓库中的项目都很简单，文件没有几个，代码也没有几行。这是因为它们都是基于官方社区的 <a href="https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner#sig-storage-lib-external-provisioner">library</a> 实现的，它实现了 <code>Provisioner Controller</code> 的整个流程，包括监听、创建 PV 资源等，我们只需要实现 <code>Provisioner</code> 接口的两个方法就可以：</p>
<pre><code>// Provisioner is an interface that creates templates for PersistentVolumes
// and can create the volume as a new resource in the infrastructure provider.
// It can also remove the volume it created from the underlying storage
// provider.
type Provisioner interface {
	// Provision creates a volume i.e. the storage asset and returns a PV object
	// for the volume
	Provision(ProvisionOptions) (*v1.PersistentVolume, error)
	// Delete removes the storage asset that was created by Provision backing the
	// given PV. Does not delete the PV object itself.
	//
	// May return IgnoredError to indicate that the call has been ignored and no
	// action taken.
	Delete(*v1.PersistentVolume) error
}
</code></pre>
<p><code>Provision</code> 方法需要根据给定的数据，分配存储，响应 PV 对象。<code>Delete</code> 方法需要在 PV 删除时，也删除对应存储中的数据。</p>
<p>我们选择仓库中的 nfs 项目来进行详细的分析，它不同于其他 client 类项目，它还维护了一份 nfs server，使得它可以不基于其他外部存储服务。可以在 <code>main</code> 函数中看到，通过 <code>runServer flag</code> 判断是否需要启动服务，默认为 <code>true</code>：</p>
<pre><code>	if *runServer {
		......
		go func() {
			for {
				// This blocks until server exits (presumably due to an error)
				err = server.Run(ganeshaLog, ganeshaPid, ganeshaConfig)
				if err != nil {
					glog.Errorf(&quot;NFS server Exited Unexpectedly with err: %v&quot;, err)
				}

				// take a moment before trying to restart
				time.Sleep(time.Second)
			}
		}()
		// Wait for NFS server to come up before continuing provisioner process
		time.Sleep(5 * time.Second)
	}
</code></pre>
<p>随后通过 <code>Provisioner Controller</code> 的 <code>Run</code> 方法启动 Provisioner 服务：</p>
<pre><code>	// Create the provisioner: it implements the Provisioner interface expected by
	// the controller
	nfsProvisioner := vol.NewNFSProvisioner(exportDir, clientset, outOfCluster, *useGanesha, ganeshaConfig, *enableXfsQuota, *serverHostname, *maxExports, *exportSubnet)

	// Start the provision controller which will dynamically provision NFS PVs
	pc := controller.NewProvisionController(
		clientset,
		*provisioner,
		nfsProvisioner,
		serverVersion.GitVersion,
	)

	pc.Run(wait.NeverStop)
</code></pre>
<p><code>NewNFSProvisioner</code> 返回的是实现了 <code>Provisioner</code> 接口的结构体：</p>
<pre><code>type nfsProvisioner struct {
  ......
}

var _ controller.Provisioner = &amp;nfsProvisioner{}
</code></pre>
<p>接下来就看下如何实现的 <code>Provision</code> 方法：</p>
<pre><code>// options 里包含创建 pv 的数据，pvName、pvc、sc、selectedNode 等
func (p *nfsProvisioner) Provision(options controller.ProvisionOptions) (*v1.PersistentVolume, error) {
  // 在这里进行验证，创建目录等操作
	volume, err := p.createVolume(options)
	if err != nil {
		return nil, err
	}

	annotations := make(map[string]string)
  ......

	pv := &amp;v1.PersistentVolume{
		ObjectMeta: metav1.ObjectMeta{
			Name:        options.PVName,
			Labels:      map[string]string{},
			Annotations: annotations,
		},
		Spec: v1.PersistentVolumeSpec{
			PersistentVolumeReclaimPolicy: *options.StorageClass.ReclaimPolicy,
			AccessModes:                   options.PVC.Spec.AccessModes,
			Capacity: v1.ResourceList{
				v1.ResourceName(v1.ResourceStorage): options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)],
			},
			PersistentVolumeSource: v1.PersistentVolumeSource{
				NFS: &amp;v1.NFSVolumeSource{
					Server:   volume.server,
					Path:     volume.path,
					ReadOnly: false,
				},
			},
			MountOptions: options.StorageClass.MountOptions,
		},
	}

	return pv, nil
}

func (p *nfsProvisioner) createVolume(options controller.ProvisionOptions) (volume, error) {
	// 在这里验证剩余磁盘空间是否超出请求大小，只计算当前剩余
  gid, rootSquash, mountOptions, err := p.validateOptions(options)
	if err != nil {
		return volume{}, fmt.Errorf(&quot;error validating options for volume: %v&quot;, err)
	}
  ......
  // 根据 pvc 创建目录
	path := path.Join(p.exportDir, options.PVName)

	err = p.createDirectory(options.PVName, gid)
	if err != nil {
		return volume{}, fmt.Errorf(&quot;error creating directory for volume: %v&quot;, err)
	}
  ......
}


func (p *nfsProvisioner) validateOptions(options controller.ProvisionOptions) (string, bool, string, error) {
  ......
	var stat syscall.Statfs_t
	if err := syscall.Statfs(p.exportDir, &amp;stat); err != nil {
		return &quot;&quot;, false, &quot;&quot;, fmt.Errorf(&quot;error calling statfs on %v: %v&quot;, p.exportDir, err)
	}
	capacity := options.PVC.Spec.Resources.Requests[v1.ResourceName(v1.ResourceStorage)]
	requestBytes := capacity.Value()
	available := int64(stat.Bavail) * int64(stat.Bsize)
	if requestBytes &gt; available {
		return &quot;&quot;, false, &quot;&quot;, fmt.Errorf(&quot;insufficient available space %v bytes to satisfy claim for %v bytes&quot;, available, requestBytes)
	}

	return gid, rootSquash, mountOptions, nil
}
</code></pre>
<p>然后是 <code>Delete</code> 方法的实现：</p>
<pre><code>func (p *nfsProvisioner) Delete(volume *v1.PersistentVolume) error {
  ......
  // pv 删除后，删除对应的目录
	err = p.deleteDirectory(volume)
	if err != nil {
		return fmt.Errorf(&quot;error deleting volume's backing path: %v&quot;, err)
	}
  ......
	return nil
}
</code></pre>
<p>这里只是简单的讲解下 <code>Provisioner</code> 的实现，省略了其他一些比如 <code>xfs quota</code> 等操作，有兴趣的可以去项目中看一下。顺便提一下，这个项目虽然部署了 nfs server，但没有部署成分布式存储，局限性很大，毕竟只是实验中的项目，生产环境慎用。</p>
<h2 id="后记">后记</h2>
<p>碰巧在项目中接触到了 nfs 这个 Provisioner，并且经过测试及源码分析验证了这个项目不可用。经过查阅学习之后写下了这篇文章，算是为以后学习 CSI 作准备吧。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vue 学习路线]]></title>
        <id>https://cnbailian.github.io/post/vue-learning-route/</id>
        <link href="https://cnbailian.github.io/post/vue-learning-route/">
        </link>
        <updated>2019-04-10T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>本文旨在规划 Vue 框架的学习路线，通过掌握基本概念了解框架，熟悉生态系统，最后深入至框架本身。并未涉及到框架使用方式等详细内容，对每个知识点也只是浅尝即止。</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文旨在规划 Vue 框架的学习路线，通过掌握基本概念了解框架，熟悉生态系统，最后深入至框架本身。并未涉及到框架使用方式等详细内容，对每个知识点也只是浅尝即止。</p>
<!--more-->  
<h2 id="为什么选择vue">为什么选择vue</h2>
<p>可能有很多人只是知道 Vue 这个框架，并没有详细的了解，所以在这里简单的列举下 Vue 的优势。</p>
<ul>
<li>
<p>Vue 有着前端框架中最多的 stars，人数众多的开发者，保证了社区的繁荣。</p>
</li>
<li>
<p>相对来说较平滑的学习曲线，这主要取决于vue是一个渐进式框架，同时使用基础的HTML模版语法，这让有HTML经验的人很少上手。</p>
</li>
<li>
<p>渐进式框架也可以更好的逐步的改变原有项目。</p>
</li>
<li>
<p>团队中有来自世界各地的专家开发者，中文社区和文档质量相对不错。</p>
</li>
</ul>
<h2 id="学习路线">学习路线</h2>
<ol>
<li>
<h3 id="javascript与web基础">JavaScript与web基础</h3>
<p>学习 Vue 框架之前必须先了解 JavaScript 与 web 开发的基本知识，就像看一本英语书前，你需要先掌握英文。</p>
</li>
<li>
<h3 id="vue-基本概念">Vue 基本概念</h3>
<p>使用 Vue 来构建项目，需要先了解一些基本概念：</p>
<p><strong>渐进式框架</strong></p>
<p>渐进式就是：一步一步，不需要在一开始就把所有的东西都用上。</p>
<p>在 Vue 上的体现就是：它的核心库只包含视图，其他的客户端路由、全局状态管理等通过核心插件提供。</p>
<p>Vue 在设计角度上，包含了解决构建大型单页面应用的大部分问题，但你不需要一开始就把所有的东西都用上。这就带来了较平滑的学习曲线与对老项目渐进式重构的好处。</p>
<p><strong>声明式渲染</strong></p>
<pre><code class="language-HTML">&lt;div id=&quot;app&quot;&gt;
  {{ message }}
&lt;/div&gt;
</code></pre>
<pre><code class="language-javascript">var app = new Vue({
  el: '#app',
  data: {
    message: 'Hello Vue!'
  }
})
</code></pre>
<p>这里的示例代码就是声明式渲染，你写出想要的结果，由框架执行渲染的命令。</p>
<p><strong>响应式数据</strong></p>
<p>在上面的示例代码中，数据就与 DOM 建立了关联，成为响应式数据。此时改变 <code>app.message</code> 的值，就可以看到页面也会发生对应改变。</p>
<p><strong>组件化</strong></p>
<p>组件化的核心思想就是：将页面结构映射为组件树。</p>
<figure data-type="image" tabindex="1"><img src="https://www.superbed.cn/pic/5c4555f25f3e509ed94b6480" alt="component-tree.png" loading="lazy"></figure>
<p>组件是资源独立的，组件可以复用，组件与组件之间可以嵌套。</p>
<p><strong>单页面应用与客户端路由</strong></p>
<p>单页面应用（SPA）可以通过单个页面实现传统网站多个页面的功能，通过客户端路由实现加载新内容，而不需要通过浏览器跳转，重新加载页面。</p>
<p>Vue Router 就是 Vue 的实现，由官方维护，通过插件的形式加载。</p>
<p><strong>状态管理</strong></p>
<p>在 Vue 中，每个组件管理着自己的状态，如果有状态需要在多个组件间复用，就需要把共享的状态抽离出来，作为全局的状态来管理，这样，在任何组件中都能获取到。</p>
<p>这就是 Vuex 所做的事情。</p>
</li>
<li>
<h3 id="使用-vue-构建单页面应用">使用 Vue 构建单页面应用</h3>
<p>以上的基本概念用于理解 Vue，如果要将它实际应用到项目中，还需要了解更多的东西。</p>
<p><strong>构建工具</strong></p>
<p>Vue 提供了一个官方的 CLI：Vue CLI，为单页面应用搭建繁杂的脚手架。</p>
<p>最新的版本 Vue CLI3中加入了 GUI 的支持，对用户更为友好。</p>
<p><strong>使用 axios 访问 Web API</strong></p>
<p>Vue 的一个核心思想就是数据驱动。所谓数据驱动，是指视图是由数据驱动生成的，Vue 将数据与 DOM 关联，构建响应式数据，我们对视图的修改，不会直接修改 DOM，而是修改数据，响应至视图。</p>
<p>作为一个单页面应用，数据需要通过 Web API 获取，这些数据可能通过 RESTful API 或 GraphQL 提供，也可能通过 WebSocket 提供。</p>
<p>如果是使用的 HTTP 协议，在 Vue Cookbook 中，推荐使用基于 promise 的 axios。</p>
<p><strong>测试</strong></p>
<p>如果想要开发出稳定可维护的项目，测试是必不可少的。</p>
<p>Vue 官方团队提供了 Vue Test Utils，Vue Test Utils 通过将组件隔离挂载，然后模拟必要的输入和对输出的断言来测试。</p>
<p><strong>Chrome 开发者工具</strong></p>
<p>Vue.js devtools 是一个用于 Chrome 的开发者工具，使用它可以清楚的看到组件树的结构，组件的状态等信息。如果使用了 Vuex，还可以看到全局状态，并将其快照发送给其他人，这个人可以在控制台导入状态，方便定位问题。</p>
<p><strong>多端支持</strong></p>
<p>可以在 Weex 中使用 Vue，Vue 的官方也与 Weex 的团队加深联系，在未来的 Vue3 中，会有更好的支持。</p>
</li>
<li>
<h3 id="前端技术栈">前端技术栈</h3>
<p>上述所讲的大多是 Vue 或 Vue 生态系统中的工具。但 Vue 并不是独立存在的，它知识前端技术栈中的一部分。</p>
<p><strong>现代 JavaScript 与 Babel</strong></p>
<p>Vue 应用程序可以使用 ES5 开发，这是现代浏览器都支持的 JavaScript 标准。</p>
<p>如果想要获得更好的开发体验，可以更新 JavaScript 标准 ES2015 或更高版本，但这会导致不支持旧版浏览器，为了解决这个问题，就需要使用 Babel，它可以将你的新语法编译为 ES5 代码。</p>
<p><strong>Webpack</strong></p>
<p>Webpack 是一个模块打包器，它可以将你的应用程序中各个模块的代码打包至一个或多个文件中，形成浏览器可读的 js 文件。还可以在打包过程中，对代码进行转换、使用 Babel、Sass、TypeScript 等。</p>
<p>虽然 Vue CLI 可以为我们构建基础的 webpack 配置，并且在新版本中，可以使用 GUI 来调整，但这并不意味着你可以不学习它，你还是不可避免的需要自行调试它的配置。</p>
<p><strong>TypeScript 与 Flow</strong></p>
<p>Vue2 版本中使用的是 Flow，在 Vue3 中将重构为使用 TypeScript。</p>
<p>这两门语言的主要目的是让 js 拥有类型系统，使用它们可以写出高健壮性的代码，并且可以编译为普通的 ES 语法。</p>
<p>Vue3 将完全使用 TypeScript 编写，这并不意味着你必须使用它。但是如果想要了解 Vue 源码，也是不可避免的。</p>
</li>
<li>
<h3 id="vue-生态系统">Vue 生态系统</h3>
<p><strong>官方核心插件</strong></p>
<p>上述提到的 Vue Router、Vuex，还有 Vue SSR 都是由官方维护的，这区别于 React，官方主要是考虑到了社区维护会导致更新频繁、解决方案太杂乱的问题。</p>
<p><strong>官方工具</strong></p>
<p>上述也提到过的 Vue devtools、Vue CLI，还有 Vue Loader，也都是基于同样的原因。但这不意味着没有社区参与，作为开源项目，依然可以提出建议，修复问题，只是官方有一个发展方向作为参考。</p>
<p><strong>UI 组件库</strong></p>
<p>也可以称为 UI 框架，主要是一系列常用的组件，例如 Form、Table 等常见的元素，方便快速开发。</p>
<p>市面上有非常多的 UI 框架可供选择，Element UI、iView、Vux 等，各有各的风格特色。</p>
</li>
<li>
<h3 id="深入理解-vue">深入理解 Vue</h3>
<p><strong>为什么是渐进式框架</strong></p>
<p><em>框架的存在是为了帮助我们应对复杂度 - 《Vue 2.0——渐进式前端解决方案》</em></p>
<p>当我们在做一个前端应用时，会遇许多的问题，这些问题可以称为应用复杂度，前端框架的出现，就是为了降低应用复杂度，解决一些重复的并且已经有良好解决方案的问题。</p>
<p>但是，框架本身由于其学习曲线，也会带来不同的复杂度，称为框架复杂度。如何权衡应用复杂度与框架复杂度就称为了一个问题。</p>
<p>React 与 Vue 的选择的模式就是：以可弹性伸缩的框架复杂度来应对不同的应用复杂度。框架核心库只包含视图层，其他的问题都由可选的附加库/工具来解决。</p>
<p>Facebook 团队只专注做 React 本身，其他的问题都是由社区贡献解决方案，社区非常活跃，也有很多优秀的想法和思路，但社区的活跃性也会带来一些副作用，版本更新太快，一个问题有太多的解决方案导致的选择困难，库与库之间可能存在的磨合问题。</p>
<p>Vue 的团队选择的方向就是渐进式，核心插件\工具由团队开发，负责一些大方向上的统一，同时也是模块化的，可供选择。</p>
<p><strong>声明式渲染</strong></p>
<p>Vue 或者说现代 js 框架，都有一个统一的看法，数据状态是唯一的真相，DOM 状态只是数据状态的映射。所有的逻辑操作都应在状态的层面进行，当状态发生改变时，DOM 在框架的帮助下自动更新至合理的状态。</p>
<p>那么，Vue 时如何实现的呢？主要是使用的虚拟（Virtual） DOM。</p>
<p>虚拟 DOM 简单来说就是使用 js 对象去描述一个 DOM 节点，它产生的前提就是一个 DOM 元素在浏览器中是非常庞大的，因为有着各种属性，各种事件，浏览器的标准就是这么设计的。相比于 DOM 对象，原生的 js 对象处理起来更快，而且更简单。</p>
<p>Vue 将它所有要监听的 DOM 映射为一个虚拟 DOM 树，这个树非常的轻量，它的职责就是描述当前页面的 DOM 状态。</p>
<p>当数据状态发生改变时，Vue 的响应系统会侦测到变化，并生成一个新的虚拟 DOM 树，通过与上一个虚拟 DOM 树进行比较，将改动应用至真实 DOM 状态。</p>
<p>不同于 React 的是，Vue 可以使用 HTML 模版，也可以是用 JSX，这是 Vue 在编译时将模版编译为渲染函数。</p>
<p><strong>状态管理</strong></p>
<p>状态管理本质上就是把整个应用抽象为下图中的循环，State 驱动 View 的渲染，而用户对 View 进行操作产生 Action，会使 State 产生变化，从而导致 View 重新渲染，这就是单向数据流。</p>
<figure data-type="image" tabindex="2"><img src="https://www.superbed.cn/pic/5c4554ae5f3e509ed94b5b8c" alt="state-单向数据流.png" loading="lazy"></figure>
<p>在 Vue 中，一个组件就已经是这样的结构了，在多个组件共享状态时，或是来自不同视图的行为变更一个状态时，应该如何管理呢？此问题的答案就是 Vuex。</p>
<p>它将组件的共享状态抽离出来，放入 Store，组件通过调度（<code>dispatch</code>）使用 Action，Action 通过提交（<code>commit</code>）Mutation 修改 State，然后响应到组件。</p>
<figure data-type="image" tabindex="3"><img src="https://www.superbed.cn/pic/5c4554c75f3e509ed94b5c5d" alt="vuex.png" loading="lazy"></figure>
</li>
<li>
<h3 id="实现原理">实现原理</h3>
<p><strong>生命周期</strong></p>
<figure data-type="image" tabindex="4"><img src="https://www.superbed.cn/pic/5c45553b5f3e509ed94b5ec4" alt="lifecycle.png" loading="lazy"></figure>
<p><strong>Virtual DOM</strong></p>
<p>Virtual DOM 在 Vue 中的实现。</p>
<p><strong>响应式数据原理</strong></p>
<p>在 Vue2，使用的是 ES5 的 <code>Object.defineProperty</code> 来构成数据监听系统，这也是 Vue2 不能兼容 IE8 及以下的原因。</p>
<p>在即将到来的 Vue3 中，会使用 <code>Proxy</code> 进行重构数据监听系统，这会导致 Vue3 不能兼容 IE11 及一下，Vue 团队会提出其他的办法来解决这个问题。</p>
<p><strong>编译与渲染函数</strong></p>
<p>在 Vue 中，会将模版编译为渲染函数，在 Vue3 中，也做出了相当的优化。</p>
<p><strong>组件化</strong></p>
<p>每一个组件就是一个 Vue 实例，组件内部是如何工作的，组件间的嵌套等实现。</p>
<p><strong>v-model</strong></p>
<p>Vue 提供了 <code>v-model</code> 的指令，用于实现表单与数据状态之间的双向绑定，这也没有破坏单向数据流，只是语法糖。</p>
<pre><code class="language-html">&lt;input v-model=&quot;sth&quot; /&gt;
&lt;input v-bind:value=&quot;sth&quot; v-on:input=&quot;sth = $event.target.value&quot; /&gt;
</code></pre>
<p><strong>核心插件</strong></p>
<p>Vue Router：客户端路由中存在的种种问题，嵌套路由、重定向/别名、懒加载等。</p>
<p>Vuex：初始化过程，如何管理全局状态等。</p>
</li>
</ol>
<h2 id="思维导图">思维导图</h2>
<figure data-type="image" tabindex="5"><img src="https://www.superbed.cn/pic/5c4554da5f3e509ed94b5cd6" alt="Vue 学习路线.png" loading="lazy"></figure>
<h2 id="相关学习资料">相关学习资料</h2>
<p><a href="https://www.infoq.cn/article/vue-2-progressive-front-end-solution">《Vue 2.0——渐进式前端解决方案》</a> 尤雨溪</p>
<p><a href="https://cn.vuejs.org/v2/guide/">《Vue Guide》</a> Vue 官方团队</p>
<p><a href="https://ustbhuangyi.github.io/vue-analysis/">《Vue.js 技术揭秘》</a> ustbhuangyi</p>
<p><a href="https://vuex.vuejs.org/zh/">《Vuex》</a> Vuex</p>
<p><a href="https://vue.w3ctech.com/">VueConf</a> VueConf</p>
<p><a href="https://vuejsdevelopers.com/">Vue.js developers</a> vuejsdevelopers.com</p>
<h2 id="参考文章">参考文章</h2>
<p><a href="https://www.infoq.cn/article/9XymmTqu*4QwahqikMka">《2019 年 Vue 学习路线图》</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go 的并发性与调度器]]></title>
        <id>https://cnbailian.github.io/post/concurrency-and-scheduler-of-go/</id>
        <link href="https://cnbailian.github.io/post/concurrency-and-scheduler-of-go/">
        </link>
        <updated>2019-03-06T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>本篇文章是我对 Go 语言并发性的理解总结，适合初步了解并发，对 Go 语言的并发编程与调度器原理有兴趣的读者。</p>
]]></summary>
        <content type="html"><![CDATA[<p>本篇文章是我对 Go 语言并发性的理解总结，适合初步了解并发，对 Go 语言的并发编程与调度器原理有兴趣的读者。</p>
<!--more-->  
<h3 id="你真的了解并发吗">你真的了解并发吗？</h3>
<p>相信读者都对并发有着一定的理解，也都对 Go 语言感兴趣，Go 最吸引人的地方可能就是它的内建并发支持，使用 <code>go</code> 关键字，就可以轻松的实现并发。但是，你真正的了解<strong>并发</strong>吗？</p>
<p>并发这个词，你去问编程领域中不同的人，会给出不同的答案。对于 WEB 领域的开发人员来说，并发通常是指<strong>同一时刻的请求量</strong>，WEB 领域的面试官经常会问到的问题：“做过多少并发的项目？”或“接触过高并发的项目吗？”就是应用的这个概念。<em>高并发在这里还有个可能的概念是：<strong>同时应对许多请求所使用的技术</strong>，这通常与分布式、并行等概念挂钩，需要结合上下文语境来判断。</em></p>
<p>并发是一个有趣的词，因为它对编程领域中的不同人员意味着不同的事情，在广义概念下，有着许多狭义概念。除了“并发”之外，你可能还听过“并行”、“多线程”、“异步”等词汇，有些人认为这些词意思相同，而其他人则在每个词之间划清界限。</p>
<p>下面，让我们看看 Go 语言编程中，“并发”这个词的概念。</p>
<h3 id="go-语言中的并发性">Go 语言中的并发性</h3>
<p>Go 语言的并发性并不是 WEB 领域的并发概念，很多人对此有所混淆。在 Go 语言发布之初，大家对 Go 的并发特性都有所疑问：</p>
<ul>
<li>为什么要有并发？</li>
<li>什么是并发？</li>
<li>这个想法源自哪里？</li>
<li>并发有什么好处？</li>
<li>我该如何使用它？</li>
</ul>
<p>面对这些问题，Rob Pike（Go 语言作者之一）在2012年的 Google I/O 上做了一次精彩的演讲：<a href="https://www.youtube.com/watch?v=f6kdp27TYZs">《Go Concurrency Patterns》</a>，在这场演讲中，他回答了上述问题，并通过详细的示例讲解了 goroutine、channel 与 select 的使用，建议大家都去看一看这场演讲。</p>
<p>简单的总结一下并发在 Go 语言编程中的概念：</p>
<p><strong>“并发是一种将程序分解成小片段独立执行的程度设计方法”，它是一种结构化程序的方式，独立执行计算的组合。</strong></p>
<p>在上述的演讲中可以看出，Go 语言推荐使用并发，我们也应该遵循这种编程方式。对于程序员来说，代码更有说服力，我们可以通过这个<a href="https://play.golang.org/p/9U22NfrXeq">素数筛选程序</a>来理解 Go 的并发编程：</p>
<pre><code class="language-go">// A concurrent prime sieve

package main

// Send the sequence 2, 3, 4, ... to channel 'ch'.
func Generate(ch chan&lt;- int) {
	for i := 2; ; i++ {
		ch &lt;- i // Send 'i' to channel 'ch'.
	}
}

// Copy the values from channel 'in' to channel 'out',
// removing those divisible by 'prime'.
func Filter(in &lt;-chan int, out chan&lt;- int, prime int) {
	for {
		i := &lt;-in // Receive value from 'in'.
		if i%prime != 0 {
			out &lt;- i // Send 'i' to 'out'.
		}
	}
}

// The prime sieve: Daisy-chain Filter processes.
func main() {
	ch := make(chan int) // Create a new channel.
	go Generate(ch)      // Launch Generate goroutine.
	for i := 0; i &lt; 10; i++ {
		prime := &lt;-ch
		print(prime, &quot;\n&quot;)
		ch1 := make(chan int)
		go Filter(ch, ch1, prime)
		ch = ch1
	}
}
</code></pre>
<p>它并不是复杂度最低的算法，特别是寻找大素数方面，但却是最能体现 Go 并发编程、<a href="https://golang.org/doc/faq#What_operations_are_atomic_What_about_mutexes"><strong>通过通信共享内存</strong></a>的理念，而且非常优雅。</p>
<p>在这段代码中，通过 goroutine 的组合，实现一层层的筛选器，筛选器之间通过 channel 通信，每一个筛选器就是一个素数，每个给 main goroutine 通信的内容也是素数，简直精妙。</p>
<p>通过下面的 gif 动画能清晰的看到程序运行过程：</p>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g85w9fqvjfg30l40c2qek.gif" alt="primesieve" loading="lazy"></figure>
<h3 id="并发不是并行">并发不是并行</h3>
<h4 id="go-的并行">Go 的并行</h4>
<p>只需要 GOMAXPROCS 的值大于1，就可以让 Go 程序在多核机器上实现以并行的形式运行。但并发的程序一定可以并行吗？</p>
<p>我们需要明确一个观点：**并发不是为了效率，并发的程序不一定可以并行。**还是上面素数的例子，这段代码是并发的，但不可以并行，因为它的每一个执行片段都需要上一个片段的筛选与通信。</p>
<h4 id="正交概念">正交概念</h4>
<p><em>正交概念：从数学上引进正交这个词，用于表示指相互独立，相互间不可替代，并且可以组合起来。</em></p>
<p>在广义概念上来讲，并发与并行是<strong>正交概念</strong>，对于 Go 语言的并发性来讲也是如此。</p>
<h4 id="concurrency-is-not-parallelism">《Concurrency is not Parallelism》</h4>
<p>同样，在 Go 语言发布之初，有很多人混淆了并发与并行的概念，对此，Rob Pike 发表了另一篇演讲<a href="https://talks.golang.org/2012/waza.slide#1">《Concurrency is not Parallelism》</a>，通过地鼠烧书的比喻与简单负载均衡器的示例，详细的阐述了并发与并行的区别。</p>
<p>这里不再复述地鼠例子，只是简单的总结，感兴趣的建议去看演讲：</p>
<p><strong>并行是指同时能执行多个事情。</strong></p>
<p><strong>并发关乎结构，是一种结构化程序的方式。</strong></p>
<p><strong>并行关乎执行，表述的是程序的运行状态。</strong></p>
<h3 id="go-语言是如何支持并发的">Go 语言是如何支持并发的？</h3>
<p>上面一直在讲 Go 语言的并发性，接下来看下 Go 语言是如何做到的并发支持。</p>
<p>我们在使用 Go 编写并发程序的过程中，无需关心线程的维护、调度等一系列问题，只需要关心程序结构的分解与组合、goroutine 之间的通信就可以写出良好的并发程序，这全部都要依赖于 Go 语言内建的 G-P-M 模型。</p>
<h4 id="模型演化过程">模型演化过程</h4>
<p>在 Go 语言1.0版本时，只有 G-M 模型，Google 工程师 Dmitry Vyukov 在<a href="https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit"><strong>《Scalable Go Scheduler Design Doc》</strong></a>中指出了该模型在并发伸缩性方面的问题：</p>
<blockquote>
<ol>
<li>所有对 G 的操作：创建、重新调用等由单个全局锁(Sched.Lock)保护，浪费时间。</li>
<li>当 M 阻塞时，G 需要传递给别的 M 执行，这导致调度延迟增大以及额外的性能损耗；</li>
<li>M 用到的 <code>mCache</code> 属于内核线程，当 M 阻塞后相应的内存资源仍被占用，导致内存占用过高；</li>
<li>由于 syscall 导致 M 的阻塞和恢复，导致了额外性能损耗。</li>
</ol>
</blockquote>
<p>并且亲自下场，重新设计、改进了 Go scheduler，在 Go1.1 版本中实现了 G-P-M 模型：</p>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g85w9rofpbj30yg0lcgn5.jpg" alt="gpm-nino" loading="lazy"></figure>
<h4 id="g-p-m-模型">G-P-M 模型</h4>
<p>那么这套模型与调度是怎样的呢？先来简单的说一下 G、P、M 的定义：</p>
<ul>
<li>G：表示 Goroutine，G 存储了 goroutine 执行的栈信息，状态，任务函数，可重用。</li>
<li>P：Processor，表示逻辑处理器，拥有一个本地队列。对于 G 来说，P 相当于 CPU 内核，只有进入到 P 的队列中，才可以被调度。对于 M 来说，P 提供了相关的执行环境（Context），如内存分配状态，任务队列等。P 的数量就是程序可最大可并行的 G 的数量（前提：物理CPU核数 &gt;= P的数量），由用户设置的 GOMAXPROCS 决定。</li>
<li>M：Machine，是对系统线程的抽象，是真正执行计算的部分。M 在绑定 P 后会在队列中获取 G，切换到 G 的执行栈并执行 G 的函数。M 数量不定，但同时只有 P 个 M 在执行，为了防止创建过多系统线程导致系统调度出现问题，目前默认最大限制10000个。</li>
</ul>
<p>接下来了解这套模型的基本调度，在调度过程中还有一个 <a href="http://supertech.csail.mit.edu/papers/steal.pdf"><em>work-stealing</em></a> 的算法：</p>
<ul>
<li>每个 P 维护一个本地队列；</li>
<li>当一个 G 被创建后，放入当前 P 的本地队列中，如果队列已满，放入全局队列；</li>
<li>当 M 执行完一个 G 后，会在 当前 P 的队列中取出新的 G，队列为空则在全局队列中加锁获取；</li>
<li>如果全局队列也为空，则去其他的 P 的队列中偷出一半的 G，放入自己的本地队列。</li>
</ul>
<p>Go 语言就是凭借着这套优秀的并发模型与调度，实现了内建的并发支持。</p>
<h3 id="goroutine-调度器的深入">Goroutine 调度器的深入</h3>
<p>让我们深入的了解一下 goroutine 调度器。</p>
<h4 id="调度器解决了什么问题">调度器解决了什么问题？</h4>
<h5 id="阻塞问题">阻塞问题</h5>
<blockquote>
<p>如果任务G陷入到阻塞的系统调用中，内核线程M将一起阻塞，于是实际的运行线程少了一个。更严重的，如果所有M都阻塞了，那些本可以运行的任务G将没有系统资源运行。</p>
</blockquote>
<p>Go 在执行阻塞的系统调用时会调用 <code>entersyscallblock</code> ，然后通过 <code>handoffp</code> 解绑 M 对应的 P。如果此时 P 的本地队列中还有 G，P 会去寻找别的 M 或创建新的 M 继续执行，若本地队列为空，则进入 <code>pidle</code> 链表，等待有需要时被取出。</p>
<p>如果是调用的 <code>entersyscall</code>，会将 P 的状态置为 <code>_Psyscall</code>。监控线程 <code>sysmon</code> 会通过 <code>retake</code> 循环所有的 P，发现是 <code>_Psyscall</code> 状态，就会调用 <code>handoffp</code> 来释放。</p>
<h5 id="抢占调度">抢占调度</h5>
<p>在 Go1.1 版本中，是没有抢占调度的，当前 G 只有涉及到锁操作，读写 channel 才会触发切换。若没有抢占机制，同一个 M 上的其他任务 G 有可能会长时间执行不到，甚至会被死循环锁住。</p>
<p>于是 Dmitry Vyukov 提出了<a href="https://docs.google.com/document/d/1ETuA2IOmnaQ4j81AtTGT40Y4_Jr6_IDASEKg0t0dBR8/edit">《Go Preemptive Scheduler Design Doc》</a>, 并在1.2版本中引入了初级的抢占。</p>
<p>监控线程 <code>sysmon</code> 会通过 <code>retake</code> 循环所有的 P，发现运行时间超出 <code>forcePreemptNS</code> 限制（10ms）的 P，就会通过 <code>preemptone</code> 发起抢占。</p>
<h5 id="goroutine-的负载均衡">Goroutine 的负载均衡</h5>
<blockquote>
<p>内核线程M不是从全局任务队列中得到G，而是从M本地维护的G缓存中获取任务。如果某个M的G执行完了，而别的M还有很多G，这时如果G不能切换将造成CPU的浪费。</p>
</blockquote>
<p>这部分的实现是在 M 的启动函数 <code>mstart</code> 中 <code>schedule</code> 的调用来实现，它会先查找本地队列，然后查找全局队列，最后是随机偷取其他 P 的一半 G，直到取到 G 或停掉 M。为了防止全局队列被“饿死”，每61次调度，会先在全局队列中查找。</p>
<h4 id="调度器相关源码">调度器相关源码</h4>
<p>调度器部分的代码主要集中在 <code>src/runtime/runtime2.go</code> 与 <code>src/runtime/proc.go</code> 这两个文件中。</p>
<p>调度器的4个基本结构：g、m、p、schedt，都在 <code>runtime2.go</code> 中，<code>schedt</code> 可能有些陌生，它是调度器的核心结构，也是全局资源池，用来存储 G 的全局队列，空闲的 P 链表 <code>pidle</code>，空闲的 M 链表 <code>midle</code> 等等。</p>
<p>调度器的具体实现函数都在 <code>proc.go</code> 中，用户的所有代码都是运行在 goroutine 中，Go 在运行时会将 <code>main</code> 中的代码放入 <code>main goroutine</code> 中运行，这时还会启动监控系统 <code>sysmon</code>。</p>
<p>更多关于调度器的细节，例如加锁，与 GC 的交互等，需要通过进一步的阅读源码来了解。</p>
<h3 id="结束语">结束语</h3>
<p>看到这里，相信大家对“并发”会有全新的认识，本文旨在讲清 Go 语言的并发性，在以后的 Go 语言编程过程中，希望更倾向于并发编程。并发编程不仅结构清晰，通常来说也会更容易并行运行，使得程序运行效率提高。</p>
<h3 id="参考文章">参考文章</h3>
<p><a href="https://talks.golang.org/2012/concurrency.slide#1">《Go Concurrency Patterns》</a></p>
<p><a href="https://talks.golang.org/2012/waza.slide#1">《Concurrency is not Parallelism》</a></p>
<p><a href="https://github.com/changkun/go-under-the-hood/blob/master/book/part2runtime/ch06sched/basic.md">《go-under-the-hood》</a></p>
<p><a href="https://tonybai.com/2017/06/23/an-intro-about-goroutine-scheduler/">《也谈goroutine调度器》</a></p>
<p><a href="https://ninokop.github.io/2017/12/10/Goroutine%E6%B5%85%E6%9E%90/">《Goroutine浅析》</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[JSON Schema]]></title>
        <id>https://cnbailian.github.io/post/json-schema/</id>
        <link href="https://cnbailian.github.io/post/json-schema/">
        </link>
        <updated>2018-12-01T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>JSON Schema 详解：未完待续</strong></p>
]]></summary>
        <content type="html"><![CDATA[<p><strong>JSON Schema 详解：未完待续</strong></p>
<!--more-->  
<h2 id="第一篇了解-json">第一篇：了解 JSON</h2>
<h4 id="介绍">介绍</h4>
<p>JSON（JavaScriptObjectNotation） 是一种轻量级、基于文本、不限语言的数据交换格式，源自 ECMAScript，《<a href="http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf">Standard ECMA-262 3rd Edition - December 1999</a>》的一个子集，而后成为写入 RFC 文档：<a href="https://tools.ietf.org/html/rfc4627">RFC 4627</a>，最新的 RFC 标准为：<a href="https://tools.ietf.org/html/rfc8259">RFC 8259</a>。</p>
<p>JSON 的官方 MIME 类型是 <code>application/json</code>，文件扩展名是 <code>.json</code>。</p>
<h4 id="类型">类型</h4>
<p><strong>两种结构化类型</strong>：</p>
<ul>
<li>Object：<code>名称/值</code>的集合（A collection of name/value pairs），在不同的语言中，被理解为 <code>object</code>，<code>record</code>，<code>struct</code>，<code>dictionary</code>，<code>hash table</code> 等等。
<ul>
<li>使用大括号<code>{</code>，<code>}</code>包裹<code>名称/值</code>的集合，名称是字符串类型，值可以是以上任意类型。名称与值使用<code>:</code>分隔。<code>名称/值</code>之间使用<code>,</code>分隔。</li>
<li>格式示例：<code>{&quot;example&quot;:&quot;string&quot;}</code>。</li>
</ul>
</li>
<li>Array：值的有序列表（An ordered list of values），在大部分语言中，被理解为 <code>array</code>。
<ul>
<li>使用中括号<code>[</code>，<code>]</code>包裹值的列表，值可以是以上任意类型。值与值之间使用`,``分隔。</li>
<li>格式示例：<code>[0,&quot;1&quot;,null,false,true,{},[]]</code>。</li>
</ul>
</li>
</ul>
<p><strong>四种基本类型</strong>：</p>
<ul>
<li>字符串（string）
<ul>
<li>字符串是 Unicode 字符组成的集合，使用双引号包裹（<code>&quot;</code>），反斜杠转义（<code>\</code>）。</li>
<li>与 C 或 Java 的字符串相似。</li>
</ul>
</li>
<li>数值（number）
<ul>
<li>使用十进制，可以为负数或者小数。还可以用<code>e</code>或者<code>E</code>表示为指数形式。</li>
<li>数值也与 C 或 Java 的数值相似。</li>
</ul>
</li>
<li>布尔（boolean）
<ul>
<li><code>true</code> 或 <code>false</code>。</li>
</ul>
</li>
<li>空（null）
<ul>
<li><code>null</code>。</li>
</ul>
</li>
</ul>
<p><strong>Tips</strong>：</p>
<ul>
<li>值的是可以嵌套的。</li>
<li>名称是字符串类型，也就是说可以是任意 Unicode 字符，但是不推荐使用英文以外的语言，虽然 JSON 允许，但是有些使用 JSON 的语言可能不支持。</li>
<li>符号中间允许空白字符。</li>
<li>JSON 没有限制必须以 <code>object</code> 或结构类型作为顶层类型。</li>
<li>但是某些语言解析某种类型会更加方便，比如给 ios 最好是使用 <code>object</code> 作为顶层类型。</li>
<li>上述的集合或列表（string，object，array）可以是空集或空列。</li>
</ul>
<h4 id="示例">示例</h4>
<pre><code># Object
{
    &quot;Image&quot;: {
        &quot;Width&quot;:  800,
        &quot;Height&quot;: 600,
        &quot;Title&quot;:  &quot;View from 15th Floor&quot;,
        &quot;Thumbnail&quot;: {
            &quot;Url&quot;:    &quot;http://www.example.com/image/481989943&quot;,
            &quot;Height&quot;: 125,
            &quot;Width&quot;:  100
        },
        &quot;Animated&quot; : false,
        &quot;IDs&quot;: [116, 943, 234, 38793]
    }
}

# Array
[
    {
        &quot;precision&quot;: &quot;zip&quot;,
        &quot;Latitude&quot;:  37.7668,
        &quot;Longitude&quot;: -122.3959,
        &quot;Address&quot;:   &quot;&quot;,
        &quot;City&quot;:      &quot;SAN FRANCISCO&quot;,
        &quot;State&quot;:     &quot;CA&quot;,
        &quot;Zip&quot;:       &quot;94107&quot;,
        &quot;Country&quot;:   &quot;US&quot;
    },
    {
        &quot;precision&quot;: &quot;zip&quot;,
        &quot;Latitude&quot;:  37.371991,
        &quot;Longitude&quot;: -122.026020,
        &quot;Address&quot;:   &quot;&quot;,
        &quot;City&quot;:      &quot;SUNNYVALE&quot;,
        &quot;State&quot;:     &quot;CA&quot;,
        &quot;Zip&quot;:       &quot;94085&quot;,
        &quot;Country&quot;:   &quot;US&quot;
    }
]

# Only values

&quot;Hello world!&quot;

42

true

null
</code></pre>
<h2 id="第二篇介绍">第二篇：介绍</h2>
<h4 id="版本">版本</h4>
<p>当前版本：<a href="http://json-schema.org/specification.html">draft-07</a>。</p>
<h4 id="简介">简介</h4>
<p>JSON Schema 是一个用于<strong>注释</strong>和<strong>验证</strong> JSON documents 的词汇表。</p>
<p>JSON Schema 本身也是基于 JSON 格式，并且提供了一系列的规范，用于描述 JSON 数据的结构，旨在定义 JSON 数据的验证，文档，超链接导航和交互控制。</p>
<h4 id="用途">用途</h4>
<ul>
<li>对数据结构进行描述</li>
<li>构建人机可读的文档</li>
<li>校验数据</li>
</ul>
<h4 id="项目状态">项目状态</h4>
<p>共有三个规范：</p>
<ul>
<li><a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema/">JSON Schema (core)</a></li>
<li><a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema-validation/">JSON Schema Validation</a></li>
<li><a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema-hyperschema/">JSON Hyper-Schema</a></li>
</ul>
<p>项目组正在努力使它们成为正式的 RFC 标准。</p>
<p><em><a href="https://github.com/json-schema-org/json-schema-spec/milestone/6">draft-08</a></em></p>
<h4 id="如何应用">如何应用</h4>
<p>可以用于生成模拟数据，确保接近真实数据。</p>
<p>用于校验数据，实现自动化测试。</p>
<p>多端共用同一份验证。</p>
<h2 id="第三篇core-规范">第三篇：Core 规范</h2>
<h4 id="简介-2">简介</h4>
<p><a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema/">JSON Schema (core)</a></p>
<p>此规范主要是定义了核心术语、机制，包括引用其他 JSON Schema，以及正在使用的词汇表。</p>
<h4 id="状态">状态</h4>
<p>草案目前于2018年03月19日最后更新，到期时间2018年09月20日，ietf地址：<a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema/">https://datatracker.ietf.org/doc/draft-handrews-json-schema/</a>。</p>
<h4 id="概述">概述</h4>
<p><strong>媒体类型</strong></p>
<p>JSON 的媒体类型：<a href="https://tools.ietf.org/html/rfc8259#section-1.2">application/json</a> 。</p>
<p>JSON Schema 的媒体类型为 <code>application/schema+json</code>，它还有另外一种可选媒体类型用于提供额外的扩展功能：<code>application/schema-instance+json</code>。</p>
<p><strong>验证</strong></p>
<p>JSON Schema 描述了 <code>JSON 文档(JSON document)</code> 的结构，例如属性、长度限制等，程序可以以此判断 <code>实例(Instance)</code> 是否符合规范。</p>
<p>规范与关键字在 <a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema-validation/">JSON Schema Validation</a> 中定义。</p>
<p><strong>注释</strong></p>
<p>JSON Schema 可以对 <code>实例(Instance)</code> 添加注释。</p>
<p>规范与关键字在 <a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema-validation/">JSON Schema Validation</a> 中定义。</p>
<p><strong>超媒体与链接</strong></p>
<p>JSON Hyper-Schema 描述了  <code>JSON 文档(JSON document)</code> 的超文本结构，包括 <code>实例(Instance)</code> 中的资源链接关系等。</p>
<p>规范与关键字在 <a href="https://datatracker.ietf.org/doc/draft-handrews-json-schema-hyperschema/">JSON Hyper-Schema</a> 中定义。</p>
<h4 id="定义">定义</h4>
<p><strong>JSON Document</strong></p>
<p><code>JSON 文档(JSON document)</code> 是使用 <code>application/json</code> 媒体类型描述的资源。</p>
<p>简言之就是 JSON 值。</p>
<p><em>JSON Schema 中， <code>JSON 值(JSON value)</code>、<code>JSON 文本(JSON text)</code>、<code>JSON 文档(JSON document)</code>是等义词。</em></p>
<p><em>JSON Schema 也是 <code>JSON 文档(JSON document)</code>。</em></p>
<p><strong>Instance</strong></p>
<p><code>实例(Instance)</code> 是使用 JSON Schema 描述的 <code>JSON 文档(JSON document)</code>。</p>
<p><strong>JSON Schema document</strong></p>
<p>使用 <code>application/schema+json</code> 媒体类型描述的 <code>JSON 文档(JSON document)</code>，简称 schema。</p>
<h4 id="关键字">关键字</h4>
<p><strong>$schema</strong></p>
<p><code>$schema</code> 关键字用于声明当前 <code>JSON 文档(JSON document)</code> 是 <code>JSON Schema document</code>。</p>
<p>值可以是 JSON Schema 版本的标识符，也是资源的位置，但必须是 <a href="https://tools.ietf.org/html/rfc3986">URI</a>。</p>
<p><strong>$id</strong></p>
<p><code>$id</code> 关键字用于定义 schema 基本的 URI，可用于 <code>$ref</code> 的引用标识，值必须是 <a href="https://tools.ietf.org/html/rfc3986">URI</a>。</p>
<p><strong>$ref</strong></p>
<p><code>$ref</code> 关键字用于定义引用 schema，值必须是 <a href="https://tools.ietf.org/html/rfc3986">URI</a>。</p>
<p>URI 只是标识符，不是网络定位器。如果 URI 是可以访问的 URL，不应该去下载。</p>
<p>如果有两个 schema 互相使用 <code>$ref</code> 进行引用，不应该陷入无限循环。</p>
<p><strong>$comment</strong></p>
<p><code>$comment</code> 用于 schema 开发人员对文档进行注释，不需要展示给最终用户看。</p>
]]></content>
    </entry>
</feed>